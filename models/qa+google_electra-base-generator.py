import torch
from torch import tensor
import torch.nn as nn
from torch.nn import *
import torchvision
import torchvision.models as models
from torchvision.ops.stochastic_depth import stochastic_depth
import time
import builtins
import operator
import sys
import os

class M(torch.nn.Module):
    def __init__(self):
        super(M, self).__init__()
        self.embedding0 = Embedding(30522, 768, padding_idx=0)
        self.embedding1 = Embedding(2, 768)
        self.embedding2 = Embedding(512, 768)
        self.layernorm0 = LayerNorm((768,), eps=1e-12, elementwise_affine=True)
        self.dropout0 = Dropout(p=0.1, inplace=False)
        self.linear0 = Linear(in_features=768, out_features=256, bias=True)
        self.linear1 = Linear(in_features=256, out_features=256, bias=True)
        self.linear2 = Linear(in_features=256, out_features=256, bias=True)
        self.linear3 = Linear(in_features=256, out_features=256, bias=True)
        self.dropout1 = Dropout(p=0.1, inplace=False)
        self.linear4 = Linear(in_features=256, out_features=256, bias=True)
        self.dropout2 = Dropout(p=0.1, inplace=False)
        self.layernorm1 = LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        self.linear5 = Linear(in_features=256, out_features=1024, bias=True)
        self.linear6 = Linear(in_features=1024, out_features=256, bias=True)
        self.dropout3 = Dropout(p=0.1, inplace=False)
        self.layernorm2 = LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        self.linear7 = Linear(in_features=256, out_features=256, bias=True)
        self.linear8 = Linear(in_features=256, out_features=256, bias=True)
        self.linear9 = Linear(in_features=256, out_features=256, bias=True)
        self.dropout4 = Dropout(p=0.1, inplace=False)
        self.linear10 = Linear(in_features=256, out_features=256, bias=True)
        self.dropout5 = Dropout(p=0.1, inplace=False)
        self.layernorm3 = LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        self.linear11 = Linear(in_features=256, out_features=1024, bias=True)
        self.linear12 = Linear(in_features=1024, out_features=256, bias=True)
        self.dropout6 = Dropout(p=0.1, inplace=False)
        self.layernorm4 = LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        self.linear13 = Linear(in_features=256, out_features=256, bias=True)
        self.linear14 = Linear(in_features=256, out_features=256, bias=True)
        self.linear15 = Linear(in_features=256, out_features=256, bias=True)
        self.dropout7 = Dropout(p=0.1, inplace=False)
        self.linear16 = Linear(in_features=256, out_features=256, bias=True)
        self.dropout8 = Dropout(p=0.1, inplace=False)
        self.layernorm5 = LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        self.linear17 = Linear(in_features=256, out_features=1024, bias=True)
        self.linear18 = Linear(in_features=1024, out_features=256, bias=True)
        self.dropout9 = Dropout(p=0.1, inplace=False)
        self.layernorm6 = LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        self.linear19 = Linear(in_features=256, out_features=256, bias=True)
        self.linear20 = Linear(in_features=256, out_features=256, bias=True)
        self.linear21 = Linear(in_features=256, out_features=256, bias=True)
        self.dropout10 = Dropout(p=0.1, inplace=False)
        self.linear22 = Linear(in_features=256, out_features=256, bias=True)
        self.dropout11 = Dropout(p=0.1, inplace=False)
        self.layernorm7 = LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        self.linear23 = Linear(in_features=256, out_features=1024, bias=True)
        self.linear24 = Linear(in_features=1024, out_features=256, bias=True)
        self.dropout12 = Dropout(p=0.1, inplace=False)
        self.layernorm8 = LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        self.linear25 = Linear(in_features=256, out_features=256, bias=True)
        self.linear26 = Linear(in_features=256, out_features=256, bias=True)
        self.linear27 = Linear(in_features=256, out_features=256, bias=True)
        self.dropout13 = Dropout(p=0.1, inplace=False)
        self.linear28 = Linear(in_features=256, out_features=256, bias=True)
        self.dropout14 = Dropout(p=0.1, inplace=False)
        self.layernorm9 = LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        self.linear29 = Linear(in_features=256, out_features=1024, bias=True)
        self.linear30 = Linear(in_features=1024, out_features=256, bias=True)
        self.dropout15 = Dropout(p=0.1, inplace=False)
        self.layernorm10 = LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        self.linear31 = Linear(in_features=256, out_features=256, bias=True)
        self.linear32 = Linear(in_features=256, out_features=256, bias=True)
        self.linear33 = Linear(in_features=256, out_features=256, bias=True)
        self.dropout16 = Dropout(p=0.1, inplace=False)
        self.linear34 = Linear(in_features=256, out_features=256, bias=True)
        self.dropout17 = Dropout(p=0.1, inplace=False)
        self.layernorm11 = LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        self.linear35 = Linear(in_features=256, out_features=1024, bias=True)
        self.linear36 = Linear(in_features=1024, out_features=256, bias=True)
        self.dropout18 = Dropout(p=0.1, inplace=False)
        self.layernorm12 = LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        self.linear37 = Linear(in_features=256, out_features=256, bias=True)
        self.linear38 = Linear(in_features=256, out_features=256, bias=True)
        self.linear39 = Linear(in_features=256, out_features=256, bias=True)
        self.dropout19 = Dropout(p=0.1, inplace=False)
        self.linear40 = Linear(in_features=256, out_features=256, bias=True)
        self.dropout20 = Dropout(p=0.1, inplace=False)
        self.layernorm13 = LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        self.linear41 = Linear(in_features=256, out_features=1024, bias=True)
        self.linear42 = Linear(in_features=1024, out_features=256, bias=True)
        self.dropout21 = Dropout(p=0.1, inplace=False)
        self.layernorm14 = LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        self.linear43 = Linear(in_features=256, out_features=256, bias=True)
        self.linear44 = Linear(in_features=256, out_features=256, bias=True)
        self.linear45 = Linear(in_features=256, out_features=256, bias=True)
        self.dropout22 = Dropout(p=0.1, inplace=False)
        self.linear46 = Linear(in_features=256, out_features=256, bias=True)
        self.dropout23 = Dropout(p=0.1, inplace=False)
        self.layernorm15 = LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        self.linear47 = Linear(in_features=256, out_features=1024, bias=True)
        self.linear48 = Linear(in_features=1024, out_features=256, bias=True)
        self.dropout24 = Dropout(p=0.1, inplace=False)
        self.layernorm16 = LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        self.linear49 = Linear(in_features=256, out_features=256, bias=True)
        self.linear50 = Linear(in_features=256, out_features=256, bias=True)
        self.linear51 = Linear(in_features=256, out_features=256, bias=True)
        self.dropout25 = Dropout(p=0.1, inplace=False)
        self.linear52 = Linear(in_features=256, out_features=256, bias=True)
        self.dropout26 = Dropout(p=0.1, inplace=False)
        self.layernorm17 = LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        self.linear53 = Linear(in_features=256, out_features=1024, bias=True)
        self.linear54 = Linear(in_features=1024, out_features=256, bias=True)
        self.dropout27 = Dropout(p=0.1, inplace=False)
        self.layernorm18 = LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        self.linear55 = Linear(in_features=256, out_features=256, bias=True)
        self.linear56 = Linear(in_features=256, out_features=256, bias=True)
        self.linear57 = Linear(in_features=256, out_features=256, bias=True)
        self.dropout28 = Dropout(p=0.1, inplace=False)
        self.linear58 = Linear(in_features=256, out_features=256, bias=True)
        self.dropout29 = Dropout(p=0.1, inplace=False)
        self.layernorm19 = LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        self.linear59 = Linear(in_features=256, out_features=1024, bias=True)
        self.linear60 = Linear(in_features=1024, out_features=256, bias=True)
        self.dropout30 = Dropout(p=0.1, inplace=False)
        self.layernorm20 = LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        self.linear61 = Linear(in_features=256, out_features=256, bias=True)
        self.linear62 = Linear(in_features=256, out_features=256, bias=True)
        self.linear63 = Linear(in_features=256, out_features=256, bias=True)
        self.dropout31 = Dropout(p=0.1, inplace=False)
        self.linear64 = Linear(in_features=256, out_features=256, bias=True)
        self.dropout32 = Dropout(p=0.1, inplace=False)
        self.layernorm21 = LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        self.linear65 = Linear(in_features=256, out_features=1024, bias=True)
        self.linear66 = Linear(in_features=1024, out_features=256, bias=True)
        self.dropout33 = Dropout(p=0.1, inplace=False)
        self.layernorm22 = LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        self.linear67 = Linear(in_features=256, out_features=256, bias=True)
        self.linear68 = Linear(in_features=256, out_features=256, bias=True)
        self.linear69 = Linear(in_features=256, out_features=256, bias=True)
        self.dropout34 = Dropout(p=0.1, inplace=False)
        self.linear70 = Linear(in_features=256, out_features=256, bias=True)
        self.dropout35 = Dropout(p=0.1, inplace=False)
        self.layernorm23 = LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        self.linear71 = Linear(in_features=256, out_features=1024, bias=True)
        self.linear72 = Linear(in_features=1024, out_features=256, bias=True)
        self.dropout36 = Dropout(p=0.1, inplace=False)
        self.layernorm24 = LayerNorm((256,), eps=1e-12, elementwise_affine=True)
        self.linear73 = Linear(in_features=256, out_features=2, bias=True)
        self._tensor_constant00 = torch.rand(torch.Size([1, 384])).to(torch.int64)
        self._tensor_constant01 = torch.rand(torch.Size([1, 384])).to(torch.int64)
        self._tensor_constant10 = torch.rand(torch.Size([1, 384])).to(torch.int64)
        self._tensor_constant20 = torch.rand(torch.Size([1, 1, 1, 384])).to(torch.float32)
        self._tensor_constant21 = torch.rand(torch.Size([1, 1, 1, 384])).to(torch.float32)
        self._tensor_constant22 = torch.rand(torch.Size([1, 1, 1, 384])).to(torch.float32)
        self._tensor_constant23 = torch.rand(torch.Size([1, 1, 1, 384])).to(torch.float32)
        self._tensor_constant24 = torch.rand(torch.Size([1, 1, 1, 384])).to(torch.float32)
        self._tensor_constant25 = torch.rand(torch.Size([1, 1, 1, 384])).to(torch.float32)
        self._tensor_constant26 = torch.rand(torch.Size([1, 1, 1, 384])).to(torch.float32)
        self._tensor_constant27 = torch.rand(torch.Size([1, 1, 1, 384])).to(torch.float32)
        self._tensor_constant28 = torch.rand(torch.Size([1, 1, 1, 384])).to(torch.float32)
        self._tensor_constant29 = torch.rand(torch.Size([1, 1, 1, 384])).to(torch.float32)
        self._tensor_constant210 = torch.rand(torch.Size([1, 1, 1, 384])).to(torch.float32)
        self._tensor_constant211 = torch.rand(torch.Size([1, 1, 1, 384])).to(torch.float32)

    def forward(self, input_ids_1, attention_mask_1, token_type_ids_1, position_ids_1, head_mask_1, inputs_embeds_1, start_positions_1, end_positions_1, output_attentions_1, output_hidden_states_1, return_dict_1):
        x0=input_ids_1
        x1=attention_mask_1
        x2=token_type_ids_1
        x3=position_ids_1
        x4=torch.fx._symbolic_trace._assert_is_none(x3, 'position_ids has been specialized to have value None but got another value')
        x5=head_mask_1
        x6=torch.fx._symbolic_trace._assert_is_none(x5, 'head_mask has been specialized to have value None but got another value')
        x7=inputs_embeds_1
        x8=torch.fx._symbolic_trace._assert_is_none(x7, 'inputs_embeds has been specialized to have value None but got another value')
        x9=start_positions_1
        x10=torch.fx._symbolic_trace._assert_is_none(x9, 'start_positions has been specialized to have value None but got another value')
        x11=end_positions_1
        x12=torch.fx._symbolic_trace._assert_is_none(x11, 'end_positions has been specialized to have value None but got another value')
        x13=output_attentions_1
        x14=torch.fx._symbolic_trace._assert_is_none(x13, 'output_attentions has been specialized to have value None but got another value')
        x15=output_hidden_states_1
        x16=torch.fx._symbolic_trace._assert_is_none(x15, 'output_hidden_states has been specialized to have value None but got another value')
        x17=return_dict_1
        x18=torch.fx._symbolic_trace._assert_is_none(x17, 'return_dict has been specialized to have value None but got another value')
        x20=self.embedding0(self._tensor_constant00)
        x22=self.embedding1(self._tensor_constant00)
        x23=operator.add(x20, x22)
        x25=self.embedding2(self._tensor_constant10)
        x26=operator.add(x23, x25)
        x27=self.layernorm0(x26)
        x28=self.dropout0(x27)
        x29=self.linear0(x28)
        x30=self.linear1(x29)
        x31=self.linear2(x29)
        x32=x31.size()
        x33=operator.getitem(x32, slice(None, -1, None))
        x34=operator.add(x33, (4, 64))
        x35=x31.view(x34)
        x36=x35.permute(0, 2, 1, 3)
        x37=self.linear3(x29)
        x38=x37.size()
        x39=operator.getitem(x38, slice(None, -1, None))
        x40=operator.add(x39, (4, 64))
        x41=x37.view(x40)
        x42=x41.permute(0, 2, 1, 3)
        x43=x30.size()
        x44=operator.getitem(x43, slice(None, -1, None))
        x45=operator.add(x44, (4, 64))
        x46=x30.view(x45)
        x47=x46.permute(0, 2, 1, 3)
        x48=x36.transpose(-1, -2)
        x49=torch.matmul(x47, x48)
        x50=operator.truediv(x49, 8.0)
        x52=operator.add(x50, self._tensor_constant20)
        x53=torch.nn.functional.softmax(x52,dim=-1, _stacklevel=3, dtype=None)
        x54=self.dropout1(x53)
        x55=torch.matmul(x54, x42)
        x56=x55.permute(0, 2, 1, 3)
        x57=x56.contiguous()
        x58=x57.size()
        x59=operator.getitem(x58, slice(None, -2, None))
        x60=operator.add(x59, (256,))
        x61=x57.view(x60)
        x62=self.linear4(x61)
        x63=self.dropout2(x62)
        x64=operator.add(x63, x29)
        x65=self.layernorm1(x64)
        x66=self.linear5(x65)
        x67=torch._C._nn.gelu(x66)
        x68=self.linear6(x67)
        x69=self.dropout3(x68)
        x70=operator.add(x69, x65)
        x71=self.layernorm2(x70)
        x72=self.linear7(x71)
        x73=self.linear8(x71)
        x74=x73.size()
        x75=operator.getitem(x74, slice(None, -1, None))
        x76=operator.add(x75, (4, 64))
        x77=x73.view(x76)
        x78=x77.permute(0, 2, 1, 3)
        x79=self.linear9(x71)
        x80=x79.size()
        x81=operator.getitem(x80, slice(None, -1, None))
        x82=operator.add(x81, (4, 64))
        x83=x79.view(x82)
        x84=x83.permute(0, 2, 1, 3)
        x85=x72.size()
        x86=operator.getitem(x85, slice(None, -1, None))
        x87=operator.add(x86, (4, 64))
        x88=x72.view(x87)
        x89=x88.permute(0, 2, 1, 3)
        x90=x78.transpose(-1, -2)
        x91=torch.matmul(x89, x90)
        x92=operator.truediv(x91, 8.0)
        x94=operator.add(x92, self._tensor_constant20)
        x95=torch.nn.functional.softmax(x94,dim=-1, _stacklevel=3, dtype=None)
        x96=self.dropout4(x95)
        x97=torch.matmul(x96, x84)
        x98=x97.permute(0, 2, 1, 3)
        x99=x98.contiguous()
        x100=x99.size()
        x101=operator.getitem(x100, slice(None, -2, None))
        x102=operator.add(x101, (256,))
        x103=x99.view(x102)
        x104=self.linear10(x103)
        x105=self.dropout5(x104)
        x106=operator.add(x105, x71)
        x107=self.layernorm3(x106)
        x108=self.linear11(x107)
        x109=torch._C._nn.gelu(x108)
        x110=self.linear12(x109)
        x111=self.dropout6(x110)
        x112=operator.add(x111, x107)
        x113=self.layernorm4(x112)
        x114=self.linear13(x113)
        x115=self.linear14(x113)
        x116=x115.size()
        x117=operator.getitem(x116, slice(None, -1, None))
        x118=operator.add(x117, (4, 64))
        x119=x115.view(x118)
        x120=x119.permute(0, 2, 1, 3)
        x121=self.linear15(x113)
        x122=x121.size()
        x123=operator.getitem(x122, slice(None, -1, None))
        x124=operator.add(x123, (4, 64))
        x125=x121.view(x124)
        x126=x125.permute(0, 2, 1, 3)
        x127=x114.size()
        x128=operator.getitem(x127, slice(None, -1, None))
        x129=operator.add(x128, (4, 64))
        x130=x114.view(x129)
        x131=x130.permute(0, 2, 1, 3)
        x132=x120.transpose(-1, -2)
        x133=torch.matmul(x131, x132)
        x134=operator.truediv(x133, 8.0)
        x136=operator.add(x134, self._tensor_constant20)
        x137=torch.nn.functional.softmax(x136,dim=-1, _stacklevel=3, dtype=None)
        x138=self.dropout7(x137)
        x139=torch.matmul(x138, x126)
        x140=x139.permute(0, 2, 1, 3)
        x141=x140.contiguous()
        x142=x141.size()
        x143=operator.getitem(x142, slice(None, -2, None))
        x144=operator.add(x143, (256,))
        x145=x141.view(x144)
        x146=self.linear16(x145)
        x147=self.dropout8(x146)
        x148=operator.add(x147, x113)
        x149=self.layernorm5(x148)
        x150=self.linear17(x149)
        x151=torch._C._nn.gelu(x150)
        x152=self.linear18(x151)
        x153=self.dropout9(x152)
        x154=operator.add(x153, x149)
        x155=self.layernorm6(x154)
        x156=self.linear19(x155)
        x157=self.linear20(x155)
        x158=x157.size()
        x159=operator.getitem(x158, slice(None, -1, None))
        x160=operator.add(x159, (4, 64))
        x161=x157.view(x160)
        x162=x161.permute(0, 2, 1, 3)
        x163=self.linear21(x155)
        x164=x163.size()
        x165=operator.getitem(x164, slice(None, -1, None))
        x166=operator.add(x165, (4, 64))
        x167=x163.view(x166)
        x168=x167.permute(0, 2, 1, 3)
        x169=x156.size()
        x170=operator.getitem(x169, slice(None, -1, None))
        x171=operator.add(x170, (4, 64))
        x172=x156.view(x171)
        x173=x172.permute(0, 2, 1, 3)
        x174=x162.transpose(-1, -2)
        x175=torch.matmul(x173, x174)
        x176=operator.truediv(x175, 8.0)
        x178=operator.add(x176, self._tensor_constant20)
        x179=torch.nn.functional.softmax(x178,dim=-1, _stacklevel=3, dtype=None)
        x180=self.dropout10(x179)
        x181=torch.matmul(x180, x168)
        x182=x181.permute(0, 2, 1, 3)
        x183=x182.contiguous()
        x184=x183.size()
        x185=operator.getitem(x184, slice(None, -2, None))
        x186=operator.add(x185, (256,))
        x187=x183.view(x186)
        x188=self.linear22(x187)
        x189=self.dropout11(x188)
        x190=operator.add(x189, x155)
        x191=self.layernorm7(x190)
        x192=self.linear23(x191)
        x193=torch._C._nn.gelu(x192)
        x194=self.linear24(x193)
        x195=self.dropout12(x194)
        x196=operator.add(x195, x191)
        x197=self.layernorm8(x196)
        x198=self.linear25(x197)
        x199=self.linear26(x197)
        x200=x199.size()
        x201=operator.getitem(x200, slice(None, -1, None))
        x202=operator.add(x201, (4, 64))
        x203=x199.view(x202)
        x204=x203.permute(0, 2, 1, 3)
        x205=self.linear27(x197)
        x206=x205.size()
        x207=operator.getitem(x206, slice(None, -1, None))
        x208=operator.add(x207, (4, 64))
        x209=x205.view(x208)
        x210=x209.permute(0, 2, 1, 3)
        x211=x198.size()
        x212=operator.getitem(x211, slice(None, -1, None))
        x213=operator.add(x212, (4, 64))
        x214=x198.view(x213)
        x215=x214.permute(0, 2, 1, 3)
        x216=x204.transpose(-1, -2)
        x217=torch.matmul(x215, x216)
        x218=operator.truediv(x217, 8.0)
        x220=operator.add(x218, self._tensor_constant20)
        x221=torch.nn.functional.softmax(x220,dim=-1, _stacklevel=3, dtype=None)
        x222=self.dropout13(x221)
        x223=torch.matmul(x222, x210)
        x224=x223.permute(0, 2, 1, 3)
        x225=x224.contiguous()
        x226=x225.size()
        x227=operator.getitem(x226, slice(None, -2, None))
        x228=operator.add(x227, (256,))
        x229=x225.view(x228)
        x230=self.linear28(x229)
        x231=self.dropout14(x230)
        x232=operator.add(x231, x197)
        x233=self.layernorm9(x232)
        x234=self.linear29(x233)
        x235=torch._C._nn.gelu(x234)
        x236=self.linear30(x235)
        x237=self.dropout15(x236)
        x238=operator.add(x237, x233)
        x239=self.layernorm10(x238)
        x240=self.linear31(x239)
        x241=self.linear32(x239)
        x242=x241.size()
        x243=operator.getitem(x242, slice(None, -1, None))
        x244=operator.add(x243, (4, 64))
        x245=x241.view(x244)
        x246=x245.permute(0, 2, 1, 3)
        x247=self.linear33(x239)
        x248=x247.size()
        x249=operator.getitem(x248, slice(None, -1, None))
        x250=operator.add(x249, (4, 64))
        x251=x247.view(x250)
        x252=x251.permute(0, 2, 1, 3)
        x253=x240.size()
        x254=operator.getitem(x253, slice(None, -1, None))
        x255=operator.add(x254, (4, 64))
        x256=x240.view(x255)
        x257=x256.permute(0, 2, 1, 3)
        x258=x246.transpose(-1, -2)
        x259=torch.matmul(x257, x258)
        x260=operator.truediv(x259, 8.0)
        x262=operator.add(x260, self._tensor_constant20)
        x263=torch.nn.functional.softmax(x262,dim=-1, _stacklevel=3, dtype=None)
        x264=self.dropout16(x263)
        x265=torch.matmul(x264, x252)
        x266=x265.permute(0, 2, 1, 3)
        x267=x266.contiguous()
        x268=x267.size()
        x269=operator.getitem(x268, slice(None, -2, None))
        x270=operator.add(x269, (256,))
        x271=x267.view(x270)
        x272=self.linear34(x271)
        x273=self.dropout17(x272)
        x274=operator.add(x273, x239)
        x275=self.layernorm11(x274)
        x276=self.linear35(x275)
        x277=torch._C._nn.gelu(x276)
        x278=self.linear36(x277)
        x279=self.dropout18(x278)
        x280=operator.add(x279, x275)
        x281=self.layernorm12(x280)
        x282=self.linear37(x281)
        x283=self.linear38(x281)
        x284=x283.size()
        x285=operator.getitem(x284, slice(None, -1, None))
        x286=operator.add(x285, (4, 64))
        x287=x283.view(x286)
        x288=x287.permute(0, 2, 1, 3)
        x289=self.linear39(x281)
        x290=x289.size()
        x291=operator.getitem(x290, slice(None, -1, None))
        x292=operator.add(x291, (4, 64))
        x293=x289.view(x292)
        x294=x293.permute(0, 2, 1, 3)
        x295=x282.size()
        x296=operator.getitem(x295, slice(None, -1, None))
        x297=operator.add(x296, (4, 64))
        x298=x282.view(x297)
        x299=x298.permute(0, 2, 1, 3)
        x300=x288.transpose(-1, -2)
        x301=torch.matmul(x299, x300)
        x302=operator.truediv(x301, 8.0)
        x304=operator.add(x302, self._tensor_constant20)
        x305=torch.nn.functional.softmax(x304,dim=-1, _stacklevel=3, dtype=None)
        x306=self.dropout19(x305)
        x307=torch.matmul(x306, x294)
        x308=x307.permute(0, 2, 1, 3)
        x309=x308.contiguous()
        x310=x309.size()
        x311=operator.getitem(x310, slice(None, -2, None))
        x312=operator.add(x311, (256,))
        x313=x309.view(x312)
        x314=self.linear40(x313)
        x315=self.dropout20(x314)
        x316=operator.add(x315, x281)
        x317=self.layernorm13(x316)
        x318=self.linear41(x317)
        x319=torch._C._nn.gelu(x318)
        x320=self.linear42(x319)
        x321=self.dropout21(x320)
        x322=operator.add(x321, x317)
        x323=self.layernorm14(x322)
        x324=self.linear43(x323)
        x325=self.linear44(x323)
        x326=x325.size()
        x327=operator.getitem(x326, slice(None, -1, None))
        x328=operator.add(x327, (4, 64))
        x329=x325.view(x328)
        x330=x329.permute(0, 2, 1, 3)
        x331=self.linear45(x323)
        x332=x331.size()
        x333=operator.getitem(x332, slice(None, -1, None))
        x334=operator.add(x333, (4, 64))
        x335=x331.view(x334)
        x336=x335.permute(0, 2, 1, 3)
        x337=x324.size()
        x338=operator.getitem(x337, slice(None, -1, None))
        x339=operator.add(x338, (4, 64))
        x340=x324.view(x339)
        x341=x340.permute(0, 2, 1, 3)
        x342=x330.transpose(-1, -2)
        x343=torch.matmul(x341, x342)
        x344=operator.truediv(x343, 8.0)
        x346=operator.add(x344, self._tensor_constant20)
        x347=torch.nn.functional.softmax(x346,dim=-1, _stacklevel=3, dtype=None)
        x348=self.dropout22(x347)
        x349=torch.matmul(x348, x336)
        x350=x349.permute(0, 2, 1, 3)
        x351=x350.contiguous()
        x352=x351.size()
        x353=operator.getitem(x352, slice(None, -2, None))
        x354=operator.add(x353, (256,))
        x355=x351.view(x354)
        x356=self.linear46(x355)
        x357=self.dropout23(x356)
        x358=operator.add(x357, x323)
        x359=self.layernorm15(x358)
        x360=self.linear47(x359)
        x361=torch._C._nn.gelu(x360)
        x362=self.linear48(x361)
        x363=self.dropout24(x362)
        x364=operator.add(x363, x359)
        x365=self.layernorm16(x364)
        x366=self.linear49(x365)
        x367=self.linear50(x365)
        x368=x367.size()
        x369=operator.getitem(x368, slice(None, -1, None))
        x370=operator.add(x369, (4, 64))
        x371=x367.view(x370)
        x372=x371.permute(0, 2, 1, 3)
        x373=self.linear51(x365)
        x374=x373.size()
        x375=operator.getitem(x374, slice(None, -1, None))
        x376=operator.add(x375, (4, 64))
        x377=x373.view(x376)
        x378=x377.permute(0, 2, 1, 3)
        x379=x366.size()
        x380=operator.getitem(x379, slice(None, -1, None))
        x381=operator.add(x380, (4, 64))
        x382=x366.view(x381)
        x383=x382.permute(0, 2, 1, 3)
        x384=x372.transpose(-1, -2)
        x385=torch.matmul(x383, x384)
        x386=operator.truediv(x385, 8.0)
        x388=operator.add(x386, self._tensor_constant20)
        x389=torch.nn.functional.softmax(x388,dim=-1, _stacklevel=3, dtype=None)
        x390=self.dropout25(x389)
        x391=torch.matmul(x390, x378)
        x392=x391.permute(0, 2, 1, 3)
        x393=x392.contiguous()
        x394=x393.size()
        x395=operator.getitem(x394, slice(None, -2, None))
        x396=operator.add(x395, (256,))
        x397=x393.view(x396)
        x398=self.linear52(x397)
        x399=self.dropout26(x398)
        x400=operator.add(x399, x365)
        x401=self.layernorm17(x400)
        x402=self.linear53(x401)
        x403=torch._C._nn.gelu(x402)
        x404=self.linear54(x403)
        x405=self.dropout27(x404)
        x406=operator.add(x405, x401)
        x407=self.layernorm18(x406)
        x408=self.linear55(x407)
        x409=self.linear56(x407)
        x410=x409.size()
        x411=operator.getitem(x410, slice(None, -1, None))
        x412=operator.add(x411, (4, 64))
        x413=x409.view(x412)
        x414=x413.permute(0, 2, 1, 3)
        x415=self.linear57(x407)
        x416=x415.size()
        x417=operator.getitem(x416, slice(None, -1, None))
        x418=operator.add(x417, (4, 64))
        x419=x415.view(x418)
        x420=x419.permute(0, 2, 1, 3)
        x421=x408.size()
        x422=operator.getitem(x421, slice(None, -1, None))
        x423=operator.add(x422, (4, 64))
        x424=x408.view(x423)
        x425=x424.permute(0, 2, 1, 3)
        x426=x414.transpose(-1, -2)
        x427=torch.matmul(x425, x426)
        x428=operator.truediv(x427, 8.0)
        x430=operator.add(x428, self._tensor_constant20)
        x431=torch.nn.functional.softmax(x430,dim=-1, _stacklevel=3, dtype=None)
        x432=self.dropout28(x431)
        x433=torch.matmul(x432, x420)
        x434=x433.permute(0, 2, 1, 3)
        x435=x434.contiguous()
        x436=x435.size()
        x437=operator.getitem(x436, slice(None, -2, None))
        x438=operator.add(x437, (256,))
        x439=x435.view(x438)
        x440=self.linear58(x439)
        x441=self.dropout29(x440)
        x442=operator.add(x441, x407)
        x443=self.layernorm19(x442)
        x444=self.linear59(x443)
        x445=torch._C._nn.gelu(x444)
        x446=self.linear60(x445)
        x447=self.dropout30(x446)
        x448=operator.add(x447, x443)
        x449=self.layernorm20(x448)
        x450=self.linear61(x449)
        x451=self.linear62(x449)
        x452=x451.size()
        x453=operator.getitem(x452, slice(None, -1, None))
        x454=operator.add(x453, (4, 64))
        x455=x451.view(x454)
        x456=x455.permute(0, 2, 1, 3)
        x457=self.linear63(x449)
        x458=x457.size()
        x459=operator.getitem(x458, slice(None, -1, None))
        x460=operator.add(x459, (4, 64))
        x461=x457.view(x460)
        x462=x461.permute(0, 2, 1, 3)
        x463=x450.size()
        x464=operator.getitem(x463, slice(None, -1, None))
        x465=operator.add(x464, (4, 64))
        x466=x450.view(x465)
        x467=x466.permute(0, 2, 1, 3)
        x468=x456.transpose(-1, -2)
        x469=torch.matmul(x467, x468)
        x470=operator.truediv(x469, 8.0)
        x472=operator.add(x470, self._tensor_constant20)
        x473=torch.nn.functional.softmax(x472,dim=-1, _stacklevel=3, dtype=None)
        x474=self.dropout31(x473)
        x475=torch.matmul(x474, x462)
        x476=x475.permute(0, 2, 1, 3)
        x477=x476.contiguous()
        x478=x477.size()
        x479=operator.getitem(x478, slice(None, -2, None))
        x480=operator.add(x479, (256,))
        x481=x477.view(x480)
        x482=self.linear64(x481)
        x483=self.dropout32(x482)
        x484=operator.add(x483, x449)
        x485=self.layernorm21(x484)
        x486=self.linear65(x485)
        x487=torch._C._nn.gelu(x486)
        x488=self.linear66(x487)
        x489=self.dropout33(x488)
        x490=operator.add(x489, x485)
        x491=self.layernorm22(x490)
        x492=self.linear67(x491)
        x493=self.linear68(x491)
        x494=x493.size()
        x495=operator.getitem(x494, slice(None, -1, None))
        x496=operator.add(x495, (4, 64))
        x497=x493.view(x496)
        x498=x497.permute(0, 2, 1, 3)
        x499=self.linear69(x491)
        x500=x499.size()
        x501=operator.getitem(x500, slice(None, -1, None))
        x502=operator.add(x501, (4, 64))
        x503=x499.view(x502)
        x504=x503.permute(0, 2, 1, 3)
        x505=x492.size()
        x506=operator.getitem(x505, slice(None, -1, None))
        x507=operator.add(x506, (4, 64))
        x508=x492.view(x507)
        x509=x508.permute(0, 2, 1, 3)
        x510=x498.transpose(-1, -2)
        x511=torch.matmul(x509, x510)
        x512=operator.truediv(x511, 8.0)
        x514=operator.add(x512, self._tensor_constant20)
        x515=torch.nn.functional.softmax(x514,dim=-1, _stacklevel=3, dtype=None)
        x516=self.dropout34(x515)
        x517=torch.matmul(x516, x504)
        x518=x517.permute(0, 2, 1, 3)
        x519=x518.contiguous()
        x520=x519.size()
        x521=operator.getitem(x520, slice(None, -2, None))
        x522=operator.add(x521, (256,))
        x523=x519.view(x522)
        x524=self.linear70(x523)
        x525=self.dropout35(x524)
        x526=operator.add(x525, x491)
        x527=self.layernorm23(x526)
        x528=self.linear71(x527)
        x529=torch._C._nn.gelu(x528)
        x530=self.linear72(x529)
        x531=self.dropout36(x530)
        x532=operator.add(x531, x527)
        x533=self.layernorm24(x532)
        x534=self.linear73(x533)
        x535=x534.split(1,dim=-1)
        x536=operator.getitem(x535, 0)
        x537=operator.getitem(x535, 1)
        x538=x536.squeeze(-1)
        x539=x538.contiguous()
        x540=x537.squeeze(-1)
        x541=x540.contiguous()

m = M().eval()
CORES=os.popen("lscpu | grep Core | awk '{print $4}'").readlines()
SOCKETS=os.popen("lscpu | grep Socket | awk '{print $2}'").readlines()
BS=int(CORES[0])*int(SOCKETS[0])
batch_size=BS
input_ids_1 = torch.ones((1, 384), dtype=torch.long)
attention_mask_1 = torch.ones((1, 384), dtype=torch.long)
token_type_ids_1 = torch.ones((1, 384), dtype=torch.long)
position_ids_1 = None
head_mask_1 = None
inputs_embeds_1 = None
start_positions_1 = None
end_positions_1 = None
output_attentions_1 = None
output_hidden_states_1 = None
return_dict_1 = None
def print_throughput(flag):
    start_time=time.time()
    for i in range(10):
        output = m(input_ids_1, attention_mask_1, token_type_ids_1, position_ids_1, head_mask_1, inputs_embeds_1, start_positions_1, end_positions_1, output_attentions_1, output_hidden_states_1, return_dict_1)
    total_iter_time = time.time() - start_time
    Throughput = batch_size * 10 / total_iter_time
    file_current = os.path.basename(__file__)
    print(file_current,',',BS,',',flag,',',Throughput)
for flag in {False,True}:
    torch._C._jit_set_texpr_fuser_enabled(flag)
    print_throughput(flag)
