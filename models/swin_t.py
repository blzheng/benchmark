import torch
from torch import tensor
import torch.nn as nn
from torch.nn import *
import torchvision
import torchvision.models as models
from torchvision.ops.stochastic_depth import stochastic_depth
import time
import builtins
import operator

class M(torch.nn.Module):
    def __init__(self):
        super(M, self).__init__()
        self.conv2d0 = Conv2d(3, 96, kernel_size=(4, 4), stride=(4, 4))
        self.layernorm0 = LayerNorm((96,), eps=1e-05, elementwise_affine=True)
        self.layernorm1 = LayerNorm((96,), eps=1e-05, elementwise_affine=True)
        self.layernorm2 = LayerNorm((96,), eps=1e-05, elementwise_affine=True)
        self.linear0 = Linear(in_features=96, out_features=384, bias=True)
        self.gelu0 = GELU(approximate='none')
        self.dropout0 = Dropout(p=0.0, inplace=False)
        self.linear1 = Linear(in_features=384, out_features=96, bias=True)
        self.dropout1 = Dropout(p=0.0, inplace=False)
        self.layernorm3 = LayerNorm((96,), eps=1e-05, elementwise_affine=True)
        self.layernorm4 = LayerNorm((96,), eps=1e-05, elementwise_affine=True)
        self.linear2 = Linear(in_features=96, out_features=384, bias=True)
        self.gelu1 = GELU(approximate='none')
        self.dropout2 = Dropout(p=0.0, inplace=False)
        self.linear3 = Linear(in_features=384, out_features=96, bias=True)
        self.dropout3 = Dropout(p=0.0, inplace=False)
        self.layernorm5 = LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        self.linear4 = Linear(in_features=384, out_features=192, bias=False)
        self.layernorm6 = LayerNorm((192,), eps=1e-05, elementwise_affine=True)
        self.layernorm7 = LayerNorm((192,), eps=1e-05, elementwise_affine=True)
        self.linear5 = Linear(in_features=192, out_features=768, bias=True)
        self.gelu2 = GELU(approximate='none')
        self.dropout4 = Dropout(p=0.0, inplace=False)
        self.linear6 = Linear(in_features=768, out_features=192, bias=True)
        self.dropout5 = Dropout(p=0.0, inplace=False)
        self.layernorm8 = LayerNorm((192,), eps=1e-05, elementwise_affine=True)
        self.layernorm9 = LayerNorm((192,), eps=1e-05, elementwise_affine=True)
        self.linear7 = Linear(in_features=192, out_features=768, bias=True)
        self.gelu3 = GELU(approximate='none')
        self.dropout6 = Dropout(p=0.0, inplace=False)
        self.linear8 = Linear(in_features=768, out_features=192, bias=True)
        self.dropout7 = Dropout(p=0.0, inplace=False)
        self.layernorm10 = LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        self.linear9 = Linear(in_features=768, out_features=384, bias=False)
        self.layernorm11 = LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        self.layernorm12 = LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        self.linear10 = Linear(in_features=384, out_features=1536, bias=True)
        self.gelu4 = GELU(approximate='none')
        self.dropout8 = Dropout(p=0.0, inplace=False)
        self.linear11 = Linear(in_features=1536, out_features=384, bias=True)
        self.dropout9 = Dropout(p=0.0, inplace=False)
        self.layernorm13 = LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        self.layernorm14 = LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        self.linear12 = Linear(in_features=384, out_features=1536, bias=True)
        self.gelu5 = GELU(approximate='none')
        self.dropout10 = Dropout(p=0.0, inplace=False)
        self.linear13 = Linear(in_features=1536, out_features=384, bias=True)
        self.dropout11 = Dropout(p=0.0, inplace=False)
        self.layernorm15 = LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        self.layernorm16 = LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        self.linear14 = Linear(in_features=384, out_features=1536, bias=True)
        self.gelu6 = GELU(approximate='none')
        self.dropout12 = Dropout(p=0.0, inplace=False)
        self.linear15 = Linear(in_features=1536, out_features=384, bias=True)
        self.dropout13 = Dropout(p=0.0, inplace=False)
        self.layernorm17 = LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        self.layernorm18 = LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        self.linear16 = Linear(in_features=384, out_features=1536, bias=True)
        self.gelu7 = GELU(approximate='none')
        self.dropout14 = Dropout(p=0.0, inplace=False)
        self.linear17 = Linear(in_features=1536, out_features=384, bias=True)
        self.dropout15 = Dropout(p=0.0, inplace=False)
        self.layernorm19 = LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        self.layernorm20 = LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        self.linear18 = Linear(in_features=384, out_features=1536, bias=True)
        self.gelu8 = GELU(approximate='none')
        self.dropout16 = Dropout(p=0.0, inplace=False)
        self.linear19 = Linear(in_features=1536, out_features=384, bias=True)
        self.dropout17 = Dropout(p=0.0, inplace=False)
        self.layernorm21 = LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        self.layernorm22 = LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        self.linear20 = Linear(in_features=384, out_features=1536, bias=True)
        self.gelu9 = GELU(approximate='none')
        self.dropout18 = Dropout(p=0.0, inplace=False)
        self.linear21 = Linear(in_features=1536, out_features=384, bias=True)
        self.dropout19 = Dropout(p=0.0, inplace=False)
        self.layernorm23 = LayerNorm((1536,), eps=1e-05, elementwise_affine=True)
        self.linear22 = Linear(in_features=1536, out_features=768, bias=False)
        self.layernorm24 = LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        self.layernorm25 = LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        self.linear23 = Linear(in_features=768, out_features=3072, bias=True)
        self.gelu10 = GELU(approximate='none')
        self.dropout20 = Dropout(p=0.0, inplace=False)
        self.linear24 = Linear(in_features=3072, out_features=768, bias=True)
        self.dropout21 = Dropout(p=0.0, inplace=False)
        self.layernorm26 = LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        self.layernorm27 = LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        self.linear25 = Linear(in_features=768, out_features=3072, bias=True)
        self.gelu11 = GELU(approximate='none')
        self.dropout22 = Dropout(p=0.0, inplace=False)
        self.linear26 = Linear(in_features=3072, out_features=768, bias=True)
        self.dropout23 = Dropout(p=0.0, inplace=False)
        self.layernorm28 = LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        self.adaptiveavgpool2d0 = AdaptiveAvgPool2d(output_size=1)
        self.linear27 = Linear(in_features=768, out_features=1000, bias=True)
        self.relative_position_bias_table0 = torch.rand(torch.Size([169, 3])).to(torch.float32)
        self.relative_position_index0 = torch.rand(torch.Size([2401])).to(torch.int64)
        self.weight0 = torch.rand(torch.Size([288, 96])).to(torch.float32)
        self.weight1 = torch.rand(torch.Size([96, 96])).to(torch.float32)
        self.bias0 = torch.rand(torch.Size([288])).to(torch.float32)
        self.bias1 = torch.rand(torch.Size([96])).to(torch.float32)
        self.relative_position_bias_table1 = torch.rand(torch.Size([169, 3])).to(torch.float32)
        self.relative_position_index1 = torch.rand(torch.Size([2401])).to(torch.int64)
        self.weight2 = torch.rand(torch.Size([288, 96])).to(torch.float32)
        self.weight3 = torch.rand(torch.Size([96, 96])).to(torch.float32)
        self.bias2 = torch.rand(torch.Size([288])).to(torch.float32)
        self.bias3 = torch.rand(torch.Size([96])).to(torch.float32)
        self.relative_position_bias_table2 = torch.rand(torch.Size([169, 6])).to(torch.float32)
        self.relative_position_index2 = torch.rand(torch.Size([2401])).to(torch.int64)
        self.weight4 = torch.rand(torch.Size([576, 192])).to(torch.float32)
        self.weight5 = torch.rand(torch.Size([192, 192])).to(torch.float32)
        self.bias4 = torch.rand(torch.Size([576])).to(torch.float32)
        self.bias5 = torch.rand(torch.Size([192])).to(torch.float32)
        self.relative_position_bias_table3 = torch.rand(torch.Size([169, 6])).to(torch.float32)
        self.relative_position_index3 = torch.rand(torch.Size([2401])).to(torch.int64)
        self.weight6 = torch.rand(torch.Size([576, 192])).to(torch.float32)
        self.weight7 = torch.rand(torch.Size([192, 192])).to(torch.float32)
        self.bias6 = torch.rand(torch.Size([576])).to(torch.float32)
        self.bias7 = torch.rand(torch.Size([192])).to(torch.float32)
        self.relative_position_bias_table4 = torch.rand(torch.Size([169, 12])).to(torch.float32)
        self.relative_position_index4 = torch.rand(torch.Size([2401])).to(torch.int64)
        self.weight8 = torch.rand(torch.Size([1152, 384])).to(torch.float32)
        self.weight9 = torch.rand(torch.Size([384, 384])).to(torch.float32)
        self.bias8 = torch.rand(torch.Size([1152])).to(torch.float32)
        self.bias9 = torch.rand(torch.Size([384])).to(torch.float32)
        self.relative_position_bias_table5 = torch.rand(torch.Size([169, 12])).to(torch.float32)
        self.relative_position_index5 = torch.rand(torch.Size([2401])).to(torch.int64)
        self.weight10 = torch.rand(torch.Size([1152, 384])).to(torch.float32)
        self.weight11 = torch.rand(torch.Size([384, 384])).to(torch.float32)
        self.bias10 = torch.rand(torch.Size([1152])).to(torch.float32)
        self.bias11 = torch.rand(torch.Size([384])).to(torch.float32)
        self.relative_position_bias_table6 = torch.rand(torch.Size([169, 12])).to(torch.float32)
        self.relative_position_index6 = torch.rand(torch.Size([2401])).to(torch.int64)
        self.weight12 = torch.rand(torch.Size([1152, 384])).to(torch.float32)
        self.weight13 = torch.rand(torch.Size([384, 384])).to(torch.float32)
        self.bias12 = torch.rand(torch.Size([1152])).to(torch.float32)
        self.bias13 = torch.rand(torch.Size([384])).to(torch.float32)
        self.relative_position_bias_table7 = torch.rand(torch.Size([169, 12])).to(torch.float32)
        self.relative_position_index7 = torch.rand(torch.Size([2401])).to(torch.int64)
        self.weight14 = torch.rand(torch.Size([1152, 384])).to(torch.float32)
        self.weight15 = torch.rand(torch.Size([384, 384])).to(torch.float32)
        self.bias14 = torch.rand(torch.Size([1152])).to(torch.float32)
        self.bias15 = torch.rand(torch.Size([384])).to(torch.float32)
        self.relative_position_bias_table8 = torch.rand(torch.Size([169, 12])).to(torch.float32)
        self.relative_position_index8 = torch.rand(torch.Size([2401])).to(torch.int64)
        self.weight16 = torch.rand(torch.Size([1152, 384])).to(torch.float32)
        self.weight17 = torch.rand(torch.Size([384, 384])).to(torch.float32)
        self.bias16 = torch.rand(torch.Size([1152])).to(torch.float32)
        self.bias17 = torch.rand(torch.Size([384])).to(torch.float32)
        self.relative_position_bias_table9 = torch.rand(torch.Size([169, 12])).to(torch.float32)
        self.relative_position_index9 = torch.rand(torch.Size([2401])).to(torch.int64)
        self.weight18 = torch.rand(torch.Size([1152, 384])).to(torch.float32)
        self.weight19 = torch.rand(torch.Size([384, 384])).to(torch.float32)
        self.bias18 = torch.rand(torch.Size([1152])).to(torch.float32)
        self.bias19 = torch.rand(torch.Size([384])).to(torch.float32)
        self.relative_position_bias_table10 = torch.rand(torch.Size([169, 24])).to(torch.float32)
        self.relative_position_index10 = torch.rand(torch.Size([2401])).to(torch.int64)
        self.weight20 = torch.rand(torch.Size([2304, 768])).to(torch.float32)
        self.weight21 = torch.rand(torch.Size([768, 768])).to(torch.float32)
        self.bias20 = torch.rand(torch.Size([2304])).to(torch.float32)
        self.bias21 = torch.rand(torch.Size([768])).to(torch.float32)
        self.relative_position_bias_table11 = torch.rand(torch.Size([169, 24])).to(torch.float32)
        self.relative_position_index11 = torch.rand(torch.Size([2401])).to(torch.int64)
        self.weight22 = torch.rand(torch.Size([2304, 768])).to(torch.float32)
        self.weight23 = torch.rand(torch.Size([768, 768])).to(torch.float32)
        self.bias22 = torch.rand(torch.Size([2304])).to(torch.float32)
        self.bias23 = torch.rand(torch.Size([768])).to(torch.float32)

    def forward(self, x):
        x0=x
        x1=self.conv2d0(x0)
        x2=torch.permute(x1, [0, 2, 3, 1])
        x3=self.layernorm0(x2)
        x4=self.layernorm1(x3)
        x7=operator.getitem(self.relative_position_bias_table0, self.relative_position_index0)
        x8=x7.view(49, 49, -1)
        x9=x8.permute(2, 0, 1)
        x10=x9.contiguous()
        x11=x10.unsqueeze(0)
        x16=torchvision.models.swin_transformer.shifted_window_attention(x4, self.weight0, self.weight1, x11, [7, 7], 3,shift_size=[0, 0], attention_dropout=0.0, dropout=0.0, qkv_bias=self.bias0, proj_bias=self.bias1)
        x17=stochastic_depth(x16, 0.0, 'row', False)
        x18=operator.add(x3, x17)
        x19=self.layernorm2(x18)
        x20=self.linear0(x19)
        x21=self.gelu0(x20)
        x22=self.dropout0(x21)
        x23=self.linear1(x22)
        x24=self.dropout1(x23)
        x25=stochastic_depth(x24, 0.0, 'row', False)
        x26=operator.add(x18, x25)
        x27=self.layernorm3(x26)
        x30=operator.getitem(self.relative_position_bias_table1, self.relative_position_index1)
        x31=x30.view(49, 49, -1)
        x32=x31.permute(2, 0, 1)
        x33=x32.contiguous()
        x34=x33.unsqueeze(0)
        x39=torchvision.models.swin_transformer.shifted_window_attention(x27, self.weight2, self.weight3, x34, [7, 7], 3,shift_size=[3, 3], attention_dropout=0.0, dropout=0.0, qkv_bias=self.bias2, proj_bias=self.bias3)
        x40=stochastic_depth(x39, 0.018181818181818184, 'row', False)
        x41=operator.add(x26, x40)
        x42=self.layernorm4(x41)
        x43=self.linear2(x42)
        x44=self.gelu1(x43)
        x45=self.dropout2(x44)
        x46=self.linear3(x45)
        x47=self.dropout3(x46)
        x48=stochastic_depth(x47, 0.018181818181818184, 'row', False)
        x49=operator.add(x41, x48)
        x50=torchvision.models.swin_transformer._patch_merging_pad(x49)
        x51=operator.getitem(x50, (Ellipsis, slice(0, None, 2), slice(0, None, 2), slice(None, None, None)))
        x52=operator.getitem(x50, (Ellipsis, slice(1, None, 2), slice(0, None, 2), slice(None, None, None)))
        x53=operator.getitem(x50, (Ellipsis, slice(0, None, 2), slice(1, None, 2), slice(None, None, None)))
        x54=operator.getitem(x50, (Ellipsis, slice(1, None, 2), slice(1, None, 2), slice(None, None, None)))
        x55=torch.cat([x51, x52, x53, x54], -1)
        x56=self.layernorm5(x55)
        x57=self.linear4(x56)
        x58=self.layernorm6(x57)
        x61=operator.getitem(self.relative_position_bias_table2, self.relative_position_index2)
        x62=x61.view(49, 49, -1)
        x63=x62.permute(2, 0, 1)
        x64=x63.contiguous()
        x65=x64.unsqueeze(0)
        x70=torchvision.models.swin_transformer.shifted_window_attention(x58, self.weight4, self.weight5, x65, [7, 7], 6,shift_size=[0, 0], attention_dropout=0.0, dropout=0.0, qkv_bias=self.bias4, proj_bias=self.bias5)
        x71=stochastic_depth(x70, 0.03636363636363637, 'row', False)
        x72=operator.add(x57, x71)
        x73=self.layernorm7(x72)
        x74=self.linear5(x73)
        x75=self.gelu2(x74)
        x76=self.dropout4(x75)
        x77=self.linear6(x76)
        x78=self.dropout5(x77)
        x79=stochastic_depth(x78, 0.03636363636363637, 'row', False)
        x80=operator.add(x72, x79)
        x81=self.layernorm8(x80)
        x84=operator.getitem(self.relative_position_bias_table3, self.relative_position_index3)
        x85=x84.view(49, 49, -1)
        x86=x85.permute(2, 0, 1)
        x87=x86.contiguous()
        x88=x87.unsqueeze(0)
        x93=torchvision.models.swin_transformer.shifted_window_attention(x81, self.weight6, self.weight7, x88, [7, 7], 6,shift_size=[3, 3], attention_dropout=0.0, dropout=0.0, qkv_bias=self.bias6, proj_bias=self.bias7)
        x94=stochastic_depth(x93, 0.05454545454545456, 'row', False)
        x95=operator.add(x80, x94)
        x96=self.layernorm9(x95)
        x97=self.linear7(x96)
        x98=self.gelu3(x97)
        x99=self.dropout6(x98)
        x100=self.linear8(x99)
        x101=self.dropout7(x100)
        x102=stochastic_depth(x101, 0.05454545454545456, 'row', False)
        x103=operator.add(x95, x102)
        x104=torchvision.models.swin_transformer._patch_merging_pad(x103)
        x105=operator.getitem(x104, (Ellipsis, slice(0, None, 2), slice(0, None, 2), slice(None, None, None)))
        x106=operator.getitem(x104, (Ellipsis, slice(1, None, 2), slice(0, None, 2), slice(None, None, None)))
        x107=operator.getitem(x104, (Ellipsis, slice(0, None, 2), slice(1, None, 2), slice(None, None, None)))
        x108=operator.getitem(x104, (Ellipsis, slice(1, None, 2), slice(1, None, 2), slice(None, None, None)))
        x109=torch.cat([x105, x106, x107, x108], -1)
        x110=self.layernorm10(x109)
        x111=self.linear9(x110)
        x112=self.layernorm11(x111)
        x115=operator.getitem(self.relative_position_bias_table4, self.relative_position_index4)
        x116=x115.view(49, 49, -1)
        x117=x116.permute(2, 0, 1)
        x118=x117.contiguous()
        x119=x118.unsqueeze(0)
        x124=torchvision.models.swin_transformer.shifted_window_attention(x112, self.weight8, self.weight9, x119, [7, 7], 12,shift_size=[0, 0], attention_dropout=0.0, dropout=0.0, qkv_bias=self.bias8, proj_bias=self.bias9)
        x125=stochastic_depth(x124, 0.07272727272727274, 'row', False)
        x126=operator.add(x111, x125)
        x127=self.layernorm12(x126)
        x128=self.linear10(x127)
        x129=self.gelu4(x128)
        x130=self.dropout8(x129)
        x131=self.linear11(x130)
        x132=self.dropout9(x131)
        x133=stochastic_depth(x132, 0.07272727272727274, 'row', False)
        x134=operator.add(x126, x133)
        x135=self.layernorm13(x134)
        x138=operator.getitem(self.relative_position_bias_table5, self.relative_position_index5)
        x139=x138.view(49, 49, -1)
        x140=x139.permute(2, 0, 1)
        x141=x140.contiguous()
        x142=x141.unsqueeze(0)
        x147=torchvision.models.swin_transformer.shifted_window_attention(x135, self.weight10, self.weight11, x142, [7, 7], 12,shift_size=[3, 3], attention_dropout=0.0, dropout=0.0, qkv_bias=self.bias10, proj_bias=self.bias11)
        x148=stochastic_depth(x147, 0.09090909090909091, 'row', False)
        x149=operator.add(x134, x148)
        x150=self.layernorm14(x149)
        x151=self.linear12(x150)
        x152=self.gelu5(x151)
        x153=self.dropout10(x152)
        x154=self.linear13(x153)
        x155=self.dropout11(x154)
        x156=stochastic_depth(x155, 0.09090909090909091, 'row', False)
        x157=operator.add(x149, x156)
        x158=self.layernorm15(x157)
        x161=operator.getitem(self.relative_position_bias_table6, self.relative_position_index6)
        x162=x161.view(49, 49, -1)
        x163=x162.permute(2, 0, 1)
        x164=x163.contiguous()
        x165=x164.unsqueeze(0)
        x170=torchvision.models.swin_transformer.shifted_window_attention(x158, self.weight12, self.weight13, x165, [7, 7], 12,shift_size=[0, 0], attention_dropout=0.0, dropout=0.0, qkv_bias=self.bias12, proj_bias=self.bias13)
        x171=stochastic_depth(x170, 0.10909090909090911, 'row', False)
        x172=operator.add(x157, x171)
        x173=self.layernorm16(x172)
        x174=self.linear14(x173)
        x175=self.gelu6(x174)
        x176=self.dropout12(x175)
        x177=self.linear15(x176)
        x178=self.dropout13(x177)
        x179=stochastic_depth(x178, 0.10909090909090911, 'row', False)
        x180=operator.add(x172, x179)
        x181=self.layernorm17(x180)
        x184=operator.getitem(self.relative_position_bias_table7, self.relative_position_index7)
        x185=x184.view(49, 49, -1)
        x186=x185.permute(2, 0, 1)
        x187=x186.contiguous()
        x188=x187.unsqueeze(0)
        x193=torchvision.models.swin_transformer.shifted_window_attention(x181, self.weight14, self.weight15, x188, [7, 7], 12,shift_size=[3, 3], attention_dropout=0.0, dropout=0.0, qkv_bias=self.bias14, proj_bias=self.bias15)
        x194=stochastic_depth(x193, 0.1272727272727273, 'row', False)
        x195=operator.add(x180, x194)
        x196=self.layernorm18(x195)
        x197=self.linear16(x196)
        x198=self.gelu7(x197)
        x199=self.dropout14(x198)
        x200=self.linear17(x199)
        x201=self.dropout15(x200)
        x202=stochastic_depth(x201, 0.1272727272727273, 'row', False)
        x203=operator.add(x195, x202)
        x204=self.layernorm19(x203)
        x207=operator.getitem(self.relative_position_bias_table8, self.relative_position_index8)
        x208=x207.view(49, 49, -1)
        x209=x208.permute(2, 0, 1)
        x210=x209.contiguous()
        x211=x210.unsqueeze(0)
        x216=torchvision.models.swin_transformer.shifted_window_attention(x204, self.weight16, self.weight17, x211, [7, 7], 12,shift_size=[0, 0], attention_dropout=0.0, dropout=0.0, qkv_bias=self.bias16, proj_bias=self.bias17)
        x217=stochastic_depth(x216, 0.14545454545454548, 'row', False)
        x218=operator.add(x203, x217)
        x219=self.layernorm20(x218)
        x220=self.linear18(x219)
        x221=self.gelu8(x220)
        x222=self.dropout16(x221)
        x223=self.linear19(x222)
        x224=self.dropout17(x223)
        x225=stochastic_depth(x224, 0.14545454545454548, 'row', False)
        x226=operator.add(x218, x225)
        x227=self.layernorm21(x226)
        x230=operator.getitem(self.relative_position_bias_table9, self.relative_position_index9)
        x231=x230.view(49, 49, -1)
        x232=x231.permute(2, 0, 1)
        x233=x232.contiguous()
        x234=x233.unsqueeze(0)
        x239=torchvision.models.swin_transformer.shifted_window_attention(x227, self.weight18, self.weight19, x234, [7, 7], 12,shift_size=[3, 3], attention_dropout=0.0, dropout=0.0, qkv_bias=self.bias18, proj_bias=self.bias19)
        x240=stochastic_depth(x239, 0.16363636363636364, 'row', False)
        x241=operator.add(x226, x240)
        x242=self.layernorm22(x241)
        x243=self.linear20(x242)
        x244=self.gelu9(x243)
        x245=self.dropout18(x244)
        x246=self.linear21(x245)
        x247=self.dropout19(x246)
        x248=stochastic_depth(x247, 0.16363636363636364, 'row', False)
        x249=operator.add(x241, x248)
        x250=torchvision.models.swin_transformer._patch_merging_pad(x249)
        x251=operator.getitem(x250, (Ellipsis, slice(0, None, 2), slice(0, None, 2), slice(None, None, None)))
        x252=operator.getitem(x250, (Ellipsis, slice(1, None, 2), slice(0, None, 2), slice(None, None, None)))
        x253=operator.getitem(x250, (Ellipsis, slice(0, None, 2), slice(1, None, 2), slice(None, None, None)))
        x254=operator.getitem(x250, (Ellipsis, slice(1, None, 2), slice(1, None, 2), slice(None, None, None)))
        x255=torch.cat([x251, x252, x253, x254], -1)
        x256=self.layernorm23(x255)
        x257=self.linear22(x256)
        x258=self.layernorm24(x257)
        x261=operator.getitem(self.relative_position_bias_table10, self.relative_position_index10)
        x262=x261.view(49, 49, -1)
        x263=x262.permute(2, 0, 1)
        x264=x263.contiguous()
        x265=x264.unsqueeze(0)
        x270=torchvision.models.swin_transformer.shifted_window_attention(x258, self.weight20, self.weight21, x265, [7, 7], 24,shift_size=[0, 0], attention_dropout=0.0, dropout=0.0, qkv_bias=self.bias20, proj_bias=self.bias21)
        x271=stochastic_depth(x270, 0.18181818181818182, 'row', False)
        x272=operator.add(x257, x271)
        x273=self.layernorm25(x272)
        x274=self.linear23(x273)
        x275=self.gelu10(x274)
        x276=self.dropout20(x275)
        x277=self.linear24(x276)
        x278=self.dropout21(x277)
        x279=stochastic_depth(x278, 0.18181818181818182, 'row', False)
        x280=operator.add(x272, x279)
        x281=self.layernorm26(x280)
        x284=operator.getitem(self.relative_position_bias_table11, self.relative_position_index11)
        x285=x284.view(49, 49, -1)
        x286=x285.permute(2, 0, 1)
        x287=x286.contiguous()
        x288=x287.unsqueeze(0)
        x293=torchvision.models.swin_transformer.shifted_window_attention(x281, self.weight22, self.weight23, x288, [7, 7], 24,shift_size=[3, 3], attention_dropout=0.0, dropout=0.0, qkv_bias=self.bias22, proj_bias=self.bias23)
        x294=stochastic_depth(x293, 0.2, 'row', False)
        x295=operator.add(x280, x294)
        x296=self.layernorm27(x295)
        x297=self.linear25(x296)
        x298=self.gelu11(x297)
        x299=self.dropout22(x298)
        x300=self.linear26(x299)
        x301=self.dropout23(x300)
        x302=stochastic_depth(x301, 0.2, 'row', False)
        x303=operator.add(x295, x302)
        x304=self.layernorm28(x303)
        x305=x304.permute(0, 3, 1, 2)
        x306=self.adaptiveavgpool2d0(x305)
        x307=torch.flatten(x306, 1)
        x308=self.linear27(x307)

m = M().eval()
x = torch.rand(1, 3, 224, 224)
start = time.time()
output = m(x)
end = time.time()
print(end-start)
