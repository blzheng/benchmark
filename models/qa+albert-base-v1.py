import torch
from torch import tensor
import torch.nn as nn
from torch.nn import *
import torchvision
import torchvision.models as models
from torchvision.ops.stochastic_depth import stochastic_depth
import time
import builtins
import operator
import sys
import os

class M(torch.nn.Module):
    def __init__(self):
        super(M, self).__init__()
        self.embedding0 = Embedding(30000, 128, padding_idx=0)
        self.embedding1 = Embedding(2, 128)
        self.embedding2 = Embedding(512, 128)
        self.layernorm0 = LayerNorm((128,), eps=1e-12, elementwise_affine=True)
        self.dropout0 = Dropout(p=0.1, inplace=False)
        self.linear0 = Linear(in_features=128, out_features=768, bias=True)
        self.linear1 = Linear(in_features=768, out_features=768, bias=True)
        self.linear2 = Linear(in_features=768, out_features=768, bias=True)
        self.linear3 = Linear(in_features=768, out_features=768, bias=True)
        self.dropout1 = Dropout(p=0.1, inplace=False)
        self.linear4 = Linear(in_features=768, out_features=768, bias=True)
        self.dropout2 = Dropout(p=0.1, inplace=False)
        self.layernorm1 = LayerNorm((768,), eps=1e-12, elementwise_affine=True)
        self.linear5 = Linear(in_features=768, out_features=3072, bias=True)
        self.linear6 = Linear(in_features=3072, out_features=768, bias=True)
        self.layernorm2 = LayerNorm((768,), eps=1e-12, elementwise_affine=True)
        self.linear7 = Linear(in_features=768, out_features=768, bias=True)
        self.linear8 = Linear(in_features=768, out_features=768, bias=True)
        self.linear9 = Linear(in_features=768, out_features=768, bias=True)
        self.dropout3 = Dropout(p=0.1, inplace=False)
        self.linear10 = Linear(in_features=768, out_features=768, bias=True)
        self.dropout4 = Dropout(p=0.1, inplace=False)
        self.layernorm3 = LayerNorm((768,), eps=1e-12, elementwise_affine=True)
        self.linear11 = Linear(in_features=768, out_features=3072, bias=True)
        self.linear12 = Linear(in_features=3072, out_features=768, bias=True)
        self.layernorm4 = LayerNorm((768,), eps=1e-12, elementwise_affine=True)
        self.linear13 = Linear(in_features=768, out_features=768, bias=True)
        self.linear14 = Linear(in_features=768, out_features=768, bias=True)
        self.linear15 = Linear(in_features=768, out_features=768, bias=True)
        self.dropout5 = Dropout(p=0.1, inplace=False)
        self.linear16 = Linear(in_features=768, out_features=768, bias=True)
        self.dropout6 = Dropout(p=0.1, inplace=False)
        self.layernorm5 = LayerNorm((768,), eps=1e-12, elementwise_affine=True)
        self.linear17 = Linear(in_features=768, out_features=3072, bias=True)
        self.linear18 = Linear(in_features=3072, out_features=768, bias=True)
        self.layernorm6 = LayerNorm((768,), eps=1e-12, elementwise_affine=True)
        self.linear19 = Linear(in_features=768, out_features=768, bias=True)
        self.linear20 = Linear(in_features=768, out_features=768, bias=True)
        self.linear21 = Linear(in_features=768, out_features=768, bias=True)
        self.dropout7 = Dropout(p=0.1, inplace=False)
        self.linear22 = Linear(in_features=768, out_features=768, bias=True)
        self.dropout8 = Dropout(p=0.1, inplace=False)
        self.layernorm7 = LayerNorm((768,), eps=1e-12, elementwise_affine=True)
        self.linear23 = Linear(in_features=768, out_features=3072, bias=True)
        self.linear24 = Linear(in_features=3072, out_features=768, bias=True)
        self.layernorm8 = LayerNorm((768,), eps=1e-12, elementwise_affine=True)
        self.linear25 = Linear(in_features=768, out_features=768, bias=True)
        self.linear26 = Linear(in_features=768, out_features=768, bias=True)
        self.linear27 = Linear(in_features=768, out_features=768, bias=True)
        self.dropout9 = Dropout(p=0.1, inplace=False)
        self.linear28 = Linear(in_features=768, out_features=768, bias=True)
        self.dropout10 = Dropout(p=0.1, inplace=False)
        self.layernorm9 = LayerNorm((768,), eps=1e-12, elementwise_affine=True)
        self.linear29 = Linear(in_features=768, out_features=3072, bias=True)
        self.linear30 = Linear(in_features=3072, out_features=768, bias=True)
        self.layernorm10 = LayerNorm((768,), eps=1e-12, elementwise_affine=True)
        self.linear31 = Linear(in_features=768, out_features=768, bias=True)
        self.linear32 = Linear(in_features=768, out_features=768, bias=True)
        self.linear33 = Linear(in_features=768, out_features=768, bias=True)
        self.dropout11 = Dropout(p=0.1, inplace=False)
        self.linear34 = Linear(in_features=768, out_features=768, bias=True)
        self.dropout12 = Dropout(p=0.1, inplace=False)
        self.layernorm11 = LayerNorm((768,), eps=1e-12, elementwise_affine=True)
        self.linear35 = Linear(in_features=768, out_features=3072, bias=True)
        self.linear36 = Linear(in_features=3072, out_features=768, bias=True)
        self.layernorm12 = LayerNorm((768,), eps=1e-12, elementwise_affine=True)
        self.linear37 = Linear(in_features=768, out_features=768, bias=True)
        self.linear38 = Linear(in_features=768, out_features=768, bias=True)
        self.linear39 = Linear(in_features=768, out_features=768, bias=True)
        self.dropout13 = Dropout(p=0.1, inplace=False)
        self.linear40 = Linear(in_features=768, out_features=768, bias=True)
        self.dropout14 = Dropout(p=0.1, inplace=False)
        self.layernorm13 = LayerNorm((768,), eps=1e-12, elementwise_affine=True)
        self.linear41 = Linear(in_features=768, out_features=3072, bias=True)
        self.linear42 = Linear(in_features=3072, out_features=768, bias=True)
        self.layernorm14 = LayerNorm((768,), eps=1e-12, elementwise_affine=True)
        self.linear43 = Linear(in_features=768, out_features=768, bias=True)
        self.linear44 = Linear(in_features=768, out_features=768, bias=True)
        self.linear45 = Linear(in_features=768, out_features=768, bias=True)
        self.dropout15 = Dropout(p=0.1, inplace=False)
        self.linear46 = Linear(in_features=768, out_features=768, bias=True)
        self.dropout16 = Dropout(p=0.1, inplace=False)
        self.layernorm15 = LayerNorm((768,), eps=1e-12, elementwise_affine=True)
        self.linear47 = Linear(in_features=768, out_features=3072, bias=True)
        self.linear48 = Linear(in_features=3072, out_features=768, bias=True)
        self.layernorm16 = LayerNorm((768,), eps=1e-12, elementwise_affine=True)
        self.linear49 = Linear(in_features=768, out_features=768, bias=True)
        self.linear50 = Linear(in_features=768, out_features=768, bias=True)
        self.linear51 = Linear(in_features=768, out_features=768, bias=True)
        self.dropout17 = Dropout(p=0.1, inplace=False)
        self.linear52 = Linear(in_features=768, out_features=768, bias=True)
        self.dropout18 = Dropout(p=0.1, inplace=False)
        self.layernorm17 = LayerNorm((768,), eps=1e-12, elementwise_affine=True)
        self.linear53 = Linear(in_features=768, out_features=3072, bias=True)
        self.linear54 = Linear(in_features=3072, out_features=768, bias=True)
        self.layernorm18 = LayerNorm((768,), eps=1e-12, elementwise_affine=True)
        self.linear55 = Linear(in_features=768, out_features=768, bias=True)
        self.linear56 = Linear(in_features=768, out_features=768, bias=True)
        self.linear57 = Linear(in_features=768, out_features=768, bias=True)
        self.dropout19 = Dropout(p=0.1, inplace=False)
        self.linear58 = Linear(in_features=768, out_features=768, bias=True)
        self.dropout20 = Dropout(p=0.1, inplace=False)
        self.layernorm19 = LayerNorm((768,), eps=1e-12, elementwise_affine=True)
        self.linear59 = Linear(in_features=768, out_features=3072, bias=True)
        self.linear60 = Linear(in_features=3072, out_features=768, bias=True)
        self.layernorm20 = LayerNorm((768,), eps=1e-12, elementwise_affine=True)
        self.linear61 = Linear(in_features=768, out_features=768, bias=True)
        self.linear62 = Linear(in_features=768, out_features=768, bias=True)
        self.linear63 = Linear(in_features=768, out_features=768, bias=True)
        self.dropout21 = Dropout(p=0.1, inplace=False)
        self.linear64 = Linear(in_features=768, out_features=768, bias=True)
        self.dropout22 = Dropout(p=0.1, inplace=False)
        self.layernorm21 = LayerNorm((768,), eps=1e-12, elementwise_affine=True)
        self.linear65 = Linear(in_features=768, out_features=3072, bias=True)
        self.linear66 = Linear(in_features=3072, out_features=768, bias=True)
        self.layernorm22 = LayerNorm((768,), eps=1e-12, elementwise_affine=True)
        self.linear67 = Linear(in_features=768, out_features=768, bias=True)
        self.linear68 = Linear(in_features=768, out_features=768, bias=True)
        self.linear69 = Linear(in_features=768, out_features=768, bias=True)
        self.dropout23 = Dropout(p=0.1, inplace=False)
        self.linear70 = Linear(in_features=768, out_features=768, bias=True)
        self.dropout24 = Dropout(p=0.1, inplace=False)
        self.layernorm23 = LayerNorm((768,), eps=1e-12, elementwise_affine=True)
        self.linear71 = Linear(in_features=768, out_features=3072, bias=True)
        self.linear72 = Linear(in_features=3072, out_features=768, bias=True)
        self.layernorm24 = LayerNorm((768,), eps=1e-12, elementwise_affine=True)
        self.linear73 = Linear(in_features=768, out_features=2, bias=True)
        self._tensor_constant00 = torch.rand(torch.Size([1, 384])).to(torch.int64)
        self._tensor_constant01 = torch.rand(torch.Size([1, 384])).to(torch.int64)
        self._tensor_constant10 = torch.rand(torch.Size([1, 384])).to(torch.int64)
        self._tensor_constant20 = torch.rand(torch.Size([1, 1, 1, 384])).to(torch.float32)
        self._tensor_constant21 = torch.rand(torch.Size([1, 1, 1, 384])).to(torch.float32)
        self._tensor_constant22 = torch.rand(torch.Size([1, 1, 1, 384])).to(torch.float32)
        self._tensor_constant23 = torch.rand(torch.Size([1, 1, 1, 384])).to(torch.float32)
        self._tensor_constant24 = torch.rand(torch.Size([1, 1, 1, 384])).to(torch.float32)
        self._tensor_constant25 = torch.rand(torch.Size([1, 1, 1, 384])).to(torch.float32)
        self._tensor_constant26 = torch.rand(torch.Size([1, 1, 1, 384])).to(torch.float32)
        self._tensor_constant27 = torch.rand(torch.Size([1, 1, 1, 384])).to(torch.float32)
        self._tensor_constant28 = torch.rand(torch.Size([1, 1, 1, 384])).to(torch.float32)
        self._tensor_constant29 = torch.rand(torch.Size([1, 1, 1, 384])).to(torch.float32)
        self._tensor_constant210 = torch.rand(torch.Size([1, 1, 1, 384])).to(torch.float32)
        self._tensor_constant211 = torch.rand(torch.Size([1, 1, 1, 384])).to(torch.float32)

    def forward(self, input_ids_1, attention_mask_1, token_type_ids_1, position_ids_1, head_mask_1, inputs_embeds_1, start_positions_1, end_positions_1, output_attentions_1, output_hidden_states_1, return_dict_1):
        x0=input_ids_1
        x1=attention_mask_1
        x2=token_type_ids_1
        x3=position_ids_1
        x4=torch.fx._symbolic_trace._assert_is_none(x3, 'position_ids has been specialized to have value None but got another value')
        x5=head_mask_1
        x6=torch.fx._symbolic_trace._assert_is_none(x5, 'head_mask has been specialized to have value None but got another value')
        x7=inputs_embeds_1
        x8=torch.fx._symbolic_trace._assert_is_none(x7, 'inputs_embeds has been specialized to have value None but got another value')
        x9=start_positions_1
        x10=torch.fx._symbolic_trace._assert_is_none(x9, 'start_positions has been specialized to have value None but got another value')
        x11=end_positions_1
        x12=torch.fx._symbolic_trace._assert_is_none(x11, 'end_positions has been specialized to have value None but got another value')
        x13=output_attentions_1
        x14=torch.fx._symbolic_trace._assert_is_none(x13, 'output_attentions has been specialized to have value None but got another value')
        x15=output_hidden_states_1
        x16=torch.fx._symbolic_trace._assert_is_none(x15, 'output_hidden_states has been specialized to have value None but got another value')
        x17=return_dict_1
        x18=torch.fx._symbolic_trace._assert_is_none(x17, 'return_dict has been specialized to have value None but got another value')
        x20=self.embedding0(self._tensor_constant00)
        x22=self.embedding1(self._tensor_constant00)
        x23=operator.add(x20, x22)
        x25=self.embedding2(self._tensor_constant10)
        x26=operator.add(x23, x25)
        x27=self.layernorm0(x26)
        x28=self.dropout0(x27)
        x29=self.linear0(x28)
        x30=self.linear1(x29)
        x31=self.linear2(x29)
        x32=self.linear3(x29)
        x33=x30.size()
        x34=operator.getitem(x33, slice(None, -1, None))
        x35=operator.add(x34, (12, 64))
        x36=x30.view(x35)
        x37=x36.permute(0, 2, 1, 3)
        x38=x31.size()
        x39=operator.getitem(x38, slice(None, -1, None))
        x40=operator.add(x39, (12, 64))
        x41=x31.view(x40)
        x42=x41.permute(0, 2, 1, 3)
        x43=x32.size()
        x44=operator.getitem(x43, slice(None, -1, None))
        x45=operator.add(x44, (12, 64))
        x46=x32.view(x45)
        x47=x46.permute(0, 2, 1, 3)
        x48=x42.transpose(-1, -2)
        x49=torch.matmul(x37, x48)
        x50=operator.truediv(x49, 8.0)
        x52=operator.add(x50, self._tensor_constant20)
        x53=torch.nn.functional.softmax(x52,dim=-1, _stacklevel=3, dtype=None)
        x54=self.dropout1(x53)
        x55=torch.matmul(x54, x47)
        x56=x55.transpose(2, 1)
        x57=x56.flatten(2)
        x58=self.linear4(x57)
        x59=self.dropout2(x58)
        x60=operator.add(x29, x59)
        x61=self.layernorm1(x60)
        x62=self.linear5(x61)
        x63=torch._C._nn.gelu(x62)
        x64=self.linear6(x63)
        x65=operator.add(x64, x61)
        x66=self.layernorm2(x65)
        x67=self.linear1(x66)
        x68=self.linear2(x66)
        x69=self.linear3(x66)
        x70=x67.size()
        x71=operator.getitem(x70, slice(None, -1, None))
        x72=operator.add(x71, (12, 64))
        x73=x67.view(x72)
        x74=x73.permute(0, 2, 1, 3)
        x75=x68.size()
        x76=operator.getitem(x75, slice(None, -1, None))
        x77=operator.add(x76, (12, 64))
        x78=x68.view(x77)
        x79=x78.permute(0, 2, 1, 3)
        x80=x69.size()
        x81=operator.getitem(x80, slice(None, -1, None))
        x82=operator.add(x81, (12, 64))
        x83=x69.view(x82)
        x84=x83.permute(0, 2, 1, 3)
        x85=x79.transpose(-1, -2)
        x86=torch.matmul(x74, x85)
        x87=operator.truediv(x86, 8.0)
        x89=operator.add(x87, self._tensor_constant20)
        x90=torch.nn.functional.softmax(x89,dim=-1, _stacklevel=3, dtype=None)
        x91=self.dropout1(x90)
        x92=torch.matmul(x91, x84)
        x93=x92.transpose(2, 1)
        x94=x93.flatten(2)
        x95=self.linear4(x94)
        x96=self.dropout2(x95)
        x97=operator.add(x66, x96)
        x98=self.layernorm1(x97)
        x99=self.linear5(x98)
        x100=torch._C._nn.gelu(x99)
        x101=self.linear6(x100)
        x102=operator.add(x101, x98)
        x103=self.layernorm2(x102)
        x104=self.linear1(x103)
        x105=self.linear2(x103)
        x106=self.linear3(x103)
        x107=x104.size()
        x108=operator.getitem(x107, slice(None, -1, None))
        x109=operator.add(x108, (12, 64))
        x110=x104.view(x109)
        x111=x110.permute(0, 2, 1, 3)
        x112=x105.size()
        x113=operator.getitem(x112, slice(None, -1, None))
        x114=operator.add(x113, (12, 64))
        x115=x105.view(x114)
        x116=x115.permute(0, 2, 1, 3)
        x117=x106.size()
        x118=operator.getitem(x117, slice(None, -1, None))
        x119=operator.add(x118, (12, 64))
        x120=x106.view(x119)
        x121=x120.permute(0, 2, 1, 3)
        x122=x116.transpose(-1, -2)
        x123=torch.matmul(x111, x122)
        x124=operator.truediv(x123, 8.0)
        x126=operator.add(x124, self._tensor_constant20)
        x127=torch.nn.functional.softmax(x126,dim=-1, _stacklevel=3, dtype=None)
        x128=self.dropout1(x127)
        x129=torch.matmul(x128, x121)
        x130=x129.transpose(2, 1)
        x131=x130.flatten(2)
        x132=self.linear4(x131)
        x133=self.dropout2(x132)
        x134=operator.add(x103, x133)
        x135=self.layernorm1(x134)
        x136=self.linear5(x135)
        x137=torch._C._nn.gelu(x136)
        x138=self.linear6(x137)
        x139=operator.add(x138, x135)
        x140=self.layernorm2(x139)
        x141=self.linear1(x140)
        x142=self.linear2(x140)
        x143=self.linear3(x140)
        x144=x141.size()
        x145=operator.getitem(x144, slice(None, -1, None))
        x146=operator.add(x145, (12, 64))
        x147=x141.view(x146)
        x148=x147.permute(0, 2, 1, 3)
        x149=x142.size()
        x150=operator.getitem(x149, slice(None, -1, None))
        x151=operator.add(x150, (12, 64))
        x152=x142.view(x151)
        x153=x152.permute(0, 2, 1, 3)
        x154=x143.size()
        x155=operator.getitem(x154, slice(None, -1, None))
        x156=operator.add(x155, (12, 64))
        x157=x143.view(x156)
        x158=x157.permute(0, 2, 1, 3)
        x159=x153.transpose(-1, -2)
        x160=torch.matmul(x148, x159)
        x161=operator.truediv(x160, 8.0)
        x163=operator.add(x161, self._tensor_constant20)
        x164=torch.nn.functional.softmax(x163,dim=-1, _stacklevel=3, dtype=None)
        x165=self.dropout1(x164)
        x166=torch.matmul(x165, x158)
        x167=x166.transpose(2, 1)
        x168=x167.flatten(2)
        x169=self.linear4(x168)
        x170=self.dropout2(x169)
        x171=operator.add(x140, x170)
        x172=self.layernorm1(x171)
        x173=self.linear5(x172)
        x174=torch._C._nn.gelu(x173)
        x175=self.linear6(x174)
        x176=operator.add(x175, x172)
        x177=self.layernorm2(x176)
        x178=self.linear1(x177)
        x179=self.linear2(x177)
        x180=self.linear3(x177)
        x181=x178.size()
        x182=operator.getitem(x181, slice(None, -1, None))
        x183=operator.add(x182, (12, 64))
        x184=x178.view(x183)
        x185=x184.permute(0, 2, 1, 3)
        x186=x179.size()
        x187=operator.getitem(x186, slice(None, -1, None))
        x188=operator.add(x187, (12, 64))
        x189=x179.view(x188)
        x190=x189.permute(0, 2, 1, 3)
        x191=x180.size()
        x192=operator.getitem(x191, slice(None, -1, None))
        x193=operator.add(x192, (12, 64))
        x194=x180.view(x193)
        x195=x194.permute(0, 2, 1, 3)
        x196=x190.transpose(-1, -2)
        x197=torch.matmul(x185, x196)
        x198=operator.truediv(x197, 8.0)
        x200=operator.add(x198, self._tensor_constant20)
        x201=torch.nn.functional.softmax(x200,dim=-1, _stacklevel=3, dtype=None)
        x202=self.dropout1(x201)
        x203=torch.matmul(x202, x195)
        x204=x203.transpose(2, 1)
        x205=x204.flatten(2)
        x206=self.linear4(x205)
        x207=self.dropout2(x206)
        x208=operator.add(x177, x207)
        x209=self.layernorm1(x208)
        x210=self.linear5(x209)
        x211=torch._C._nn.gelu(x210)
        x212=self.linear6(x211)
        x213=operator.add(x212, x209)
        x214=self.layernorm2(x213)
        x215=self.linear1(x214)
        x216=self.linear2(x214)
        x217=self.linear3(x214)
        x218=x215.size()
        x219=operator.getitem(x218, slice(None, -1, None))
        x220=operator.add(x219, (12, 64))
        x221=x215.view(x220)
        x222=x221.permute(0, 2, 1, 3)
        x223=x216.size()
        x224=operator.getitem(x223, slice(None, -1, None))
        x225=operator.add(x224, (12, 64))
        x226=x216.view(x225)
        x227=x226.permute(0, 2, 1, 3)
        x228=x217.size()
        x229=operator.getitem(x228, slice(None, -1, None))
        x230=operator.add(x229, (12, 64))
        x231=x217.view(x230)
        x232=x231.permute(0, 2, 1, 3)
        x233=x227.transpose(-1, -2)
        x234=torch.matmul(x222, x233)
        x235=operator.truediv(x234, 8.0)
        x237=operator.add(x235, self._tensor_constant20)
        x238=torch.nn.functional.softmax(x237,dim=-1, _stacklevel=3, dtype=None)
        x239=self.dropout1(x238)
        x240=torch.matmul(x239, x232)
        x241=x240.transpose(2, 1)
        x242=x241.flatten(2)
        x243=self.linear4(x242)
        x244=self.dropout2(x243)
        x245=operator.add(x214, x244)
        x246=self.layernorm1(x245)
        x247=self.linear5(x246)
        x248=torch._C._nn.gelu(x247)
        x249=self.linear6(x248)
        x250=operator.add(x249, x246)
        x251=self.layernorm2(x250)
        x252=self.linear1(x251)
        x253=self.linear2(x251)
        x254=self.linear3(x251)
        x255=x252.size()
        x256=operator.getitem(x255, slice(None, -1, None))
        x257=operator.add(x256, (12, 64))
        x258=x252.view(x257)
        x259=x258.permute(0, 2, 1, 3)
        x260=x253.size()
        x261=operator.getitem(x260, slice(None, -1, None))
        x262=operator.add(x261, (12, 64))
        x263=x253.view(x262)
        x264=x263.permute(0, 2, 1, 3)
        x265=x254.size()
        x266=operator.getitem(x265, slice(None, -1, None))
        x267=operator.add(x266, (12, 64))
        x268=x254.view(x267)
        x269=x268.permute(0, 2, 1, 3)
        x270=x264.transpose(-1, -2)
        x271=torch.matmul(x259, x270)
        x272=operator.truediv(x271, 8.0)
        x274=operator.add(x272, self._tensor_constant20)
        x275=torch.nn.functional.softmax(x274,dim=-1, _stacklevel=3, dtype=None)
        x276=self.dropout1(x275)
        x277=torch.matmul(x276, x269)
        x278=x277.transpose(2, 1)
        x279=x278.flatten(2)
        x280=self.linear4(x279)
        x281=self.dropout2(x280)
        x282=operator.add(x251, x281)
        x283=self.layernorm1(x282)
        x284=self.linear5(x283)
        x285=torch._C._nn.gelu(x284)
        x286=self.linear6(x285)
        x287=operator.add(x286, x283)
        x288=self.layernorm2(x287)
        x289=self.linear1(x288)
        x290=self.linear2(x288)
        x291=self.linear3(x288)
        x292=x289.size()
        x293=operator.getitem(x292, slice(None, -1, None))
        x294=operator.add(x293, (12, 64))
        x295=x289.view(x294)
        x296=x295.permute(0, 2, 1, 3)
        x297=x290.size()
        x298=operator.getitem(x297, slice(None, -1, None))
        x299=operator.add(x298, (12, 64))
        x300=x290.view(x299)
        x301=x300.permute(0, 2, 1, 3)
        x302=x291.size()
        x303=operator.getitem(x302, slice(None, -1, None))
        x304=operator.add(x303, (12, 64))
        x305=x291.view(x304)
        x306=x305.permute(0, 2, 1, 3)
        x307=x301.transpose(-1, -2)
        x308=torch.matmul(x296, x307)
        x309=operator.truediv(x308, 8.0)
        x311=operator.add(x309, self._tensor_constant20)
        x312=torch.nn.functional.softmax(x311,dim=-1, _stacklevel=3, dtype=None)
        x313=self.dropout1(x312)
        x314=torch.matmul(x313, x306)
        x315=x314.transpose(2, 1)
        x316=x315.flatten(2)
        x317=self.linear4(x316)
        x318=self.dropout2(x317)
        x319=operator.add(x288, x318)
        x320=self.layernorm1(x319)
        x321=self.linear5(x320)
        x322=torch._C._nn.gelu(x321)
        x323=self.linear6(x322)
        x324=operator.add(x323, x320)
        x325=self.layernorm2(x324)
        x326=self.linear1(x325)
        x327=self.linear2(x325)
        x328=self.linear3(x325)
        x329=x326.size()
        x330=operator.getitem(x329, slice(None, -1, None))
        x331=operator.add(x330, (12, 64))
        x332=x326.view(x331)
        x333=x332.permute(0, 2, 1, 3)
        x334=x327.size()
        x335=operator.getitem(x334, slice(None, -1, None))
        x336=operator.add(x335, (12, 64))
        x337=x327.view(x336)
        x338=x337.permute(0, 2, 1, 3)
        x339=x328.size()
        x340=operator.getitem(x339, slice(None, -1, None))
        x341=operator.add(x340, (12, 64))
        x342=x328.view(x341)
        x343=x342.permute(0, 2, 1, 3)
        x344=x338.transpose(-1, -2)
        x345=torch.matmul(x333, x344)
        x346=operator.truediv(x345, 8.0)
        x348=operator.add(x346, self._tensor_constant20)
        x349=torch.nn.functional.softmax(x348,dim=-1, _stacklevel=3, dtype=None)
        x350=self.dropout1(x349)
        x351=torch.matmul(x350, x343)
        x352=x351.transpose(2, 1)
        x353=x352.flatten(2)
        x354=self.linear4(x353)
        x355=self.dropout2(x354)
        x356=operator.add(x325, x355)
        x357=self.layernorm1(x356)
        x358=self.linear5(x357)
        x359=torch._C._nn.gelu(x358)
        x360=self.linear6(x359)
        x361=operator.add(x360, x357)
        x362=self.layernorm2(x361)
        x363=self.linear1(x362)
        x364=self.linear2(x362)
        x365=self.linear3(x362)
        x366=x363.size()
        x367=operator.getitem(x366, slice(None, -1, None))
        x368=operator.add(x367, (12, 64))
        x369=x363.view(x368)
        x370=x369.permute(0, 2, 1, 3)
        x371=x364.size()
        x372=operator.getitem(x371, slice(None, -1, None))
        x373=operator.add(x372, (12, 64))
        x374=x364.view(x373)
        x375=x374.permute(0, 2, 1, 3)
        x376=x365.size()
        x377=operator.getitem(x376, slice(None, -1, None))
        x378=operator.add(x377, (12, 64))
        x379=x365.view(x378)
        x380=x379.permute(0, 2, 1, 3)
        x381=x375.transpose(-1, -2)
        x382=torch.matmul(x370, x381)
        x383=operator.truediv(x382, 8.0)
        x385=operator.add(x383, self._tensor_constant20)
        x386=torch.nn.functional.softmax(x385,dim=-1, _stacklevel=3, dtype=None)
        x387=self.dropout1(x386)
        x388=torch.matmul(x387, x380)
        x389=x388.transpose(2, 1)
        x390=x389.flatten(2)
        x391=self.linear4(x390)
        x392=self.dropout2(x391)
        x393=operator.add(x362, x392)
        x394=self.layernorm1(x393)
        x395=self.linear5(x394)
        x396=torch._C._nn.gelu(x395)
        x397=self.linear6(x396)
        x398=operator.add(x397, x394)
        x399=self.layernorm2(x398)
        x400=self.linear1(x399)
        x401=self.linear2(x399)
        x402=self.linear3(x399)
        x403=x400.size()
        x404=operator.getitem(x403, slice(None, -1, None))
        x405=operator.add(x404, (12, 64))
        x406=x400.view(x405)
        x407=x406.permute(0, 2, 1, 3)
        x408=x401.size()
        x409=operator.getitem(x408, slice(None, -1, None))
        x410=operator.add(x409, (12, 64))
        x411=x401.view(x410)
        x412=x411.permute(0, 2, 1, 3)
        x413=x402.size()
        x414=operator.getitem(x413, slice(None, -1, None))
        x415=operator.add(x414, (12, 64))
        x416=x402.view(x415)
        x417=x416.permute(0, 2, 1, 3)
        x418=x412.transpose(-1, -2)
        x419=torch.matmul(x407, x418)
        x420=operator.truediv(x419, 8.0)
        x422=operator.add(x420, self._tensor_constant20)
        x423=torch.nn.functional.softmax(x422,dim=-1, _stacklevel=3, dtype=None)
        x424=self.dropout1(x423)
        x425=torch.matmul(x424, x417)
        x426=x425.transpose(2, 1)
        x427=x426.flatten(2)
        x428=self.linear4(x427)
        x429=self.dropout2(x428)
        x430=operator.add(x399, x429)
        x431=self.layernorm1(x430)
        x432=self.linear5(x431)
        x433=torch._C._nn.gelu(x432)
        x434=self.linear6(x433)
        x435=operator.add(x434, x431)
        x436=self.layernorm2(x435)
        x437=self.linear1(x436)
        x438=self.linear2(x436)
        x439=self.linear3(x436)
        x440=x437.size()
        x441=operator.getitem(x440, slice(None, -1, None))
        x442=operator.add(x441, (12, 64))
        x443=x437.view(x442)
        x444=x443.permute(0, 2, 1, 3)
        x445=x438.size()
        x446=operator.getitem(x445, slice(None, -1, None))
        x447=operator.add(x446, (12, 64))
        x448=x438.view(x447)
        x449=x448.permute(0, 2, 1, 3)
        x450=x439.size()
        x451=operator.getitem(x450, slice(None, -1, None))
        x452=operator.add(x451, (12, 64))
        x453=x439.view(x452)
        x454=x453.permute(0, 2, 1, 3)
        x455=x449.transpose(-1, -2)
        x456=torch.matmul(x444, x455)
        x457=operator.truediv(x456, 8.0)
        x459=operator.add(x457, self._tensor_constant20)
        x460=torch.nn.functional.softmax(x459,dim=-1, _stacklevel=3, dtype=None)
        x461=self.dropout1(x460)
        x462=torch.matmul(x461, x454)
        x463=x462.transpose(2, 1)
        x464=x463.flatten(2)
        x465=self.linear4(x464)
        x466=self.dropout2(x465)
        x467=operator.add(x436, x466)
        x468=self.layernorm1(x467)
        x469=self.linear5(x468)
        x470=torch._C._nn.gelu(x469)
        x471=self.linear6(x470)
        x472=operator.add(x471, x468)
        x473=self.layernorm2(x472)
        x474=self.linear73(x473)
        x475=x474.split(1,dim=-1)
        x476=operator.getitem(x475, 0)
        x477=operator.getitem(x475, 1)
        x478=x476.squeeze(-1)
        x479=x478.contiguous()
        x480=x477.squeeze(-1)
        x481=x480.contiguous()

m = M().eval()
CORES=os.popen("lscpu | grep Core | awk '{print $4}'").readlines()
SOCKETS=os.popen("lscpu | grep Socket | awk '{print $2}'").readlines()
BS=int(CORES[0])*int(SOCKETS[0])
batch_size=BS
input_ids_1 = torch.ones((1, 384), dtype=torch.long)
attention_mask_1 = torch.ones((1, 384), dtype=torch.long)
token_type_ids_1 = torch.ones((1, 384), dtype=torch.long)
position_ids_1 = None
head_mask_1 = None
inputs_embeds_1 = None
start_positions_1 = None
end_positions_1 = None
output_attentions_1 = None
output_hidden_states_1 = None
return_dict_1 = None
def print_throughput(flag):
    start_time=time.time()
    for i in range(10):
        output = m(input_ids_1, attention_mask_1, token_type_ids_1, position_ids_1, head_mask_1, inputs_embeds_1, start_positions_1, end_positions_1, output_attentions_1, output_hidden_states_1, return_dict_1)
    total_iter_time = time.time() - start_time
    Throughput = batch_size * 10 / total_iter_time
    file_current = os.path.basename(__file__)
    print(file_current,',',BS,',',flag,',',Throughput)
for flag in {False,True}:
    torch._C._jit_set_texpr_fuser_enabled(flag)
    print_throughput(flag)
