import torch
from torch import tensor
import torch.nn as nn
from torch.nn import *
import torchvision
import torchvision.models as models
from torchvision.ops.stochastic_depth import stochastic_depth
import time
import builtins
import operator

class M(torch.nn.Module):
    def __init__(self):
        super(M, self).__init__()
        self.conv2d0 = Conv2d(3, 96, kernel_size=(4, 4), stride=(4, 4))
        self.layernorm0 = LayerNorm((96,), eps=1e-05, elementwise_affine=True)
        self.layernorm1 = LayerNorm((96,), eps=1e-05, elementwise_affine=True)
        self.layernorm2 = LayerNorm((96,), eps=1e-05, elementwise_affine=True)
        self.linear0 = Linear(in_features=96, out_features=384, bias=True)
        self.gelu0 = GELU(approximate='none')
        self.dropout0 = Dropout(p=0.0, inplace=False)
        self.linear1 = Linear(in_features=384, out_features=96, bias=True)
        self.dropout1 = Dropout(p=0.0, inplace=False)
        self.layernorm3 = LayerNorm((96,), eps=1e-05, elementwise_affine=True)
        self.layernorm4 = LayerNorm((96,), eps=1e-05, elementwise_affine=True)
        self.linear2 = Linear(in_features=96, out_features=384, bias=True)
        self.gelu1 = GELU(approximate='none')
        self.dropout2 = Dropout(p=0.0, inplace=False)
        self.linear3 = Linear(in_features=384, out_features=96, bias=True)
        self.dropout3 = Dropout(p=0.0, inplace=False)
        self.layernorm5 = LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        self.linear4 = Linear(in_features=384, out_features=192, bias=False)
        self.layernorm6 = LayerNorm((192,), eps=1e-05, elementwise_affine=True)
        self.layernorm7 = LayerNorm((192,), eps=1e-05, elementwise_affine=True)
        self.linear5 = Linear(in_features=192, out_features=768, bias=True)
        self.gelu2 = GELU(approximate='none')
        self.dropout4 = Dropout(p=0.0, inplace=False)
        self.linear6 = Linear(in_features=768, out_features=192, bias=True)
        self.dropout5 = Dropout(p=0.0, inplace=False)
        self.layernorm8 = LayerNorm((192,), eps=1e-05, elementwise_affine=True)
        self.layernorm9 = LayerNorm((192,), eps=1e-05, elementwise_affine=True)
        self.linear7 = Linear(in_features=192, out_features=768, bias=True)
        self.gelu3 = GELU(approximate='none')
        self.dropout6 = Dropout(p=0.0, inplace=False)
        self.linear8 = Linear(in_features=768, out_features=192, bias=True)
        self.dropout7 = Dropout(p=0.0, inplace=False)
        self.layernorm10 = LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        self.linear9 = Linear(in_features=768, out_features=384, bias=False)
        self.layernorm11 = LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        self.layernorm12 = LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        self.linear10 = Linear(in_features=384, out_features=1536, bias=True)
        self.gelu4 = GELU(approximate='none')
        self.dropout8 = Dropout(p=0.0, inplace=False)
        self.linear11 = Linear(in_features=1536, out_features=384, bias=True)
        self.dropout9 = Dropout(p=0.0, inplace=False)
        self.layernorm13 = LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        self.layernorm14 = LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        self.linear12 = Linear(in_features=384, out_features=1536, bias=True)
        self.gelu5 = GELU(approximate='none')
        self.dropout10 = Dropout(p=0.0, inplace=False)
        self.linear13 = Linear(in_features=1536, out_features=384, bias=True)
        self.dropout11 = Dropout(p=0.0, inplace=False)
        self.layernorm15 = LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        self.layernorm16 = LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        self.linear14 = Linear(in_features=384, out_features=1536, bias=True)
        self.gelu6 = GELU(approximate='none')
        self.dropout12 = Dropout(p=0.0, inplace=False)
        self.linear15 = Linear(in_features=1536, out_features=384, bias=True)
        self.dropout13 = Dropout(p=0.0, inplace=False)
        self.layernorm17 = LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        self.layernorm18 = LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        self.linear16 = Linear(in_features=384, out_features=1536, bias=True)
        self.gelu7 = GELU(approximate='none')
        self.dropout14 = Dropout(p=0.0, inplace=False)
        self.linear17 = Linear(in_features=1536, out_features=384, bias=True)
        self.dropout15 = Dropout(p=0.0, inplace=False)
        self.layernorm19 = LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        self.layernorm20 = LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        self.linear18 = Linear(in_features=384, out_features=1536, bias=True)
        self.gelu8 = GELU(approximate='none')
        self.dropout16 = Dropout(p=0.0, inplace=False)
        self.linear19 = Linear(in_features=1536, out_features=384, bias=True)
        self.dropout17 = Dropout(p=0.0, inplace=False)
        self.layernorm21 = LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        self.layernorm22 = LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        self.linear20 = Linear(in_features=384, out_features=1536, bias=True)
        self.gelu9 = GELU(approximate='none')
        self.dropout18 = Dropout(p=0.0, inplace=False)
        self.linear21 = Linear(in_features=1536, out_features=384, bias=True)
        self.dropout19 = Dropout(p=0.0, inplace=False)
        self.layernorm23 = LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        self.layernorm24 = LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        self.linear22 = Linear(in_features=384, out_features=1536, bias=True)
        self.gelu10 = GELU(approximate='none')
        self.dropout20 = Dropout(p=0.0, inplace=False)
        self.linear23 = Linear(in_features=1536, out_features=384, bias=True)
        self.dropout21 = Dropout(p=0.0, inplace=False)
        self.layernorm25 = LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        self.layernorm26 = LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        self.linear24 = Linear(in_features=384, out_features=1536, bias=True)
        self.gelu11 = GELU(approximate='none')
        self.dropout22 = Dropout(p=0.0, inplace=False)
        self.linear25 = Linear(in_features=1536, out_features=384, bias=True)
        self.dropout23 = Dropout(p=0.0, inplace=False)
        self.layernorm27 = LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        self.layernorm28 = LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        self.linear26 = Linear(in_features=384, out_features=1536, bias=True)
        self.gelu12 = GELU(approximate='none')
        self.dropout24 = Dropout(p=0.0, inplace=False)
        self.linear27 = Linear(in_features=1536, out_features=384, bias=True)
        self.dropout25 = Dropout(p=0.0, inplace=False)
        self.layernorm29 = LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        self.layernorm30 = LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        self.linear28 = Linear(in_features=384, out_features=1536, bias=True)
        self.gelu13 = GELU(approximate='none')
        self.dropout26 = Dropout(p=0.0, inplace=False)
        self.linear29 = Linear(in_features=1536, out_features=384, bias=True)
        self.dropout27 = Dropout(p=0.0, inplace=False)
        self.layernorm31 = LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        self.layernorm32 = LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        self.linear30 = Linear(in_features=384, out_features=1536, bias=True)
        self.gelu14 = GELU(approximate='none')
        self.dropout28 = Dropout(p=0.0, inplace=False)
        self.linear31 = Linear(in_features=1536, out_features=384, bias=True)
        self.dropout29 = Dropout(p=0.0, inplace=False)
        self.layernorm33 = LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        self.layernorm34 = LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        self.linear32 = Linear(in_features=384, out_features=1536, bias=True)
        self.gelu15 = GELU(approximate='none')
        self.dropout30 = Dropout(p=0.0, inplace=False)
        self.linear33 = Linear(in_features=1536, out_features=384, bias=True)
        self.dropout31 = Dropout(p=0.0, inplace=False)
        self.layernorm35 = LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        self.layernorm36 = LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        self.linear34 = Linear(in_features=384, out_features=1536, bias=True)
        self.gelu16 = GELU(approximate='none')
        self.dropout32 = Dropout(p=0.0, inplace=False)
        self.linear35 = Linear(in_features=1536, out_features=384, bias=True)
        self.dropout33 = Dropout(p=0.0, inplace=False)
        self.layernorm37 = LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        self.layernorm38 = LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        self.linear36 = Linear(in_features=384, out_features=1536, bias=True)
        self.gelu17 = GELU(approximate='none')
        self.dropout34 = Dropout(p=0.0, inplace=False)
        self.linear37 = Linear(in_features=1536, out_features=384, bias=True)
        self.dropout35 = Dropout(p=0.0, inplace=False)
        self.layernorm39 = LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        self.layernorm40 = LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        self.linear38 = Linear(in_features=384, out_features=1536, bias=True)
        self.gelu18 = GELU(approximate='none')
        self.dropout36 = Dropout(p=0.0, inplace=False)
        self.linear39 = Linear(in_features=1536, out_features=384, bias=True)
        self.dropout37 = Dropout(p=0.0, inplace=False)
        self.layernorm41 = LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        self.layernorm42 = LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        self.linear40 = Linear(in_features=384, out_features=1536, bias=True)
        self.gelu19 = GELU(approximate='none')
        self.dropout38 = Dropout(p=0.0, inplace=False)
        self.linear41 = Linear(in_features=1536, out_features=384, bias=True)
        self.dropout39 = Dropout(p=0.0, inplace=False)
        self.layernorm43 = LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        self.layernorm44 = LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        self.linear42 = Linear(in_features=384, out_features=1536, bias=True)
        self.gelu20 = GELU(approximate='none')
        self.dropout40 = Dropout(p=0.0, inplace=False)
        self.linear43 = Linear(in_features=1536, out_features=384, bias=True)
        self.dropout41 = Dropout(p=0.0, inplace=False)
        self.layernorm45 = LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        self.layernorm46 = LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        self.linear44 = Linear(in_features=384, out_features=1536, bias=True)
        self.gelu21 = GELU(approximate='none')
        self.dropout42 = Dropout(p=0.0, inplace=False)
        self.linear45 = Linear(in_features=1536, out_features=384, bias=True)
        self.dropout43 = Dropout(p=0.0, inplace=False)
        self.layernorm47 = LayerNorm((1536,), eps=1e-05, elementwise_affine=True)
        self.linear46 = Linear(in_features=1536, out_features=768, bias=False)
        self.layernorm48 = LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        self.layernorm49 = LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        self.linear47 = Linear(in_features=768, out_features=3072, bias=True)
        self.gelu22 = GELU(approximate='none')
        self.dropout44 = Dropout(p=0.0, inplace=False)
        self.linear48 = Linear(in_features=3072, out_features=768, bias=True)
        self.dropout45 = Dropout(p=0.0, inplace=False)
        self.layernorm50 = LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        self.layernorm51 = LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        self.linear49 = Linear(in_features=768, out_features=3072, bias=True)
        self.gelu23 = GELU(approximate='none')
        self.dropout46 = Dropout(p=0.0, inplace=False)
        self.linear50 = Linear(in_features=3072, out_features=768, bias=True)
        self.dropout47 = Dropout(p=0.0, inplace=False)
        self.layernorm52 = LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        self.adaptiveavgpool2d0 = AdaptiveAvgPool2d(output_size=1)
        self.linear51 = Linear(in_features=768, out_features=1000, bias=True)
        self.relative_position_bias_table0 = torch.rand(torch.Size([169, 3])).to(torch.float32)
        self.relative_position_index0 = torch.rand(torch.Size([2401])).to(torch.int64)
        self.weight0 = torch.rand(torch.Size([288, 96])).to(torch.float32)
        self.weight1 = torch.rand(torch.Size([96, 96])).to(torch.float32)
        self.bias0 = torch.rand(torch.Size([288])).to(torch.float32)
        self.bias1 = torch.rand(torch.Size([96])).to(torch.float32)
        self.relative_position_bias_table1 = torch.rand(torch.Size([169, 3])).to(torch.float32)
        self.relative_position_index1 = torch.rand(torch.Size([2401])).to(torch.int64)
        self.weight2 = torch.rand(torch.Size([288, 96])).to(torch.float32)
        self.weight3 = torch.rand(torch.Size([96, 96])).to(torch.float32)
        self.bias2 = torch.rand(torch.Size([288])).to(torch.float32)
        self.bias3 = torch.rand(torch.Size([96])).to(torch.float32)
        self.relative_position_bias_table2 = torch.rand(torch.Size([169, 6])).to(torch.float32)
        self.relative_position_index2 = torch.rand(torch.Size([2401])).to(torch.int64)
        self.weight4 = torch.rand(torch.Size([576, 192])).to(torch.float32)
        self.weight5 = torch.rand(torch.Size([192, 192])).to(torch.float32)
        self.bias4 = torch.rand(torch.Size([576])).to(torch.float32)
        self.bias5 = torch.rand(torch.Size([192])).to(torch.float32)
        self.relative_position_bias_table3 = torch.rand(torch.Size([169, 6])).to(torch.float32)
        self.relative_position_index3 = torch.rand(torch.Size([2401])).to(torch.int64)
        self.weight6 = torch.rand(torch.Size([576, 192])).to(torch.float32)
        self.weight7 = torch.rand(torch.Size([192, 192])).to(torch.float32)
        self.bias6 = torch.rand(torch.Size([576])).to(torch.float32)
        self.bias7 = torch.rand(torch.Size([192])).to(torch.float32)
        self.relative_position_bias_table4 = torch.rand(torch.Size([169, 12])).to(torch.float32)
        self.relative_position_index4 = torch.rand(torch.Size([2401])).to(torch.int64)
        self.weight8 = torch.rand(torch.Size([1152, 384])).to(torch.float32)
        self.weight9 = torch.rand(torch.Size([384, 384])).to(torch.float32)
        self.bias8 = torch.rand(torch.Size([1152])).to(torch.float32)
        self.bias9 = torch.rand(torch.Size([384])).to(torch.float32)
        self.relative_position_bias_table5 = torch.rand(torch.Size([169, 12])).to(torch.float32)
        self.relative_position_index5 = torch.rand(torch.Size([2401])).to(torch.int64)
        self.weight10 = torch.rand(torch.Size([1152, 384])).to(torch.float32)
        self.weight11 = torch.rand(torch.Size([384, 384])).to(torch.float32)
        self.bias10 = torch.rand(torch.Size([1152])).to(torch.float32)
        self.bias11 = torch.rand(torch.Size([384])).to(torch.float32)
        self.relative_position_bias_table6 = torch.rand(torch.Size([169, 12])).to(torch.float32)
        self.relative_position_index6 = torch.rand(torch.Size([2401])).to(torch.int64)
        self.weight12 = torch.rand(torch.Size([1152, 384])).to(torch.float32)
        self.weight13 = torch.rand(torch.Size([384, 384])).to(torch.float32)
        self.bias12 = torch.rand(torch.Size([1152])).to(torch.float32)
        self.bias13 = torch.rand(torch.Size([384])).to(torch.float32)
        self.relative_position_bias_table7 = torch.rand(torch.Size([169, 12])).to(torch.float32)
        self.relative_position_index7 = torch.rand(torch.Size([2401])).to(torch.int64)
        self.weight14 = torch.rand(torch.Size([1152, 384])).to(torch.float32)
        self.weight15 = torch.rand(torch.Size([384, 384])).to(torch.float32)
        self.bias14 = torch.rand(torch.Size([1152])).to(torch.float32)
        self.bias15 = torch.rand(torch.Size([384])).to(torch.float32)
        self.relative_position_bias_table8 = torch.rand(torch.Size([169, 12])).to(torch.float32)
        self.relative_position_index8 = torch.rand(torch.Size([2401])).to(torch.int64)
        self.weight16 = torch.rand(torch.Size([1152, 384])).to(torch.float32)
        self.weight17 = torch.rand(torch.Size([384, 384])).to(torch.float32)
        self.bias16 = torch.rand(torch.Size([1152])).to(torch.float32)
        self.bias17 = torch.rand(torch.Size([384])).to(torch.float32)
        self.relative_position_bias_table9 = torch.rand(torch.Size([169, 12])).to(torch.float32)
        self.relative_position_index9 = torch.rand(torch.Size([2401])).to(torch.int64)
        self.weight18 = torch.rand(torch.Size([1152, 384])).to(torch.float32)
        self.weight19 = torch.rand(torch.Size([384, 384])).to(torch.float32)
        self.bias18 = torch.rand(torch.Size([1152])).to(torch.float32)
        self.bias19 = torch.rand(torch.Size([384])).to(torch.float32)
        self.relative_position_bias_table10 = torch.rand(torch.Size([169, 12])).to(torch.float32)
        self.relative_position_index10 = torch.rand(torch.Size([2401])).to(torch.int64)
        self.weight20 = torch.rand(torch.Size([1152, 384])).to(torch.float32)
        self.weight21 = torch.rand(torch.Size([384, 384])).to(torch.float32)
        self.bias20 = torch.rand(torch.Size([1152])).to(torch.float32)
        self.bias21 = torch.rand(torch.Size([384])).to(torch.float32)
        self.relative_position_bias_table11 = torch.rand(torch.Size([169, 12])).to(torch.float32)
        self.relative_position_index11 = torch.rand(torch.Size([2401])).to(torch.int64)
        self.weight22 = torch.rand(torch.Size([1152, 384])).to(torch.float32)
        self.weight23 = torch.rand(torch.Size([384, 384])).to(torch.float32)
        self.bias22 = torch.rand(torch.Size([1152])).to(torch.float32)
        self.bias23 = torch.rand(torch.Size([384])).to(torch.float32)
        self.relative_position_bias_table12 = torch.rand(torch.Size([169, 12])).to(torch.float32)
        self.relative_position_index12 = torch.rand(torch.Size([2401])).to(torch.int64)
        self.weight24 = torch.rand(torch.Size([1152, 384])).to(torch.float32)
        self.weight25 = torch.rand(torch.Size([384, 384])).to(torch.float32)
        self.bias24 = torch.rand(torch.Size([1152])).to(torch.float32)
        self.bias25 = torch.rand(torch.Size([384])).to(torch.float32)
        self.relative_position_bias_table13 = torch.rand(torch.Size([169, 12])).to(torch.float32)
        self.relative_position_index13 = torch.rand(torch.Size([2401])).to(torch.int64)
        self.weight26 = torch.rand(torch.Size([1152, 384])).to(torch.float32)
        self.weight27 = torch.rand(torch.Size([384, 384])).to(torch.float32)
        self.bias26 = torch.rand(torch.Size([1152])).to(torch.float32)
        self.bias27 = torch.rand(torch.Size([384])).to(torch.float32)
        self.relative_position_bias_table14 = torch.rand(torch.Size([169, 12])).to(torch.float32)
        self.relative_position_index14 = torch.rand(torch.Size([2401])).to(torch.int64)
        self.weight28 = torch.rand(torch.Size([1152, 384])).to(torch.float32)
        self.weight29 = torch.rand(torch.Size([384, 384])).to(torch.float32)
        self.bias28 = torch.rand(torch.Size([1152])).to(torch.float32)
        self.bias29 = torch.rand(torch.Size([384])).to(torch.float32)
        self.relative_position_bias_table15 = torch.rand(torch.Size([169, 12])).to(torch.float32)
        self.relative_position_index15 = torch.rand(torch.Size([2401])).to(torch.int64)
        self.weight30 = torch.rand(torch.Size([1152, 384])).to(torch.float32)
        self.weight31 = torch.rand(torch.Size([384, 384])).to(torch.float32)
        self.bias30 = torch.rand(torch.Size([1152])).to(torch.float32)
        self.bias31 = torch.rand(torch.Size([384])).to(torch.float32)
        self.relative_position_bias_table16 = torch.rand(torch.Size([169, 12])).to(torch.float32)
        self.relative_position_index16 = torch.rand(torch.Size([2401])).to(torch.int64)
        self.weight32 = torch.rand(torch.Size([1152, 384])).to(torch.float32)
        self.weight33 = torch.rand(torch.Size([384, 384])).to(torch.float32)
        self.bias32 = torch.rand(torch.Size([1152])).to(torch.float32)
        self.bias33 = torch.rand(torch.Size([384])).to(torch.float32)
        self.relative_position_bias_table17 = torch.rand(torch.Size([169, 12])).to(torch.float32)
        self.relative_position_index17 = torch.rand(torch.Size([2401])).to(torch.int64)
        self.weight34 = torch.rand(torch.Size([1152, 384])).to(torch.float32)
        self.weight35 = torch.rand(torch.Size([384, 384])).to(torch.float32)
        self.bias34 = torch.rand(torch.Size([1152])).to(torch.float32)
        self.bias35 = torch.rand(torch.Size([384])).to(torch.float32)
        self.relative_position_bias_table18 = torch.rand(torch.Size([169, 12])).to(torch.float32)
        self.relative_position_index18 = torch.rand(torch.Size([2401])).to(torch.int64)
        self.weight36 = torch.rand(torch.Size([1152, 384])).to(torch.float32)
        self.weight37 = torch.rand(torch.Size([384, 384])).to(torch.float32)
        self.bias36 = torch.rand(torch.Size([1152])).to(torch.float32)
        self.bias37 = torch.rand(torch.Size([384])).to(torch.float32)
        self.relative_position_bias_table19 = torch.rand(torch.Size([169, 12])).to(torch.float32)
        self.relative_position_index19 = torch.rand(torch.Size([2401])).to(torch.int64)
        self.weight38 = torch.rand(torch.Size([1152, 384])).to(torch.float32)
        self.weight39 = torch.rand(torch.Size([384, 384])).to(torch.float32)
        self.bias38 = torch.rand(torch.Size([1152])).to(torch.float32)
        self.bias39 = torch.rand(torch.Size([384])).to(torch.float32)
        self.relative_position_bias_table20 = torch.rand(torch.Size([169, 12])).to(torch.float32)
        self.relative_position_index20 = torch.rand(torch.Size([2401])).to(torch.int64)
        self.weight40 = torch.rand(torch.Size([1152, 384])).to(torch.float32)
        self.weight41 = torch.rand(torch.Size([384, 384])).to(torch.float32)
        self.bias40 = torch.rand(torch.Size([1152])).to(torch.float32)
        self.bias41 = torch.rand(torch.Size([384])).to(torch.float32)
        self.relative_position_bias_table21 = torch.rand(torch.Size([169, 12])).to(torch.float32)
        self.relative_position_index21 = torch.rand(torch.Size([2401])).to(torch.int64)
        self.weight42 = torch.rand(torch.Size([1152, 384])).to(torch.float32)
        self.weight43 = torch.rand(torch.Size([384, 384])).to(torch.float32)
        self.bias42 = torch.rand(torch.Size([1152])).to(torch.float32)
        self.bias43 = torch.rand(torch.Size([384])).to(torch.float32)
        self.relative_position_bias_table22 = torch.rand(torch.Size([169, 24])).to(torch.float32)
        self.relative_position_index22 = torch.rand(torch.Size([2401])).to(torch.int64)
        self.weight44 = torch.rand(torch.Size([2304, 768])).to(torch.float32)
        self.weight45 = torch.rand(torch.Size([768, 768])).to(torch.float32)
        self.bias44 = torch.rand(torch.Size([2304])).to(torch.float32)
        self.bias45 = torch.rand(torch.Size([768])).to(torch.float32)
        self.relative_position_bias_table23 = torch.rand(torch.Size([169, 24])).to(torch.float32)
        self.relative_position_index23 = torch.rand(torch.Size([2401])).to(torch.int64)
        self.weight46 = torch.rand(torch.Size([2304, 768])).to(torch.float32)
        self.weight47 = torch.rand(torch.Size([768, 768])).to(torch.float32)
        self.bias46 = torch.rand(torch.Size([2304])).to(torch.float32)
        self.bias47 = torch.rand(torch.Size([768])).to(torch.float32)

    def forward(self, x):
        x0=x
        print('x0: {}'.format(x0.shape))
        x1=self.conv2d0(x0)
        print('x1: {}'.format(x1.shape))
        x2=torch.permute(x1, [0, 2, 3, 1])
        print('x2: {}'.format(x2.shape))
        x3=self.layernorm0(x2)
        print('x3: {}'.format(x3.shape))
        x4=self.layernorm1(x3)
        print('x4: {}'.format(x4.shape))
        x7=operator.getitem(self.relative_position_bias_table0, self.relative_position_index0)
        print('x7: {}'.format(x7.shape))
        x8=x7.view(49, 49, -1)
        print('x8: {}'.format(x8.shape))
        x9=x8.permute(2, 0, 1)
        print('x9: {}'.format(x9.shape))
        x10=x9.contiguous()
        print('x10: {}'.format(x10.shape))
        x11=x10.unsqueeze(0)
        print('x11: {}'.format(x11.shape))
        x16=torchvision.models.swin_transformer.shifted_window_attention(x4, self.weight0, self.weight1, x11, [7, 7], 3,shift_size=[0, 0], attention_dropout=0.0, dropout=0.0, qkv_bias=self.bias0, proj_bias=self.bias1)
        print('x16: {}'.format(x16.shape))
        x17=stochastic_depth(x16, 0.0, 'row', False)
        print('x17: {}'.format(x17.shape))
        x18=operator.add(x3, x17)
        print('x18: {}'.format(x18.shape))
        x19=self.layernorm2(x18)
        print('x19: {}'.format(x19.shape))
        x20=self.linear0(x19)
        print('x20: {}'.format(x20.shape))
        x21=self.gelu0(x20)
        print('x21: {}'.format(x21.shape))
        x22=self.dropout0(x21)
        print('x22: {}'.format(x22.shape))
        x23=self.linear1(x22)
        print('x23: {}'.format(x23.shape))
        x24=self.dropout1(x23)
        print('x24: {}'.format(x24.shape))
        x25=stochastic_depth(x24, 0.0, 'row', False)
        print('x25: {}'.format(x25.shape))
        x26=operator.add(x18, x25)
        print('x26: {}'.format(x26.shape))
        x27=self.layernorm3(x26)
        print('x27: {}'.format(x27.shape))
        x30=operator.getitem(self.relative_position_bias_table1, self.relative_position_index1)
        print('x30: {}'.format(x30.shape))
        x31=x30.view(49, 49, -1)
        print('x31: {}'.format(x31.shape))
        x32=x31.permute(2, 0, 1)
        print('x32: {}'.format(x32.shape))
        x33=x32.contiguous()
        print('x33: {}'.format(x33.shape))
        x34=x33.unsqueeze(0)
        print('x34: {}'.format(x34.shape))
        x39=torchvision.models.swin_transformer.shifted_window_attention(x27, self.weight2, self.weight3, x34, [7, 7], 3,shift_size=[3, 3], attention_dropout=0.0, dropout=0.0, qkv_bias=self.bias2, proj_bias=self.bias3)
        print('x39: {}'.format(x39.shape))
        x40=stochastic_depth(x39, 0.013043478260869565, 'row', False)
        print('x40: {}'.format(x40.shape))
        x41=operator.add(x26, x40)
        print('x41: {}'.format(x41.shape))
        x42=self.layernorm4(x41)
        print('x42: {}'.format(x42.shape))
        x43=self.linear2(x42)
        print('x43: {}'.format(x43.shape))
        x44=self.gelu1(x43)
        print('x44: {}'.format(x44.shape))
        x45=self.dropout2(x44)
        print('x45: {}'.format(x45.shape))
        x46=self.linear3(x45)
        print('x46: {}'.format(x46.shape))
        x47=self.dropout3(x46)
        print('x47: {}'.format(x47.shape))
        x48=stochastic_depth(x47, 0.013043478260869565, 'row', False)
        print('x48: {}'.format(x48.shape))
        x49=operator.add(x41, x48)
        print('x49: {}'.format(x49.shape))
        x50=builtins.getattr(x49, 'shape')
        print('x50: {}'.format(x50.shape))
        x51=operator.getitem(x50, slice(-3, None, None))
        print('x51: {}'.format(x51.shape))
        x52=operator.getitem(x51, 0)
        print('x52: {}'.format(x52.shape))
        x53=operator.getitem(x51, 1)
        print('x53: {}'.format(x53.shape))
        x54=operator.getitem(x51, 2)
        print('x54: {}'.format(x54.shape))
        x55=operator.mod(x53, 2)
        print('x55: {}'.format(x55.shape))
        x56=operator.mod(x52, 2)
        print('x56: {}'.format(x56.shape))
        x57=torch.nn.functional.pad(x49, (0, 0, 0, x55, 0, x56))
        print('x57: {}'.format(x57.shape))
        x58=operator.getitem(x57, (Ellipsis, slice(0, None, 2), slice(0, None, 2), slice(None, None, None)))
        print('x58: {}'.format(x58.shape))
        x59=operator.getitem(x57, (Ellipsis, slice(1, None, 2), slice(0, None, 2), slice(None, None, None)))
        print('x59: {}'.format(x59.shape))
        x60=operator.getitem(x57, (Ellipsis, slice(0, None, 2), slice(1, None, 2), slice(None, None, None)))
        print('x60: {}'.format(x60.shape))
        x61=operator.getitem(x57, (Ellipsis, slice(1, None, 2), slice(1, None, 2), slice(None, None, None)))
        print('x61: {}'.format(x61.shape))
        x62=torch.cat([x58, x59, x60, x61], -1)
        print('x62: {}'.format(x62.shape))
        x63=self.layernorm5(x62)
        print('x63: {}'.format(x63.shape))
        x64=self.linear4(x63)
        print('x64: {}'.format(x64.shape))
        x65=self.layernorm6(x64)
        print('x65: {}'.format(x65.shape))
        x68=operator.getitem(self.relative_position_bias_table2, self.relative_position_index2)
        print('x68: {}'.format(x68.shape))
        x69=x68.view(49, 49, -1)
        print('x69: {}'.format(x69.shape))
        x70=x69.permute(2, 0, 1)
        print('x70: {}'.format(x70.shape))
        x71=x70.contiguous()
        print('x71: {}'.format(x71.shape))
        x72=x71.unsqueeze(0)
        print('x72: {}'.format(x72.shape))
        x77=torchvision.models.swin_transformer.shifted_window_attention(x65, self.weight4, self.weight5, x72, [7, 7], 6,shift_size=[0, 0], attention_dropout=0.0, dropout=0.0, qkv_bias=self.bias4, proj_bias=self.bias5)
        print('x77: {}'.format(x77.shape))
        x78=stochastic_depth(x77, 0.02608695652173913, 'row', False)
        print('x78: {}'.format(x78.shape))
        x79=operator.add(x64, x78)
        print('x79: {}'.format(x79.shape))
        x80=self.layernorm7(x79)
        print('x80: {}'.format(x80.shape))
        x81=self.linear5(x80)
        print('x81: {}'.format(x81.shape))
        x82=self.gelu2(x81)
        print('x82: {}'.format(x82.shape))
        x83=self.dropout4(x82)
        print('x83: {}'.format(x83.shape))
        x84=self.linear6(x83)
        print('x84: {}'.format(x84.shape))
        x85=self.dropout5(x84)
        print('x85: {}'.format(x85.shape))
        x86=stochastic_depth(x85, 0.02608695652173913, 'row', False)
        print('x86: {}'.format(x86.shape))
        x87=operator.add(x79, x86)
        print('x87: {}'.format(x87.shape))
        x88=self.layernorm8(x87)
        print('x88: {}'.format(x88.shape))
        x91=operator.getitem(self.relative_position_bias_table3, self.relative_position_index3)
        print('x91: {}'.format(x91.shape))
        x92=x91.view(49, 49, -1)
        print('x92: {}'.format(x92.shape))
        x93=x92.permute(2, 0, 1)
        print('x93: {}'.format(x93.shape))
        x94=x93.contiguous()
        print('x94: {}'.format(x94.shape))
        x95=x94.unsqueeze(0)
        print('x95: {}'.format(x95.shape))
        x100=torchvision.models.swin_transformer.shifted_window_attention(x88, self.weight6, self.weight7, x95, [7, 7], 6,shift_size=[3, 3], attention_dropout=0.0, dropout=0.0, qkv_bias=self.bias6, proj_bias=self.bias7)
        print('x100: {}'.format(x100.shape))
        x101=stochastic_depth(x100, 0.03913043478260869, 'row', False)
        print('x101: {}'.format(x101.shape))
        x102=operator.add(x87, x101)
        print('x102: {}'.format(x102.shape))
        x103=self.layernorm9(x102)
        print('x103: {}'.format(x103.shape))
        x104=self.linear7(x103)
        print('x104: {}'.format(x104.shape))
        x105=self.gelu3(x104)
        print('x105: {}'.format(x105.shape))
        x106=self.dropout6(x105)
        print('x106: {}'.format(x106.shape))
        x107=self.linear8(x106)
        print('x107: {}'.format(x107.shape))
        x108=self.dropout7(x107)
        print('x108: {}'.format(x108.shape))
        x109=stochastic_depth(x108, 0.03913043478260869, 'row', False)
        print('x109: {}'.format(x109.shape))
        x110=operator.add(x102, x109)
        print('x110: {}'.format(x110.shape))
        x111=builtins.getattr(x110, 'shape')
        print('x111: {}'.format(x111.shape))
        x112=operator.getitem(x111, slice(-3, None, None))
        print('x112: {}'.format(x112.shape))
        x113=operator.getitem(x112, 0)
        print('x113: {}'.format(x113.shape))
        x114=operator.getitem(x112, 1)
        print('x114: {}'.format(x114.shape))
        x115=operator.getitem(x112, 2)
        print('x115: {}'.format(x115.shape))
        x116=operator.mod(x114, 2)
        print('x116: {}'.format(x116.shape))
        x117=operator.mod(x113, 2)
        print('x117: {}'.format(x117.shape))
        x118=torch.nn.functional.pad(x110, (0, 0, 0, x116, 0, x117))
        print('x118: {}'.format(x118.shape))
        x119=operator.getitem(x118, (Ellipsis, slice(0, None, 2), slice(0, None, 2), slice(None, None, None)))
        print('x119: {}'.format(x119.shape))
        x120=operator.getitem(x118, (Ellipsis, slice(1, None, 2), slice(0, None, 2), slice(None, None, None)))
        print('x120: {}'.format(x120.shape))
        x121=operator.getitem(x118, (Ellipsis, slice(0, None, 2), slice(1, None, 2), slice(None, None, None)))
        print('x121: {}'.format(x121.shape))
        x122=operator.getitem(x118, (Ellipsis, slice(1, None, 2), slice(1, None, 2), slice(None, None, None)))
        print('x122: {}'.format(x122.shape))
        x123=torch.cat([x119, x120, x121, x122], -1)
        print('x123: {}'.format(x123.shape))
        x124=self.layernorm10(x123)
        print('x124: {}'.format(x124.shape))
        x125=self.linear9(x124)
        print('x125: {}'.format(x125.shape))
        x126=self.layernorm11(x125)
        print('x126: {}'.format(x126.shape))
        x129=operator.getitem(self.relative_position_bias_table4, self.relative_position_index4)
        print('x129: {}'.format(x129.shape))
        x130=x129.view(49, 49, -1)
        print('x130: {}'.format(x130.shape))
        x131=x130.permute(2, 0, 1)
        print('x131: {}'.format(x131.shape))
        x132=x131.contiguous()
        print('x132: {}'.format(x132.shape))
        x133=x132.unsqueeze(0)
        print('x133: {}'.format(x133.shape))
        x138=torchvision.models.swin_transformer.shifted_window_attention(x126, self.weight8, self.weight9, x133, [7, 7], 12,shift_size=[0, 0], attention_dropout=0.0, dropout=0.0, qkv_bias=self.bias8, proj_bias=self.bias9)
        print('x138: {}'.format(x138.shape))
        x139=stochastic_depth(x138, 0.05217391304347826, 'row', False)
        print('x139: {}'.format(x139.shape))
        x140=operator.add(x125, x139)
        print('x140: {}'.format(x140.shape))
        x141=self.layernorm12(x140)
        print('x141: {}'.format(x141.shape))
        x142=self.linear10(x141)
        print('x142: {}'.format(x142.shape))
        x143=self.gelu4(x142)
        print('x143: {}'.format(x143.shape))
        x144=self.dropout8(x143)
        print('x144: {}'.format(x144.shape))
        x145=self.linear11(x144)
        print('x145: {}'.format(x145.shape))
        x146=self.dropout9(x145)
        print('x146: {}'.format(x146.shape))
        x147=stochastic_depth(x146, 0.05217391304347826, 'row', False)
        print('x147: {}'.format(x147.shape))
        x148=operator.add(x140, x147)
        print('x148: {}'.format(x148.shape))
        x149=self.layernorm13(x148)
        print('x149: {}'.format(x149.shape))
        x152=operator.getitem(self.relative_position_bias_table5, self.relative_position_index5)
        print('x152: {}'.format(x152.shape))
        x153=x152.view(49, 49, -1)
        print('x153: {}'.format(x153.shape))
        x154=x153.permute(2, 0, 1)
        print('x154: {}'.format(x154.shape))
        x155=x154.contiguous()
        print('x155: {}'.format(x155.shape))
        x156=x155.unsqueeze(0)
        print('x156: {}'.format(x156.shape))
        x161=torchvision.models.swin_transformer.shifted_window_attention(x149, self.weight10, self.weight11, x156, [7, 7], 12,shift_size=[3, 3], attention_dropout=0.0, dropout=0.0, qkv_bias=self.bias10, proj_bias=self.bias11)
        print('x161: {}'.format(x161.shape))
        x162=stochastic_depth(x161, 0.06521739130434782, 'row', False)
        print('x162: {}'.format(x162.shape))
        x163=operator.add(x148, x162)
        print('x163: {}'.format(x163.shape))
        x164=self.layernorm14(x163)
        print('x164: {}'.format(x164.shape))
        x165=self.linear12(x164)
        print('x165: {}'.format(x165.shape))
        x166=self.gelu5(x165)
        print('x166: {}'.format(x166.shape))
        x167=self.dropout10(x166)
        print('x167: {}'.format(x167.shape))
        x168=self.linear13(x167)
        print('x168: {}'.format(x168.shape))
        x169=self.dropout11(x168)
        print('x169: {}'.format(x169.shape))
        x170=stochastic_depth(x169, 0.06521739130434782, 'row', False)
        print('x170: {}'.format(x170.shape))
        x171=operator.add(x163, x170)
        print('x171: {}'.format(x171.shape))
        x172=self.layernorm15(x171)
        print('x172: {}'.format(x172.shape))
        x175=operator.getitem(self.relative_position_bias_table6, self.relative_position_index6)
        print('x175: {}'.format(x175.shape))
        x176=x175.view(49, 49, -1)
        print('x176: {}'.format(x176.shape))
        x177=x176.permute(2, 0, 1)
        print('x177: {}'.format(x177.shape))
        x178=x177.contiguous()
        print('x178: {}'.format(x178.shape))
        x179=x178.unsqueeze(0)
        print('x179: {}'.format(x179.shape))
        x184=torchvision.models.swin_transformer.shifted_window_attention(x172, self.weight12, self.weight13, x179, [7, 7], 12,shift_size=[0, 0], attention_dropout=0.0, dropout=0.0, qkv_bias=self.bias12, proj_bias=self.bias13)
        print('x184: {}'.format(x184.shape))
        x185=stochastic_depth(x184, 0.07826086956521738, 'row', False)
        print('x185: {}'.format(x185.shape))
        x186=operator.add(x171, x185)
        print('x186: {}'.format(x186.shape))
        x187=self.layernorm16(x186)
        print('x187: {}'.format(x187.shape))
        x188=self.linear14(x187)
        print('x188: {}'.format(x188.shape))
        x189=self.gelu6(x188)
        print('x189: {}'.format(x189.shape))
        x190=self.dropout12(x189)
        print('x190: {}'.format(x190.shape))
        x191=self.linear15(x190)
        print('x191: {}'.format(x191.shape))
        x192=self.dropout13(x191)
        print('x192: {}'.format(x192.shape))
        x193=stochastic_depth(x192, 0.07826086956521738, 'row', False)
        print('x193: {}'.format(x193.shape))
        x194=operator.add(x186, x193)
        print('x194: {}'.format(x194.shape))
        x195=self.layernorm17(x194)
        print('x195: {}'.format(x195.shape))
        x198=operator.getitem(self.relative_position_bias_table7, self.relative_position_index7)
        print('x198: {}'.format(x198.shape))
        x199=x198.view(49, 49, -1)
        print('x199: {}'.format(x199.shape))
        x200=x199.permute(2, 0, 1)
        print('x200: {}'.format(x200.shape))
        x201=x200.contiguous()
        print('x201: {}'.format(x201.shape))
        x202=x201.unsqueeze(0)
        print('x202: {}'.format(x202.shape))
        x207=torchvision.models.swin_transformer.shifted_window_attention(x195, self.weight14, self.weight15, x202, [7, 7], 12,shift_size=[3, 3], attention_dropout=0.0, dropout=0.0, qkv_bias=self.bias14, proj_bias=self.bias15)
        print('x207: {}'.format(x207.shape))
        x208=stochastic_depth(x207, 0.09130434782608696, 'row', False)
        print('x208: {}'.format(x208.shape))
        x209=operator.add(x194, x208)
        print('x209: {}'.format(x209.shape))
        x210=self.layernorm18(x209)
        print('x210: {}'.format(x210.shape))
        x211=self.linear16(x210)
        print('x211: {}'.format(x211.shape))
        x212=self.gelu7(x211)
        print('x212: {}'.format(x212.shape))
        x213=self.dropout14(x212)
        print('x213: {}'.format(x213.shape))
        x214=self.linear17(x213)
        print('x214: {}'.format(x214.shape))
        x215=self.dropout15(x214)
        print('x215: {}'.format(x215.shape))
        x216=stochastic_depth(x215, 0.09130434782608696, 'row', False)
        print('x216: {}'.format(x216.shape))
        x217=operator.add(x209, x216)
        print('x217: {}'.format(x217.shape))
        x218=self.layernorm19(x217)
        print('x218: {}'.format(x218.shape))
        x221=operator.getitem(self.relative_position_bias_table8, self.relative_position_index8)
        print('x221: {}'.format(x221.shape))
        x222=x221.view(49, 49, -1)
        print('x222: {}'.format(x222.shape))
        x223=x222.permute(2, 0, 1)
        print('x223: {}'.format(x223.shape))
        x224=x223.contiguous()
        print('x224: {}'.format(x224.shape))
        x225=x224.unsqueeze(0)
        print('x225: {}'.format(x225.shape))
        x230=torchvision.models.swin_transformer.shifted_window_attention(x218, self.weight16, self.weight17, x225, [7, 7], 12,shift_size=[0, 0], attention_dropout=0.0, dropout=0.0, qkv_bias=self.bias16, proj_bias=self.bias17)
        print('x230: {}'.format(x230.shape))
        x231=stochastic_depth(x230, 0.10434782608695652, 'row', False)
        print('x231: {}'.format(x231.shape))
        x232=operator.add(x217, x231)
        print('x232: {}'.format(x232.shape))
        x233=self.layernorm20(x232)
        print('x233: {}'.format(x233.shape))
        x234=self.linear18(x233)
        print('x234: {}'.format(x234.shape))
        x235=self.gelu8(x234)
        print('x235: {}'.format(x235.shape))
        x236=self.dropout16(x235)
        print('x236: {}'.format(x236.shape))
        x237=self.linear19(x236)
        print('x237: {}'.format(x237.shape))
        x238=self.dropout17(x237)
        print('x238: {}'.format(x238.shape))
        x239=stochastic_depth(x238, 0.10434782608695652, 'row', False)
        print('x239: {}'.format(x239.shape))
        x240=operator.add(x232, x239)
        print('x240: {}'.format(x240.shape))
        x241=self.layernorm21(x240)
        print('x241: {}'.format(x241.shape))
        x244=operator.getitem(self.relative_position_bias_table9, self.relative_position_index9)
        print('x244: {}'.format(x244.shape))
        x245=x244.view(49, 49, -1)
        print('x245: {}'.format(x245.shape))
        x246=x245.permute(2, 0, 1)
        print('x246: {}'.format(x246.shape))
        x247=x246.contiguous()
        print('x247: {}'.format(x247.shape))
        x248=x247.unsqueeze(0)
        print('x248: {}'.format(x248.shape))
        x253=torchvision.models.swin_transformer.shifted_window_attention(x241, self.weight18, self.weight19, x248, [7, 7], 12,shift_size=[3, 3], attention_dropout=0.0, dropout=0.0, qkv_bias=self.bias18, proj_bias=self.bias19)
        print('x253: {}'.format(x253.shape))
        x254=stochastic_depth(x253, 0.11739130434782608, 'row', False)
        print('x254: {}'.format(x254.shape))
        x255=operator.add(x240, x254)
        print('x255: {}'.format(x255.shape))
        x256=self.layernorm22(x255)
        print('x256: {}'.format(x256.shape))
        x257=self.linear20(x256)
        print('x257: {}'.format(x257.shape))
        x258=self.gelu9(x257)
        print('x258: {}'.format(x258.shape))
        x259=self.dropout18(x258)
        print('x259: {}'.format(x259.shape))
        x260=self.linear21(x259)
        print('x260: {}'.format(x260.shape))
        x261=self.dropout19(x260)
        print('x261: {}'.format(x261.shape))
        x262=stochastic_depth(x261, 0.11739130434782608, 'row', False)
        print('x262: {}'.format(x262.shape))
        x263=operator.add(x255, x262)
        print('x263: {}'.format(x263.shape))
        x264=self.layernorm23(x263)
        print('x264: {}'.format(x264.shape))
        x267=operator.getitem(self.relative_position_bias_table10, self.relative_position_index10)
        print('x267: {}'.format(x267.shape))
        x268=x267.view(49, 49, -1)
        print('x268: {}'.format(x268.shape))
        x269=x268.permute(2, 0, 1)
        print('x269: {}'.format(x269.shape))
        x270=x269.contiguous()
        print('x270: {}'.format(x270.shape))
        x271=x270.unsqueeze(0)
        print('x271: {}'.format(x271.shape))
        x276=torchvision.models.swin_transformer.shifted_window_attention(x264, self.weight20, self.weight21, x271, [7, 7], 12,shift_size=[0, 0], attention_dropout=0.0, dropout=0.0, qkv_bias=self.bias20, proj_bias=self.bias21)
        print('x276: {}'.format(x276.shape))
        x277=stochastic_depth(x276, 0.13043478260869565, 'row', False)
        print('x277: {}'.format(x277.shape))
        x278=operator.add(x263, x277)
        print('x278: {}'.format(x278.shape))
        x279=self.layernorm24(x278)
        print('x279: {}'.format(x279.shape))
        x280=self.linear22(x279)
        print('x280: {}'.format(x280.shape))
        x281=self.gelu10(x280)
        print('x281: {}'.format(x281.shape))
        x282=self.dropout20(x281)
        print('x282: {}'.format(x282.shape))
        x283=self.linear23(x282)
        print('x283: {}'.format(x283.shape))
        x284=self.dropout21(x283)
        print('x284: {}'.format(x284.shape))
        x285=stochastic_depth(x284, 0.13043478260869565, 'row', False)
        print('x285: {}'.format(x285.shape))
        x286=operator.add(x278, x285)
        print('x286: {}'.format(x286.shape))
        x287=self.layernorm25(x286)
        print('x287: {}'.format(x287.shape))
        x290=operator.getitem(self.relative_position_bias_table11, self.relative_position_index11)
        print('x290: {}'.format(x290.shape))
        x291=x290.view(49, 49, -1)
        print('x291: {}'.format(x291.shape))
        x292=x291.permute(2, 0, 1)
        print('x292: {}'.format(x292.shape))
        x293=x292.contiguous()
        print('x293: {}'.format(x293.shape))
        x294=x293.unsqueeze(0)
        print('x294: {}'.format(x294.shape))
        x299=torchvision.models.swin_transformer.shifted_window_attention(x287, self.weight22, self.weight23, x294, [7, 7], 12,shift_size=[3, 3], attention_dropout=0.0, dropout=0.0, qkv_bias=self.bias22, proj_bias=self.bias23)
        print('x299: {}'.format(x299.shape))
        x300=stochastic_depth(x299, 0.14347826086956522, 'row', False)
        print('x300: {}'.format(x300.shape))
        x301=operator.add(x286, x300)
        print('x301: {}'.format(x301.shape))
        x302=self.layernorm26(x301)
        print('x302: {}'.format(x302.shape))
        x303=self.linear24(x302)
        print('x303: {}'.format(x303.shape))
        x304=self.gelu11(x303)
        print('x304: {}'.format(x304.shape))
        x305=self.dropout22(x304)
        print('x305: {}'.format(x305.shape))
        x306=self.linear25(x305)
        print('x306: {}'.format(x306.shape))
        x307=self.dropout23(x306)
        print('x307: {}'.format(x307.shape))
        x308=stochastic_depth(x307, 0.14347826086956522, 'row', False)
        print('x308: {}'.format(x308.shape))
        x309=operator.add(x301, x308)
        print('x309: {}'.format(x309.shape))
        x310=self.layernorm27(x309)
        print('x310: {}'.format(x310.shape))
        x313=operator.getitem(self.relative_position_bias_table12, self.relative_position_index12)
        print('x313: {}'.format(x313.shape))
        x314=x313.view(49, 49, -1)
        print('x314: {}'.format(x314.shape))
        x315=x314.permute(2, 0, 1)
        print('x315: {}'.format(x315.shape))
        x316=x315.contiguous()
        print('x316: {}'.format(x316.shape))
        x317=x316.unsqueeze(0)
        print('x317: {}'.format(x317.shape))
        x322=torchvision.models.swin_transformer.shifted_window_attention(x310, self.weight24, self.weight25, x317, [7, 7], 12,shift_size=[0, 0], attention_dropout=0.0, dropout=0.0, qkv_bias=self.bias24, proj_bias=self.bias25)
        print('x322: {}'.format(x322.shape))
        x323=stochastic_depth(x322, 0.15652173913043477, 'row', False)
        print('x323: {}'.format(x323.shape))
        x324=operator.add(x309, x323)
        print('x324: {}'.format(x324.shape))
        x325=self.layernorm28(x324)
        print('x325: {}'.format(x325.shape))
        x326=self.linear26(x325)
        print('x326: {}'.format(x326.shape))
        x327=self.gelu12(x326)
        print('x327: {}'.format(x327.shape))
        x328=self.dropout24(x327)
        print('x328: {}'.format(x328.shape))
        x329=self.linear27(x328)
        print('x329: {}'.format(x329.shape))
        x330=self.dropout25(x329)
        print('x330: {}'.format(x330.shape))
        x331=stochastic_depth(x330, 0.15652173913043477, 'row', False)
        print('x331: {}'.format(x331.shape))
        x332=operator.add(x324, x331)
        print('x332: {}'.format(x332.shape))
        x333=self.layernorm29(x332)
        print('x333: {}'.format(x333.shape))
        x336=operator.getitem(self.relative_position_bias_table13, self.relative_position_index13)
        print('x336: {}'.format(x336.shape))
        x337=x336.view(49, 49, -1)
        print('x337: {}'.format(x337.shape))
        x338=x337.permute(2, 0, 1)
        print('x338: {}'.format(x338.shape))
        x339=x338.contiguous()
        print('x339: {}'.format(x339.shape))
        x340=x339.unsqueeze(0)
        print('x340: {}'.format(x340.shape))
        x345=torchvision.models.swin_transformer.shifted_window_attention(x333, self.weight26, self.weight27, x340, [7, 7], 12,shift_size=[3, 3], attention_dropout=0.0, dropout=0.0, qkv_bias=self.bias26, proj_bias=self.bias27)
        print('x345: {}'.format(x345.shape))
        x346=stochastic_depth(x345, 0.16956521739130434, 'row', False)
        print('x346: {}'.format(x346.shape))
        x347=operator.add(x332, x346)
        print('x347: {}'.format(x347.shape))
        x348=self.layernorm30(x347)
        print('x348: {}'.format(x348.shape))
        x349=self.linear28(x348)
        print('x349: {}'.format(x349.shape))
        x350=self.gelu13(x349)
        print('x350: {}'.format(x350.shape))
        x351=self.dropout26(x350)
        print('x351: {}'.format(x351.shape))
        x352=self.linear29(x351)
        print('x352: {}'.format(x352.shape))
        x353=self.dropout27(x352)
        print('x353: {}'.format(x353.shape))
        x354=stochastic_depth(x353, 0.16956521739130434, 'row', False)
        print('x354: {}'.format(x354.shape))
        x355=operator.add(x347, x354)
        print('x355: {}'.format(x355.shape))
        x356=self.layernorm31(x355)
        print('x356: {}'.format(x356.shape))
        x359=operator.getitem(self.relative_position_bias_table14, self.relative_position_index14)
        print('x359: {}'.format(x359.shape))
        x360=x359.view(49, 49, -1)
        print('x360: {}'.format(x360.shape))
        x361=x360.permute(2, 0, 1)
        print('x361: {}'.format(x361.shape))
        x362=x361.contiguous()
        print('x362: {}'.format(x362.shape))
        x363=x362.unsqueeze(0)
        print('x363: {}'.format(x363.shape))
        x368=torchvision.models.swin_transformer.shifted_window_attention(x356, self.weight28, self.weight29, x363, [7, 7], 12,shift_size=[0, 0], attention_dropout=0.0, dropout=0.0, qkv_bias=self.bias28, proj_bias=self.bias29)
        print('x368: {}'.format(x368.shape))
        x369=stochastic_depth(x368, 0.1826086956521739, 'row', False)
        print('x369: {}'.format(x369.shape))
        x370=operator.add(x355, x369)
        print('x370: {}'.format(x370.shape))
        x371=self.layernorm32(x370)
        print('x371: {}'.format(x371.shape))
        x372=self.linear30(x371)
        print('x372: {}'.format(x372.shape))
        x373=self.gelu14(x372)
        print('x373: {}'.format(x373.shape))
        x374=self.dropout28(x373)
        print('x374: {}'.format(x374.shape))
        x375=self.linear31(x374)
        print('x375: {}'.format(x375.shape))
        x376=self.dropout29(x375)
        print('x376: {}'.format(x376.shape))
        x377=stochastic_depth(x376, 0.1826086956521739, 'row', False)
        print('x377: {}'.format(x377.shape))
        x378=operator.add(x370, x377)
        print('x378: {}'.format(x378.shape))
        x379=self.layernorm33(x378)
        print('x379: {}'.format(x379.shape))
        x382=operator.getitem(self.relative_position_bias_table15, self.relative_position_index15)
        print('x382: {}'.format(x382.shape))
        x383=x382.view(49, 49, -1)
        print('x383: {}'.format(x383.shape))
        x384=x383.permute(2, 0, 1)
        print('x384: {}'.format(x384.shape))
        x385=x384.contiguous()
        print('x385: {}'.format(x385.shape))
        x386=x385.unsqueeze(0)
        print('x386: {}'.format(x386.shape))
        x391=torchvision.models.swin_transformer.shifted_window_attention(x379, self.weight30, self.weight31, x386, [7, 7], 12,shift_size=[3, 3], attention_dropout=0.0, dropout=0.0, qkv_bias=self.bias30, proj_bias=self.bias31)
        print('x391: {}'.format(x391.shape))
        x392=stochastic_depth(x391, 0.1956521739130435, 'row', False)
        print('x392: {}'.format(x392.shape))
        x393=operator.add(x378, x392)
        print('x393: {}'.format(x393.shape))
        x394=self.layernorm34(x393)
        print('x394: {}'.format(x394.shape))
        x395=self.linear32(x394)
        print('x395: {}'.format(x395.shape))
        x396=self.gelu15(x395)
        print('x396: {}'.format(x396.shape))
        x397=self.dropout30(x396)
        print('x397: {}'.format(x397.shape))
        x398=self.linear33(x397)
        print('x398: {}'.format(x398.shape))
        x399=self.dropout31(x398)
        print('x399: {}'.format(x399.shape))
        x400=stochastic_depth(x399, 0.1956521739130435, 'row', False)
        print('x400: {}'.format(x400.shape))
        x401=operator.add(x393, x400)
        print('x401: {}'.format(x401.shape))
        x402=self.layernorm35(x401)
        print('x402: {}'.format(x402.shape))
        x405=operator.getitem(self.relative_position_bias_table16, self.relative_position_index16)
        print('x405: {}'.format(x405.shape))
        x406=x405.view(49, 49, -1)
        print('x406: {}'.format(x406.shape))
        x407=x406.permute(2, 0, 1)
        print('x407: {}'.format(x407.shape))
        x408=x407.contiguous()
        print('x408: {}'.format(x408.shape))
        x409=x408.unsqueeze(0)
        print('x409: {}'.format(x409.shape))
        x414=torchvision.models.swin_transformer.shifted_window_attention(x402, self.weight32, self.weight33, x409, [7, 7], 12,shift_size=[0, 0], attention_dropout=0.0, dropout=0.0, qkv_bias=self.bias32, proj_bias=self.bias33)
        print('x414: {}'.format(x414.shape))
        x415=stochastic_depth(x414, 0.20869565217391303, 'row', False)
        print('x415: {}'.format(x415.shape))
        x416=operator.add(x401, x415)
        print('x416: {}'.format(x416.shape))
        x417=self.layernorm36(x416)
        print('x417: {}'.format(x417.shape))
        x418=self.linear34(x417)
        print('x418: {}'.format(x418.shape))
        x419=self.gelu16(x418)
        print('x419: {}'.format(x419.shape))
        x420=self.dropout32(x419)
        print('x420: {}'.format(x420.shape))
        x421=self.linear35(x420)
        print('x421: {}'.format(x421.shape))
        x422=self.dropout33(x421)
        print('x422: {}'.format(x422.shape))
        x423=stochastic_depth(x422, 0.20869565217391303, 'row', False)
        print('x423: {}'.format(x423.shape))
        x424=operator.add(x416, x423)
        print('x424: {}'.format(x424.shape))
        x425=self.layernorm37(x424)
        print('x425: {}'.format(x425.shape))
        x428=operator.getitem(self.relative_position_bias_table17, self.relative_position_index17)
        print('x428: {}'.format(x428.shape))
        x429=x428.view(49, 49, -1)
        print('x429: {}'.format(x429.shape))
        x430=x429.permute(2, 0, 1)
        print('x430: {}'.format(x430.shape))
        x431=x430.contiguous()
        print('x431: {}'.format(x431.shape))
        x432=x431.unsqueeze(0)
        print('x432: {}'.format(x432.shape))
        x437=torchvision.models.swin_transformer.shifted_window_attention(x425, self.weight34, self.weight35, x432, [7, 7], 12,shift_size=[3, 3], attention_dropout=0.0, dropout=0.0, qkv_bias=self.bias34, proj_bias=self.bias35)
        print('x437: {}'.format(x437.shape))
        x438=stochastic_depth(x437, 0.2217391304347826, 'row', False)
        print('x438: {}'.format(x438.shape))
        x439=operator.add(x424, x438)
        print('x439: {}'.format(x439.shape))
        x440=self.layernorm38(x439)
        print('x440: {}'.format(x440.shape))
        x441=self.linear36(x440)
        print('x441: {}'.format(x441.shape))
        x442=self.gelu17(x441)
        print('x442: {}'.format(x442.shape))
        x443=self.dropout34(x442)
        print('x443: {}'.format(x443.shape))
        x444=self.linear37(x443)
        print('x444: {}'.format(x444.shape))
        x445=self.dropout35(x444)
        print('x445: {}'.format(x445.shape))
        x446=stochastic_depth(x445, 0.2217391304347826, 'row', False)
        print('x446: {}'.format(x446.shape))
        x447=operator.add(x439, x446)
        print('x447: {}'.format(x447.shape))
        x448=self.layernorm39(x447)
        print('x448: {}'.format(x448.shape))
        x451=operator.getitem(self.relative_position_bias_table18, self.relative_position_index18)
        print('x451: {}'.format(x451.shape))
        x452=x451.view(49, 49, -1)
        print('x452: {}'.format(x452.shape))
        x453=x452.permute(2, 0, 1)
        print('x453: {}'.format(x453.shape))
        x454=x453.contiguous()
        print('x454: {}'.format(x454.shape))
        x455=x454.unsqueeze(0)
        print('x455: {}'.format(x455.shape))
        x460=torchvision.models.swin_transformer.shifted_window_attention(x448, self.weight36, self.weight37, x455, [7, 7], 12,shift_size=[0, 0], attention_dropout=0.0, dropout=0.0, qkv_bias=self.bias36, proj_bias=self.bias37)
        print('x460: {}'.format(x460.shape))
        x461=stochastic_depth(x460, 0.23478260869565215, 'row', False)
        print('x461: {}'.format(x461.shape))
        x462=operator.add(x447, x461)
        print('x462: {}'.format(x462.shape))
        x463=self.layernorm40(x462)
        print('x463: {}'.format(x463.shape))
        x464=self.linear38(x463)
        print('x464: {}'.format(x464.shape))
        x465=self.gelu18(x464)
        print('x465: {}'.format(x465.shape))
        x466=self.dropout36(x465)
        print('x466: {}'.format(x466.shape))
        x467=self.linear39(x466)
        print('x467: {}'.format(x467.shape))
        x468=self.dropout37(x467)
        print('x468: {}'.format(x468.shape))
        x469=stochastic_depth(x468, 0.23478260869565215, 'row', False)
        print('x469: {}'.format(x469.shape))
        x470=operator.add(x462, x469)
        print('x470: {}'.format(x470.shape))
        x471=self.layernorm41(x470)
        print('x471: {}'.format(x471.shape))
        x474=operator.getitem(self.relative_position_bias_table19, self.relative_position_index19)
        print('x474: {}'.format(x474.shape))
        x475=x474.view(49, 49, -1)
        print('x475: {}'.format(x475.shape))
        x476=x475.permute(2, 0, 1)
        print('x476: {}'.format(x476.shape))
        x477=x476.contiguous()
        print('x477: {}'.format(x477.shape))
        x478=x477.unsqueeze(0)
        print('x478: {}'.format(x478.shape))
        x483=torchvision.models.swin_transformer.shifted_window_attention(x471, self.weight38, self.weight39, x478, [7, 7], 12,shift_size=[3, 3], attention_dropout=0.0, dropout=0.0, qkv_bias=self.bias38, proj_bias=self.bias39)
        print('x483: {}'.format(x483.shape))
        x484=stochastic_depth(x483, 0.24782608695652175, 'row', False)
        print('x484: {}'.format(x484.shape))
        x485=operator.add(x470, x484)
        print('x485: {}'.format(x485.shape))
        x486=self.layernorm42(x485)
        print('x486: {}'.format(x486.shape))
        x487=self.linear40(x486)
        print('x487: {}'.format(x487.shape))
        x488=self.gelu19(x487)
        print('x488: {}'.format(x488.shape))
        x489=self.dropout38(x488)
        print('x489: {}'.format(x489.shape))
        x490=self.linear41(x489)
        print('x490: {}'.format(x490.shape))
        x491=self.dropout39(x490)
        print('x491: {}'.format(x491.shape))
        x492=stochastic_depth(x491, 0.24782608695652175, 'row', False)
        print('x492: {}'.format(x492.shape))
        x493=operator.add(x485, x492)
        print('x493: {}'.format(x493.shape))
        x494=self.layernorm43(x493)
        print('x494: {}'.format(x494.shape))
        x497=operator.getitem(self.relative_position_bias_table20, self.relative_position_index20)
        print('x497: {}'.format(x497.shape))
        x498=x497.view(49, 49, -1)
        print('x498: {}'.format(x498.shape))
        x499=x498.permute(2, 0, 1)
        print('x499: {}'.format(x499.shape))
        x500=x499.contiguous()
        print('x500: {}'.format(x500.shape))
        x501=x500.unsqueeze(0)
        print('x501: {}'.format(x501.shape))
        x506=torchvision.models.swin_transformer.shifted_window_attention(x494, self.weight40, self.weight41, x501, [7, 7], 12,shift_size=[0, 0], attention_dropout=0.0, dropout=0.0, qkv_bias=self.bias40, proj_bias=self.bias41)
        print('x506: {}'.format(x506.shape))
        x507=stochastic_depth(x506, 0.2608695652173913, 'row', False)
        print('x507: {}'.format(x507.shape))
        x508=operator.add(x493, x507)
        print('x508: {}'.format(x508.shape))
        x509=self.layernorm44(x508)
        print('x509: {}'.format(x509.shape))
        x510=self.linear42(x509)
        print('x510: {}'.format(x510.shape))
        x511=self.gelu20(x510)
        print('x511: {}'.format(x511.shape))
        x512=self.dropout40(x511)
        print('x512: {}'.format(x512.shape))
        x513=self.linear43(x512)
        print('x513: {}'.format(x513.shape))
        x514=self.dropout41(x513)
        print('x514: {}'.format(x514.shape))
        x515=stochastic_depth(x514, 0.2608695652173913, 'row', False)
        print('x515: {}'.format(x515.shape))
        x516=operator.add(x508, x515)
        print('x516: {}'.format(x516.shape))
        x517=self.layernorm45(x516)
        print('x517: {}'.format(x517.shape))
        x520=operator.getitem(self.relative_position_bias_table21, self.relative_position_index21)
        print('x520: {}'.format(x520.shape))
        x521=x520.view(49, 49, -1)
        print('x521: {}'.format(x521.shape))
        x522=x521.permute(2, 0, 1)
        print('x522: {}'.format(x522.shape))
        x523=x522.contiguous()
        print('x523: {}'.format(x523.shape))
        x524=x523.unsqueeze(0)
        print('x524: {}'.format(x524.shape))
        x529=torchvision.models.swin_transformer.shifted_window_attention(x517, self.weight42, self.weight43, x524, [7, 7], 12,shift_size=[3, 3], attention_dropout=0.0, dropout=0.0, qkv_bias=self.bias42, proj_bias=self.bias43)
        print('x529: {}'.format(x529.shape))
        x530=stochastic_depth(x529, 0.27391304347826084, 'row', False)
        print('x530: {}'.format(x530.shape))
        x531=operator.add(x516, x530)
        print('x531: {}'.format(x531.shape))
        x532=self.layernorm46(x531)
        print('x532: {}'.format(x532.shape))
        x533=self.linear44(x532)
        print('x533: {}'.format(x533.shape))
        x534=self.gelu21(x533)
        print('x534: {}'.format(x534.shape))
        x535=self.dropout42(x534)
        print('x535: {}'.format(x535.shape))
        x536=self.linear45(x535)
        print('x536: {}'.format(x536.shape))
        x537=self.dropout43(x536)
        print('x537: {}'.format(x537.shape))
        x538=stochastic_depth(x537, 0.27391304347826084, 'row', False)
        print('x538: {}'.format(x538.shape))
        x539=operator.add(x531, x538)
        print('x539: {}'.format(x539.shape))
        x540=builtins.getattr(x539, 'shape')
        print('x540: {}'.format(x540.shape))
        x541=operator.getitem(x540, slice(-3, None, None))
        print('x541: {}'.format(x541.shape))
        x542=operator.getitem(x541, 0)
        print('x542: {}'.format(x542.shape))
        x543=operator.getitem(x541, 1)
        print('x543: {}'.format(x543.shape))
        x544=operator.getitem(x541, 2)
        print('x544: {}'.format(x544.shape))
        x545=operator.mod(x543, 2)
        print('x545: {}'.format(x545.shape))
        x546=operator.mod(x542, 2)
        print('x546: {}'.format(x546.shape))
        x547=torch.nn.functional.pad(x539, (0, 0, 0, x545, 0, x546))
        print('x547: {}'.format(x547.shape))
        x548=operator.getitem(x547, (Ellipsis, slice(0, None, 2), slice(0, None, 2), slice(None, None, None)))
        print('x548: {}'.format(x548.shape))
        x549=operator.getitem(x547, (Ellipsis, slice(1, None, 2), slice(0, None, 2), slice(None, None, None)))
        print('x549: {}'.format(x549.shape))
        x550=operator.getitem(x547, (Ellipsis, slice(0, None, 2), slice(1, None, 2), slice(None, None, None)))
        print('x550: {}'.format(x550.shape))
        x551=operator.getitem(x547, (Ellipsis, slice(1, None, 2), slice(1, None, 2), slice(None, None, None)))
        print('x551: {}'.format(x551.shape))
        x552=torch.cat([x548, x549, x550, x551], -1)
        print('x552: {}'.format(x552.shape))
        x553=self.layernorm47(x552)
        print('x553: {}'.format(x553.shape))
        x554=self.linear46(x553)
        print('x554: {}'.format(x554.shape))
        x555=self.layernorm48(x554)
        print('x555: {}'.format(x555.shape))
        x558=operator.getitem(self.relative_position_bias_table22, self.relative_position_index22)
        print('x558: {}'.format(x558.shape))
        x559=x558.view(49, 49, -1)
        print('x559: {}'.format(x559.shape))
        x560=x559.permute(2, 0, 1)
        print('x560: {}'.format(x560.shape))
        x561=x560.contiguous()
        print('x561: {}'.format(x561.shape))
        x562=x561.unsqueeze(0)
        print('x562: {}'.format(x562.shape))
        x567=torchvision.models.swin_transformer.shifted_window_attention(x555, self.weight44, self.weight45, x562, [7, 7], 24,shift_size=[0, 0], attention_dropout=0.0, dropout=0.0, qkv_bias=self.bias44, proj_bias=self.bias45)
        print('x567: {}'.format(x567.shape))
        x568=stochastic_depth(x567, 0.28695652173913044, 'row', False)
        print('x568: {}'.format(x568.shape))
        x569=operator.add(x554, x568)
        print('x569: {}'.format(x569.shape))
        x570=self.layernorm49(x569)
        print('x570: {}'.format(x570.shape))
        x571=self.linear47(x570)
        print('x571: {}'.format(x571.shape))
        x572=self.gelu22(x571)
        print('x572: {}'.format(x572.shape))
        x573=self.dropout44(x572)
        print('x573: {}'.format(x573.shape))
        x574=self.linear48(x573)
        print('x574: {}'.format(x574.shape))
        x575=self.dropout45(x574)
        print('x575: {}'.format(x575.shape))
        x576=stochastic_depth(x575, 0.28695652173913044, 'row', False)
        print('x576: {}'.format(x576.shape))
        x577=operator.add(x569, x576)
        print('x577: {}'.format(x577.shape))
        x578=self.layernorm50(x577)
        print('x578: {}'.format(x578.shape))
        x581=operator.getitem(self.relative_position_bias_table23, self.relative_position_index23)
        print('x581: {}'.format(x581.shape))
        x582=x581.view(49, 49, -1)
        print('x582: {}'.format(x582.shape))
        x583=x582.permute(2, 0, 1)
        print('x583: {}'.format(x583.shape))
        x584=x583.contiguous()
        print('x584: {}'.format(x584.shape))
        x585=x584.unsqueeze(0)
        print('x585: {}'.format(x585.shape))
        x590=torchvision.models.swin_transformer.shifted_window_attention(x578, self.weight46, self.weight47, x585, [7, 7], 24,shift_size=[3, 3], attention_dropout=0.0, dropout=0.0, qkv_bias=self.bias46, proj_bias=self.bias47)
        print('x590: {}'.format(x590.shape))
        x591=stochastic_depth(x590, 0.3, 'row', False)
        print('x591: {}'.format(x591.shape))
        x592=operator.add(x577, x591)
        print('x592: {}'.format(x592.shape))
        x593=self.layernorm51(x592)
        print('x593: {}'.format(x593.shape))
        x594=self.linear49(x593)
        print('x594: {}'.format(x594.shape))
        x595=self.gelu23(x594)
        print('x595: {}'.format(x595.shape))
        x596=self.dropout46(x595)
        print('x596: {}'.format(x596.shape))
        x597=self.linear50(x596)
        print('x597: {}'.format(x597.shape))
        x598=self.dropout47(x597)
        print('x598: {}'.format(x598.shape))
        x599=stochastic_depth(x598, 0.3, 'row', False)
        print('x599: {}'.format(x599.shape))
        x600=operator.add(x592, x599)
        print('x600: {}'.format(x600.shape))
        x601=self.layernorm52(x600)
        print('x601: {}'.format(x601.shape))
        x602=x601.permute(0, 3, 1, 2)
        print('x602: {}'.format(x602.shape))
        x603=self.adaptiveavgpool2d0(x602)
        print('x603: {}'.format(x603.shape))
        x604=torch.flatten(x603, 1)
        print('x604: {}'.format(x604.shape))
        x605=self.linear51(x604)
        print('x605: {}'.format(x605.shape))

m = M().eval()
x = torch.randn(1, 3, 224, 224)
output = m(x)
