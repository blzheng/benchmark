import torch
from torch import tensor
import torch.nn as nn
from torch.nn import *
import torchvision
import torchvision.models as models
from torchvision.ops.stochastic_depth import stochastic_depth
import time
import builtins
import operator

class M(torch.nn.Module):
    def __init__(self):
        super(M, self).__init__()
        self.conv2d0 = Conv2d(3, 96, kernel_size=(4, 4), stride=(4, 4))
        self.layernorm0 = LayerNorm((96,), eps=1e-05, elementwise_affine=True)
        self.layernorm1 = LayerNorm((96,), eps=1e-05, elementwise_affine=True)
        self.layernorm2 = LayerNorm((96,), eps=1e-05, elementwise_affine=True)
        self.linear0 = Linear(in_features=96, out_features=384, bias=True)
        self.gelu0 = GELU(approximate='none')
        self.dropout0 = Dropout(p=0.0, inplace=False)
        self.linear1 = Linear(in_features=384, out_features=96, bias=True)
        self.dropout1 = Dropout(p=0.0, inplace=False)
        self.layernorm3 = LayerNorm((96,), eps=1e-05, elementwise_affine=True)
        self.layernorm4 = LayerNorm((96,), eps=1e-05, elementwise_affine=True)
        self.linear2 = Linear(in_features=96, out_features=384, bias=True)
        self.gelu1 = GELU(approximate='none')
        self.dropout2 = Dropout(p=0.0, inplace=False)
        self.linear3 = Linear(in_features=384, out_features=96, bias=True)
        self.dropout3 = Dropout(p=0.0, inplace=False)
        self.layernorm5 = LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        self.linear4 = Linear(in_features=384, out_features=192, bias=False)
        self.layernorm6 = LayerNorm((192,), eps=1e-05, elementwise_affine=True)
        self.layernorm7 = LayerNorm((192,), eps=1e-05, elementwise_affine=True)
        self.linear5 = Linear(in_features=192, out_features=768, bias=True)
        self.gelu2 = GELU(approximate='none')
        self.dropout4 = Dropout(p=0.0, inplace=False)
        self.linear6 = Linear(in_features=768, out_features=192, bias=True)
        self.dropout5 = Dropout(p=0.0, inplace=False)
        self.layernorm8 = LayerNorm((192,), eps=1e-05, elementwise_affine=True)
        self.layernorm9 = LayerNorm((192,), eps=1e-05, elementwise_affine=True)
        self.linear7 = Linear(in_features=192, out_features=768, bias=True)
        self.gelu3 = GELU(approximate='none')
        self.dropout6 = Dropout(p=0.0, inplace=False)
        self.linear8 = Linear(in_features=768, out_features=192, bias=True)
        self.dropout7 = Dropout(p=0.0, inplace=False)
        self.layernorm10 = LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        self.linear9 = Linear(in_features=768, out_features=384, bias=False)
        self.layernorm11 = LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        self.layernorm12 = LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        self.linear10 = Linear(in_features=384, out_features=1536, bias=True)
        self.gelu4 = GELU(approximate='none')
        self.dropout8 = Dropout(p=0.0, inplace=False)
        self.linear11 = Linear(in_features=1536, out_features=384, bias=True)
        self.dropout9 = Dropout(p=0.0, inplace=False)
        self.layernorm13 = LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        self.layernorm14 = LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        self.linear12 = Linear(in_features=384, out_features=1536, bias=True)
        self.gelu5 = GELU(approximate='none')
        self.dropout10 = Dropout(p=0.0, inplace=False)
        self.linear13 = Linear(in_features=1536, out_features=384, bias=True)
        self.dropout11 = Dropout(p=0.0, inplace=False)
        self.layernorm15 = LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        self.layernorm16 = LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        self.linear14 = Linear(in_features=384, out_features=1536, bias=True)
        self.gelu6 = GELU(approximate='none')
        self.dropout12 = Dropout(p=0.0, inplace=False)
        self.linear15 = Linear(in_features=1536, out_features=384, bias=True)
        self.dropout13 = Dropout(p=0.0, inplace=False)
        self.layernorm17 = LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        self.layernorm18 = LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        self.linear16 = Linear(in_features=384, out_features=1536, bias=True)
        self.gelu7 = GELU(approximate='none')
        self.dropout14 = Dropout(p=0.0, inplace=False)
        self.linear17 = Linear(in_features=1536, out_features=384, bias=True)
        self.dropout15 = Dropout(p=0.0, inplace=False)
        self.layernorm19 = LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        self.layernorm20 = LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        self.linear18 = Linear(in_features=384, out_features=1536, bias=True)
        self.gelu8 = GELU(approximate='none')
        self.dropout16 = Dropout(p=0.0, inplace=False)
        self.linear19 = Linear(in_features=1536, out_features=384, bias=True)
        self.dropout17 = Dropout(p=0.0, inplace=False)
        self.layernorm21 = LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        self.layernorm22 = LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        self.linear20 = Linear(in_features=384, out_features=1536, bias=True)
        self.gelu9 = GELU(approximate='none')
        self.dropout18 = Dropout(p=0.0, inplace=False)
        self.linear21 = Linear(in_features=1536, out_features=384, bias=True)
        self.dropout19 = Dropout(p=0.0, inplace=False)
        self.layernorm23 = LayerNorm((1536,), eps=1e-05, elementwise_affine=True)
        self.linear22 = Linear(in_features=1536, out_features=768, bias=False)
        self.layernorm24 = LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        self.layernorm25 = LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        self.linear23 = Linear(in_features=768, out_features=3072, bias=True)
        self.gelu10 = GELU(approximate='none')
        self.dropout20 = Dropout(p=0.0, inplace=False)
        self.linear24 = Linear(in_features=3072, out_features=768, bias=True)
        self.dropout21 = Dropout(p=0.0, inplace=False)
        self.layernorm26 = LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        self.layernorm27 = LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        self.linear25 = Linear(in_features=768, out_features=3072, bias=True)
        self.gelu11 = GELU(approximate='none')
        self.dropout22 = Dropout(p=0.0, inplace=False)
        self.linear26 = Linear(in_features=3072, out_features=768, bias=True)
        self.dropout23 = Dropout(p=0.0, inplace=False)
        self.layernorm28 = LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        self.adaptiveavgpool2d0 = AdaptiveAvgPool2d(output_size=1)
        self.linear27 = Linear(in_features=768, out_features=1000, bias=True)
        self.relative_position_bias_table0 = torch.rand(torch.Size([169, 3])).to(torch.float32)
        self.relative_position_index0 = torch.rand(torch.Size([2401])).to(torch.int64)
        self.weight0 = torch.rand(torch.Size([288, 96])).to(torch.float32)
        self.weight1 = torch.rand(torch.Size([96, 96])).to(torch.float32)
        self.bias0 = torch.rand(torch.Size([288])).to(torch.float32)
        self.bias1 = torch.rand(torch.Size([96])).to(torch.float32)
        self.relative_position_bias_table1 = torch.rand(torch.Size([169, 3])).to(torch.float32)
        self.relative_position_index1 = torch.rand(torch.Size([2401])).to(torch.int64)
        self.weight2 = torch.rand(torch.Size([288, 96])).to(torch.float32)
        self.weight3 = torch.rand(torch.Size([96, 96])).to(torch.float32)
        self.bias2 = torch.rand(torch.Size([288])).to(torch.float32)
        self.bias3 = torch.rand(torch.Size([96])).to(torch.float32)
        self.relative_position_bias_table2 = torch.rand(torch.Size([169, 6])).to(torch.float32)
        self.relative_position_index2 = torch.rand(torch.Size([2401])).to(torch.int64)
        self.weight4 = torch.rand(torch.Size([576, 192])).to(torch.float32)
        self.weight5 = torch.rand(torch.Size([192, 192])).to(torch.float32)
        self.bias4 = torch.rand(torch.Size([576])).to(torch.float32)
        self.bias5 = torch.rand(torch.Size([192])).to(torch.float32)
        self.relative_position_bias_table3 = torch.rand(torch.Size([169, 6])).to(torch.float32)
        self.relative_position_index3 = torch.rand(torch.Size([2401])).to(torch.int64)
        self.weight6 = torch.rand(torch.Size([576, 192])).to(torch.float32)
        self.weight7 = torch.rand(torch.Size([192, 192])).to(torch.float32)
        self.bias6 = torch.rand(torch.Size([576])).to(torch.float32)
        self.bias7 = torch.rand(torch.Size([192])).to(torch.float32)
        self.relative_position_bias_table4 = torch.rand(torch.Size([169, 12])).to(torch.float32)
        self.relative_position_index4 = torch.rand(torch.Size([2401])).to(torch.int64)
        self.weight8 = torch.rand(torch.Size([1152, 384])).to(torch.float32)
        self.weight9 = torch.rand(torch.Size([384, 384])).to(torch.float32)
        self.bias8 = torch.rand(torch.Size([1152])).to(torch.float32)
        self.bias9 = torch.rand(torch.Size([384])).to(torch.float32)
        self.relative_position_bias_table5 = torch.rand(torch.Size([169, 12])).to(torch.float32)
        self.relative_position_index5 = torch.rand(torch.Size([2401])).to(torch.int64)
        self.weight10 = torch.rand(torch.Size([1152, 384])).to(torch.float32)
        self.weight11 = torch.rand(torch.Size([384, 384])).to(torch.float32)
        self.bias10 = torch.rand(torch.Size([1152])).to(torch.float32)
        self.bias11 = torch.rand(torch.Size([384])).to(torch.float32)
        self.relative_position_bias_table6 = torch.rand(torch.Size([169, 12])).to(torch.float32)
        self.relative_position_index6 = torch.rand(torch.Size([2401])).to(torch.int64)
        self.weight12 = torch.rand(torch.Size([1152, 384])).to(torch.float32)
        self.weight13 = torch.rand(torch.Size([384, 384])).to(torch.float32)
        self.bias12 = torch.rand(torch.Size([1152])).to(torch.float32)
        self.bias13 = torch.rand(torch.Size([384])).to(torch.float32)
        self.relative_position_bias_table7 = torch.rand(torch.Size([169, 12])).to(torch.float32)
        self.relative_position_index7 = torch.rand(torch.Size([2401])).to(torch.int64)
        self.weight14 = torch.rand(torch.Size([1152, 384])).to(torch.float32)
        self.weight15 = torch.rand(torch.Size([384, 384])).to(torch.float32)
        self.bias14 = torch.rand(torch.Size([1152])).to(torch.float32)
        self.bias15 = torch.rand(torch.Size([384])).to(torch.float32)
        self.relative_position_bias_table8 = torch.rand(torch.Size([169, 12])).to(torch.float32)
        self.relative_position_index8 = torch.rand(torch.Size([2401])).to(torch.int64)
        self.weight16 = torch.rand(torch.Size([1152, 384])).to(torch.float32)
        self.weight17 = torch.rand(torch.Size([384, 384])).to(torch.float32)
        self.bias16 = torch.rand(torch.Size([1152])).to(torch.float32)
        self.bias17 = torch.rand(torch.Size([384])).to(torch.float32)
        self.relative_position_bias_table9 = torch.rand(torch.Size([169, 12])).to(torch.float32)
        self.relative_position_index9 = torch.rand(torch.Size([2401])).to(torch.int64)
        self.weight18 = torch.rand(torch.Size([1152, 384])).to(torch.float32)
        self.weight19 = torch.rand(torch.Size([384, 384])).to(torch.float32)
        self.bias18 = torch.rand(torch.Size([1152])).to(torch.float32)
        self.bias19 = torch.rand(torch.Size([384])).to(torch.float32)
        self.relative_position_bias_table10 = torch.rand(torch.Size([169, 24])).to(torch.float32)
        self.relative_position_index10 = torch.rand(torch.Size([2401])).to(torch.int64)
        self.weight20 = torch.rand(torch.Size([2304, 768])).to(torch.float32)
        self.weight21 = torch.rand(torch.Size([768, 768])).to(torch.float32)
        self.bias20 = torch.rand(torch.Size([2304])).to(torch.float32)
        self.bias21 = torch.rand(torch.Size([768])).to(torch.float32)
        self.relative_position_bias_table11 = torch.rand(torch.Size([169, 24])).to(torch.float32)
        self.relative_position_index11 = torch.rand(torch.Size([2401])).to(torch.int64)
        self.weight22 = torch.rand(torch.Size([2304, 768])).to(torch.float32)
        self.weight23 = torch.rand(torch.Size([768, 768])).to(torch.float32)
        self.bias22 = torch.rand(torch.Size([2304])).to(torch.float32)
        self.bias23 = torch.rand(torch.Size([768])).to(torch.float32)

    def forward(self, x):
        x0=x
        print('x0: {}'.format(x0.shape))
        x1=self.conv2d0(x0)
        print('x1: {}'.format(x1.shape))
        x2=torch.permute(x1, [0, 2, 3, 1])
        print('x2: {}'.format(x2.shape))
        x3=self.layernorm0(x2)
        print('x3: {}'.format(x3.shape))
        x4=self.layernorm1(x3)
        print('x4: {}'.format(x4.shape))
        x7=operator.getitem(self.relative_position_bias_table0, self.relative_position_index0)
        print('x7: {}'.format(x7.shape))
        x8=x7.view(49, 49, -1)
        print('x8: {}'.format(x8.shape))
        x9=x8.permute(2, 0, 1)
        print('x9: {}'.format(x9.shape))
        x10=x9.contiguous()
        print('x10: {}'.format(x10.shape))
        x11=x10.unsqueeze(0)
        print('x11: {}'.format(x11.shape))
        x16=torchvision.models.swin_transformer.shifted_window_attention(x4, self.weight0, self.weight1, x11, [7, 7], 3,shift_size=[0, 0], attention_dropout=0.0, dropout=0.0, qkv_bias=self.bias0, proj_bias=self.bias1)
        print('x16: {}'.format(x16.shape))
        x17=stochastic_depth(x16, 0.0, 'row', False)
        print('x17: {}'.format(x17.shape))
        x18=operator.add(x3, x17)
        print('x18: {}'.format(x18.shape))
        x19=self.layernorm2(x18)
        print('x19: {}'.format(x19.shape))
        x20=self.linear0(x19)
        print('x20: {}'.format(x20.shape))
        x21=self.gelu0(x20)
        print('x21: {}'.format(x21.shape))
        x22=self.dropout0(x21)
        print('x22: {}'.format(x22.shape))
        x23=self.linear1(x22)
        print('x23: {}'.format(x23.shape))
        x24=self.dropout1(x23)
        print('x24: {}'.format(x24.shape))
        x25=stochastic_depth(x24, 0.0, 'row', False)
        print('x25: {}'.format(x25.shape))
        x26=operator.add(x18, x25)
        print('x26: {}'.format(x26.shape))
        x27=self.layernorm3(x26)
        print('x27: {}'.format(x27.shape))
        x30=operator.getitem(self.relative_position_bias_table1, self.relative_position_index1)
        print('x30: {}'.format(x30.shape))
        x31=x30.view(49, 49, -1)
        print('x31: {}'.format(x31.shape))
        x32=x31.permute(2, 0, 1)
        print('x32: {}'.format(x32.shape))
        x33=x32.contiguous()
        print('x33: {}'.format(x33.shape))
        x34=x33.unsqueeze(0)
        print('x34: {}'.format(x34.shape))
        x39=torchvision.models.swin_transformer.shifted_window_attention(x27, self.weight2, self.weight3, x34, [7, 7], 3,shift_size=[3, 3], attention_dropout=0.0, dropout=0.0, qkv_bias=self.bias2, proj_bias=self.bias3)
        print('x39: {}'.format(x39.shape))
        x40=stochastic_depth(x39, 0.018181818181818184, 'row', False)
        print('x40: {}'.format(x40.shape))
        x41=operator.add(x26, x40)
        print('x41: {}'.format(x41.shape))
        x42=self.layernorm4(x41)
        print('x42: {}'.format(x42.shape))
        x43=self.linear2(x42)
        print('x43: {}'.format(x43.shape))
        x44=self.gelu1(x43)
        print('x44: {}'.format(x44.shape))
        x45=self.dropout2(x44)
        print('x45: {}'.format(x45.shape))
        x46=self.linear3(x45)
        print('x46: {}'.format(x46.shape))
        x47=self.dropout3(x46)
        print('x47: {}'.format(x47.shape))
        x48=stochastic_depth(x47, 0.018181818181818184, 'row', False)
        print('x48: {}'.format(x48.shape))
        x49=operator.add(x41, x48)
        print('x49: {}'.format(x49.shape))
        x50=builtins.getattr(x49, 'shape')
        print('x50: {}'.format(x50.shape))
        x51=operator.getitem(x50, slice(-3, None, None))
        print('x51: {}'.format(x51.shape))
        x52=operator.getitem(x51, 0)
        print('x52: {}'.format(x52.shape))
        x53=operator.getitem(x51, 1)
        print('x53: {}'.format(x53.shape))
        x54=operator.getitem(x51, 2)
        print('x54: {}'.format(x54.shape))
        x55=operator.mod(x53, 2)
        print('x55: {}'.format(x55.shape))
        x56=operator.mod(x52, 2)
        print('x56: {}'.format(x56.shape))
        x57=torch.nn.functional.pad(x49, (0, 0, 0, x55, 0, x56))
        print('x57: {}'.format(x57.shape))
        x58=operator.getitem(x57, (Ellipsis, slice(0, None, 2), slice(0, None, 2), slice(None, None, None)))
        print('x58: {}'.format(x58.shape))
        x59=operator.getitem(x57, (Ellipsis, slice(1, None, 2), slice(0, None, 2), slice(None, None, None)))
        print('x59: {}'.format(x59.shape))
        x60=operator.getitem(x57, (Ellipsis, slice(0, None, 2), slice(1, None, 2), slice(None, None, None)))
        print('x60: {}'.format(x60.shape))
        x61=operator.getitem(x57, (Ellipsis, slice(1, None, 2), slice(1, None, 2), slice(None, None, None)))
        print('x61: {}'.format(x61.shape))
        x62=torch.cat([x58, x59, x60, x61], -1)
        print('x62: {}'.format(x62.shape))
        x63=self.layernorm5(x62)
        print('x63: {}'.format(x63.shape))
        x64=self.linear4(x63)
        print('x64: {}'.format(x64.shape))
        x65=self.layernorm6(x64)
        print('x65: {}'.format(x65.shape))
        x68=operator.getitem(self.relative_position_bias_table2, self.relative_position_index2)
        print('x68: {}'.format(x68.shape))
        x69=x68.view(49, 49, -1)
        print('x69: {}'.format(x69.shape))
        x70=x69.permute(2, 0, 1)
        print('x70: {}'.format(x70.shape))
        x71=x70.contiguous()
        print('x71: {}'.format(x71.shape))
        x72=x71.unsqueeze(0)
        print('x72: {}'.format(x72.shape))
        x77=torchvision.models.swin_transformer.shifted_window_attention(x65, self.weight4, self.weight5, x72, [7, 7], 6,shift_size=[0, 0], attention_dropout=0.0, dropout=0.0, qkv_bias=self.bias4, proj_bias=self.bias5)
        print('x77: {}'.format(x77.shape))
        x78=stochastic_depth(x77, 0.03636363636363637, 'row', False)
        print('x78: {}'.format(x78.shape))
        x79=operator.add(x64, x78)
        print('x79: {}'.format(x79.shape))
        x80=self.layernorm7(x79)
        print('x80: {}'.format(x80.shape))
        x81=self.linear5(x80)
        print('x81: {}'.format(x81.shape))
        x82=self.gelu2(x81)
        print('x82: {}'.format(x82.shape))
        x83=self.dropout4(x82)
        print('x83: {}'.format(x83.shape))
        x84=self.linear6(x83)
        print('x84: {}'.format(x84.shape))
        x85=self.dropout5(x84)
        print('x85: {}'.format(x85.shape))
        x86=stochastic_depth(x85, 0.03636363636363637, 'row', False)
        print('x86: {}'.format(x86.shape))
        x87=operator.add(x79, x86)
        print('x87: {}'.format(x87.shape))
        x88=self.layernorm8(x87)
        print('x88: {}'.format(x88.shape))
        x91=operator.getitem(self.relative_position_bias_table3, self.relative_position_index3)
        print('x91: {}'.format(x91.shape))
        x92=x91.view(49, 49, -1)
        print('x92: {}'.format(x92.shape))
        x93=x92.permute(2, 0, 1)
        print('x93: {}'.format(x93.shape))
        x94=x93.contiguous()
        print('x94: {}'.format(x94.shape))
        x95=x94.unsqueeze(0)
        print('x95: {}'.format(x95.shape))
        x100=torchvision.models.swin_transformer.shifted_window_attention(x88, self.weight6, self.weight7, x95, [7, 7], 6,shift_size=[3, 3], attention_dropout=0.0, dropout=0.0, qkv_bias=self.bias6, proj_bias=self.bias7)
        print('x100: {}'.format(x100.shape))
        x101=stochastic_depth(x100, 0.05454545454545456, 'row', False)
        print('x101: {}'.format(x101.shape))
        x102=operator.add(x87, x101)
        print('x102: {}'.format(x102.shape))
        x103=self.layernorm9(x102)
        print('x103: {}'.format(x103.shape))
        x104=self.linear7(x103)
        print('x104: {}'.format(x104.shape))
        x105=self.gelu3(x104)
        print('x105: {}'.format(x105.shape))
        x106=self.dropout6(x105)
        print('x106: {}'.format(x106.shape))
        x107=self.linear8(x106)
        print('x107: {}'.format(x107.shape))
        x108=self.dropout7(x107)
        print('x108: {}'.format(x108.shape))
        x109=stochastic_depth(x108, 0.05454545454545456, 'row', False)
        print('x109: {}'.format(x109.shape))
        x110=operator.add(x102, x109)
        print('x110: {}'.format(x110.shape))
        x111=builtins.getattr(x110, 'shape')
        print('x111: {}'.format(x111.shape))
        x112=operator.getitem(x111, slice(-3, None, None))
        print('x112: {}'.format(x112.shape))
        x113=operator.getitem(x112, 0)
        print('x113: {}'.format(x113.shape))
        x114=operator.getitem(x112, 1)
        print('x114: {}'.format(x114.shape))
        x115=operator.getitem(x112, 2)
        print('x115: {}'.format(x115.shape))
        x116=operator.mod(x114, 2)
        print('x116: {}'.format(x116.shape))
        x117=operator.mod(x113, 2)
        print('x117: {}'.format(x117.shape))
        x118=torch.nn.functional.pad(x110, (0, 0, 0, x116, 0, x117))
        print('x118: {}'.format(x118.shape))
        x119=operator.getitem(x118, (Ellipsis, slice(0, None, 2), slice(0, None, 2), slice(None, None, None)))
        print('x119: {}'.format(x119.shape))
        x120=operator.getitem(x118, (Ellipsis, slice(1, None, 2), slice(0, None, 2), slice(None, None, None)))
        print('x120: {}'.format(x120.shape))
        x121=operator.getitem(x118, (Ellipsis, slice(0, None, 2), slice(1, None, 2), slice(None, None, None)))
        print('x121: {}'.format(x121.shape))
        x122=operator.getitem(x118, (Ellipsis, slice(1, None, 2), slice(1, None, 2), slice(None, None, None)))
        print('x122: {}'.format(x122.shape))
        x123=torch.cat([x119, x120, x121, x122], -1)
        print('x123: {}'.format(x123.shape))
        x124=self.layernorm10(x123)
        print('x124: {}'.format(x124.shape))
        x125=self.linear9(x124)
        print('x125: {}'.format(x125.shape))
        x126=self.layernorm11(x125)
        print('x126: {}'.format(x126.shape))
        x129=operator.getitem(self.relative_position_bias_table4, self.relative_position_index4)
        print('x129: {}'.format(x129.shape))
        x130=x129.view(49, 49, -1)
        print('x130: {}'.format(x130.shape))
        x131=x130.permute(2, 0, 1)
        print('x131: {}'.format(x131.shape))
        x132=x131.contiguous()
        print('x132: {}'.format(x132.shape))
        x133=x132.unsqueeze(0)
        print('x133: {}'.format(x133.shape))
        x138=torchvision.models.swin_transformer.shifted_window_attention(x126, self.weight8, self.weight9, x133, [7, 7], 12,shift_size=[0, 0], attention_dropout=0.0, dropout=0.0, qkv_bias=self.bias8, proj_bias=self.bias9)
        print('x138: {}'.format(x138.shape))
        x139=stochastic_depth(x138, 0.07272727272727274, 'row', False)
        print('x139: {}'.format(x139.shape))
        x140=operator.add(x125, x139)
        print('x140: {}'.format(x140.shape))
        x141=self.layernorm12(x140)
        print('x141: {}'.format(x141.shape))
        x142=self.linear10(x141)
        print('x142: {}'.format(x142.shape))
        x143=self.gelu4(x142)
        print('x143: {}'.format(x143.shape))
        x144=self.dropout8(x143)
        print('x144: {}'.format(x144.shape))
        x145=self.linear11(x144)
        print('x145: {}'.format(x145.shape))
        x146=self.dropout9(x145)
        print('x146: {}'.format(x146.shape))
        x147=stochastic_depth(x146, 0.07272727272727274, 'row', False)
        print('x147: {}'.format(x147.shape))
        x148=operator.add(x140, x147)
        print('x148: {}'.format(x148.shape))
        x149=self.layernorm13(x148)
        print('x149: {}'.format(x149.shape))
        x152=operator.getitem(self.relative_position_bias_table5, self.relative_position_index5)
        print('x152: {}'.format(x152.shape))
        x153=x152.view(49, 49, -1)
        print('x153: {}'.format(x153.shape))
        x154=x153.permute(2, 0, 1)
        print('x154: {}'.format(x154.shape))
        x155=x154.contiguous()
        print('x155: {}'.format(x155.shape))
        x156=x155.unsqueeze(0)
        print('x156: {}'.format(x156.shape))
        x161=torchvision.models.swin_transformer.shifted_window_attention(x149, self.weight10, self.weight11, x156, [7, 7], 12,shift_size=[3, 3], attention_dropout=0.0, dropout=0.0, qkv_bias=self.bias10, proj_bias=self.bias11)
        print('x161: {}'.format(x161.shape))
        x162=stochastic_depth(x161, 0.09090909090909091, 'row', False)
        print('x162: {}'.format(x162.shape))
        x163=operator.add(x148, x162)
        print('x163: {}'.format(x163.shape))
        x164=self.layernorm14(x163)
        print('x164: {}'.format(x164.shape))
        x165=self.linear12(x164)
        print('x165: {}'.format(x165.shape))
        x166=self.gelu5(x165)
        print('x166: {}'.format(x166.shape))
        x167=self.dropout10(x166)
        print('x167: {}'.format(x167.shape))
        x168=self.linear13(x167)
        print('x168: {}'.format(x168.shape))
        x169=self.dropout11(x168)
        print('x169: {}'.format(x169.shape))
        x170=stochastic_depth(x169, 0.09090909090909091, 'row', False)
        print('x170: {}'.format(x170.shape))
        x171=operator.add(x163, x170)
        print('x171: {}'.format(x171.shape))
        x172=self.layernorm15(x171)
        print('x172: {}'.format(x172.shape))
        x175=operator.getitem(self.relative_position_bias_table6, self.relative_position_index6)
        print('x175: {}'.format(x175.shape))
        x176=x175.view(49, 49, -1)
        print('x176: {}'.format(x176.shape))
        x177=x176.permute(2, 0, 1)
        print('x177: {}'.format(x177.shape))
        x178=x177.contiguous()
        print('x178: {}'.format(x178.shape))
        x179=x178.unsqueeze(0)
        print('x179: {}'.format(x179.shape))
        x184=torchvision.models.swin_transformer.shifted_window_attention(x172, self.weight12, self.weight13, x179, [7, 7], 12,shift_size=[0, 0], attention_dropout=0.0, dropout=0.0, qkv_bias=self.bias12, proj_bias=self.bias13)
        print('x184: {}'.format(x184.shape))
        x185=stochastic_depth(x184, 0.10909090909090911, 'row', False)
        print('x185: {}'.format(x185.shape))
        x186=operator.add(x171, x185)
        print('x186: {}'.format(x186.shape))
        x187=self.layernorm16(x186)
        print('x187: {}'.format(x187.shape))
        x188=self.linear14(x187)
        print('x188: {}'.format(x188.shape))
        x189=self.gelu6(x188)
        print('x189: {}'.format(x189.shape))
        x190=self.dropout12(x189)
        print('x190: {}'.format(x190.shape))
        x191=self.linear15(x190)
        print('x191: {}'.format(x191.shape))
        x192=self.dropout13(x191)
        print('x192: {}'.format(x192.shape))
        x193=stochastic_depth(x192, 0.10909090909090911, 'row', False)
        print('x193: {}'.format(x193.shape))
        x194=operator.add(x186, x193)
        print('x194: {}'.format(x194.shape))
        x195=self.layernorm17(x194)
        print('x195: {}'.format(x195.shape))
        x198=operator.getitem(self.relative_position_bias_table7, self.relative_position_index7)
        print('x198: {}'.format(x198.shape))
        x199=x198.view(49, 49, -1)
        print('x199: {}'.format(x199.shape))
        x200=x199.permute(2, 0, 1)
        print('x200: {}'.format(x200.shape))
        x201=x200.contiguous()
        print('x201: {}'.format(x201.shape))
        x202=x201.unsqueeze(0)
        print('x202: {}'.format(x202.shape))
        x207=torchvision.models.swin_transformer.shifted_window_attention(x195, self.weight14, self.weight15, x202, [7, 7], 12,shift_size=[3, 3], attention_dropout=0.0, dropout=0.0, qkv_bias=self.bias14, proj_bias=self.bias15)
        print('x207: {}'.format(x207.shape))
        x208=stochastic_depth(x207, 0.1272727272727273, 'row', False)
        print('x208: {}'.format(x208.shape))
        x209=operator.add(x194, x208)
        print('x209: {}'.format(x209.shape))
        x210=self.layernorm18(x209)
        print('x210: {}'.format(x210.shape))
        x211=self.linear16(x210)
        print('x211: {}'.format(x211.shape))
        x212=self.gelu7(x211)
        print('x212: {}'.format(x212.shape))
        x213=self.dropout14(x212)
        print('x213: {}'.format(x213.shape))
        x214=self.linear17(x213)
        print('x214: {}'.format(x214.shape))
        x215=self.dropout15(x214)
        print('x215: {}'.format(x215.shape))
        x216=stochastic_depth(x215, 0.1272727272727273, 'row', False)
        print('x216: {}'.format(x216.shape))
        x217=operator.add(x209, x216)
        print('x217: {}'.format(x217.shape))
        x218=self.layernorm19(x217)
        print('x218: {}'.format(x218.shape))
        x221=operator.getitem(self.relative_position_bias_table8, self.relative_position_index8)
        print('x221: {}'.format(x221.shape))
        x222=x221.view(49, 49, -1)
        print('x222: {}'.format(x222.shape))
        x223=x222.permute(2, 0, 1)
        print('x223: {}'.format(x223.shape))
        x224=x223.contiguous()
        print('x224: {}'.format(x224.shape))
        x225=x224.unsqueeze(0)
        print('x225: {}'.format(x225.shape))
        x230=torchvision.models.swin_transformer.shifted_window_attention(x218, self.weight16, self.weight17, x225, [7, 7], 12,shift_size=[0, 0], attention_dropout=0.0, dropout=0.0, qkv_bias=self.bias16, proj_bias=self.bias17)
        print('x230: {}'.format(x230.shape))
        x231=stochastic_depth(x230, 0.14545454545454548, 'row', False)
        print('x231: {}'.format(x231.shape))
        x232=operator.add(x217, x231)
        print('x232: {}'.format(x232.shape))
        x233=self.layernorm20(x232)
        print('x233: {}'.format(x233.shape))
        x234=self.linear18(x233)
        print('x234: {}'.format(x234.shape))
        x235=self.gelu8(x234)
        print('x235: {}'.format(x235.shape))
        x236=self.dropout16(x235)
        print('x236: {}'.format(x236.shape))
        x237=self.linear19(x236)
        print('x237: {}'.format(x237.shape))
        x238=self.dropout17(x237)
        print('x238: {}'.format(x238.shape))
        x239=stochastic_depth(x238, 0.14545454545454548, 'row', False)
        print('x239: {}'.format(x239.shape))
        x240=operator.add(x232, x239)
        print('x240: {}'.format(x240.shape))
        x241=self.layernorm21(x240)
        print('x241: {}'.format(x241.shape))
        x244=operator.getitem(self.relative_position_bias_table9, self.relative_position_index9)
        print('x244: {}'.format(x244.shape))
        x245=x244.view(49, 49, -1)
        print('x245: {}'.format(x245.shape))
        x246=x245.permute(2, 0, 1)
        print('x246: {}'.format(x246.shape))
        x247=x246.contiguous()
        print('x247: {}'.format(x247.shape))
        x248=x247.unsqueeze(0)
        print('x248: {}'.format(x248.shape))
        x253=torchvision.models.swin_transformer.shifted_window_attention(x241, self.weight18, self.weight19, x248, [7, 7], 12,shift_size=[3, 3], attention_dropout=0.0, dropout=0.0, qkv_bias=self.bias18, proj_bias=self.bias19)
        print('x253: {}'.format(x253.shape))
        x254=stochastic_depth(x253, 0.16363636363636364, 'row', False)
        print('x254: {}'.format(x254.shape))
        x255=operator.add(x240, x254)
        print('x255: {}'.format(x255.shape))
        x256=self.layernorm22(x255)
        print('x256: {}'.format(x256.shape))
        x257=self.linear20(x256)
        print('x257: {}'.format(x257.shape))
        x258=self.gelu9(x257)
        print('x258: {}'.format(x258.shape))
        x259=self.dropout18(x258)
        print('x259: {}'.format(x259.shape))
        x260=self.linear21(x259)
        print('x260: {}'.format(x260.shape))
        x261=self.dropout19(x260)
        print('x261: {}'.format(x261.shape))
        x262=stochastic_depth(x261, 0.16363636363636364, 'row', False)
        print('x262: {}'.format(x262.shape))
        x263=operator.add(x255, x262)
        print('x263: {}'.format(x263.shape))
        x264=builtins.getattr(x263, 'shape')
        print('x264: {}'.format(x264.shape))
        x265=operator.getitem(x264, slice(-3, None, None))
        print('x265: {}'.format(x265.shape))
        x266=operator.getitem(x265, 0)
        print('x266: {}'.format(x266.shape))
        x267=operator.getitem(x265, 1)
        print('x267: {}'.format(x267.shape))
        x268=operator.getitem(x265, 2)
        print('x268: {}'.format(x268.shape))
        x269=operator.mod(x267, 2)
        print('x269: {}'.format(x269.shape))
        x270=operator.mod(x266, 2)
        print('x270: {}'.format(x270.shape))
        x271=torch.nn.functional.pad(x263, (0, 0, 0, x269, 0, x270))
        print('x271: {}'.format(x271.shape))
        x272=operator.getitem(x271, (Ellipsis, slice(0, None, 2), slice(0, None, 2), slice(None, None, None)))
        print('x272: {}'.format(x272.shape))
        x273=operator.getitem(x271, (Ellipsis, slice(1, None, 2), slice(0, None, 2), slice(None, None, None)))
        print('x273: {}'.format(x273.shape))
        x274=operator.getitem(x271, (Ellipsis, slice(0, None, 2), slice(1, None, 2), slice(None, None, None)))
        print('x274: {}'.format(x274.shape))
        x275=operator.getitem(x271, (Ellipsis, slice(1, None, 2), slice(1, None, 2), slice(None, None, None)))
        print('x275: {}'.format(x275.shape))
        x276=torch.cat([x272, x273, x274, x275], -1)
        print('x276: {}'.format(x276.shape))
        x277=self.layernorm23(x276)
        print('x277: {}'.format(x277.shape))
        x278=self.linear22(x277)
        print('x278: {}'.format(x278.shape))
        x279=self.layernorm24(x278)
        print('x279: {}'.format(x279.shape))
        x282=operator.getitem(self.relative_position_bias_table10, self.relative_position_index10)
        print('x282: {}'.format(x282.shape))
        x283=x282.view(49, 49, -1)
        print('x283: {}'.format(x283.shape))
        x284=x283.permute(2, 0, 1)
        print('x284: {}'.format(x284.shape))
        x285=x284.contiguous()
        print('x285: {}'.format(x285.shape))
        x286=x285.unsqueeze(0)
        print('x286: {}'.format(x286.shape))
        x291=torchvision.models.swin_transformer.shifted_window_attention(x279, self.weight20, self.weight21, x286, [7, 7], 24,shift_size=[0, 0], attention_dropout=0.0, dropout=0.0, qkv_bias=self.bias20, proj_bias=self.bias21)
        print('x291: {}'.format(x291.shape))
        x292=stochastic_depth(x291, 0.18181818181818182, 'row', False)
        print('x292: {}'.format(x292.shape))
        x293=operator.add(x278, x292)
        print('x293: {}'.format(x293.shape))
        x294=self.layernorm25(x293)
        print('x294: {}'.format(x294.shape))
        x295=self.linear23(x294)
        print('x295: {}'.format(x295.shape))
        x296=self.gelu10(x295)
        print('x296: {}'.format(x296.shape))
        x297=self.dropout20(x296)
        print('x297: {}'.format(x297.shape))
        x298=self.linear24(x297)
        print('x298: {}'.format(x298.shape))
        x299=self.dropout21(x298)
        print('x299: {}'.format(x299.shape))
        x300=stochastic_depth(x299, 0.18181818181818182, 'row', False)
        print('x300: {}'.format(x300.shape))
        x301=operator.add(x293, x300)
        print('x301: {}'.format(x301.shape))
        x302=self.layernorm26(x301)
        print('x302: {}'.format(x302.shape))
        x305=operator.getitem(self.relative_position_bias_table11, self.relative_position_index11)
        print('x305: {}'.format(x305.shape))
        x306=x305.view(49, 49, -1)
        print('x306: {}'.format(x306.shape))
        x307=x306.permute(2, 0, 1)
        print('x307: {}'.format(x307.shape))
        x308=x307.contiguous()
        print('x308: {}'.format(x308.shape))
        x309=x308.unsqueeze(0)
        print('x309: {}'.format(x309.shape))
        x314=torchvision.models.swin_transformer.shifted_window_attention(x302, self.weight22, self.weight23, x309, [7, 7], 24,shift_size=[3, 3], attention_dropout=0.0, dropout=0.0, qkv_bias=self.bias22, proj_bias=self.bias23)
        print('x314: {}'.format(x314.shape))
        x315=stochastic_depth(x314, 0.2, 'row', False)
        print('x315: {}'.format(x315.shape))
        x316=operator.add(x301, x315)
        print('x316: {}'.format(x316.shape))
        x317=self.layernorm27(x316)
        print('x317: {}'.format(x317.shape))
        x318=self.linear25(x317)
        print('x318: {}'.format(x318.shape))
        x319=self.gelu11(x318)
        print('x319: {}'.format(x319.shape))
        x320=self.dropout22(x319)
        print('x320: {}'.format(x320.shape))
        x321=self.linear26(x320)
        print('x321: {}'.format(x321.shape))
        x322=self.dropout23(x321)
        print('x322: {}'.format(x322.shape))
        x323=stochastic_depth(x322, 0.2, 'row', False)
        print('x323: {}'.format(x323.shape))
        x324=operator.add(x316, x323)
        print('x324: {}'.format(x324.shape))
        x325=self.layernorm28(x324)
        print('x325: {}'.format(x325.shape))
        x326=x325.permute(0, 3, 1, 2)
        print('x326: {}'.format(x326.shape))
        x327=self.adaptiveavgpool2d0(x326)
        print('x327: {}'.format(x327.shape))
        x328=torch.flatten(x327, 1)
        print('x328: {}'.format(x328.shape))
        x329=self.linear27(x328)
        print('x329: {}'.format(x329.shape))

m = M().eval()
x = torch.randn(1, 3, 224, 224)
output = m(x)
